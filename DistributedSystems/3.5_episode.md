# Episode 3.5 — Consistency Models: Choose Your Guarantee
## From Majority Element to Quorum-Based Reads and Writes

**Season 3 — Distributed Systems: Building DistKV**

---

## Previously on DistKV...

In Episodes 3.1-3.4, we built a sharded, replicated KV store with distributed transactions. Every read goes through the Raft leader. Every write gets majority-replicated before commitment.

**This is maximally consistent — and maximally slow.** A social media "likes" counter doesn't need the same guarantees as a bank balance. Reading from a follower replica is 10x faster than routing to the leader across data centers.

**Today's fix:** Add tunable consistency to DistKV. Clients choose their guarantee per operation — strong when correctness matters, eventual when speed matters more.

**Arc So Far:**
- **3.1**: Single-node KV store ✅
- **3.2**: Raft replication ✅
- **3.3**: Sharding ✅
- **3.4**: Distributed transactions ✅
- **3.5**: Consistency models ← YOU ARE HERE

---

## 1. The Hook: Real-World Production Failure

### The 2015 Amazon DynamoDB Blast Radius
**"30,000 leader elections in 10 minutes took down half of AWS"**

**The Incident:**
1. DynamoDB's metadata system used strong consistency for ALL reads
2. A network blip caused cascading Raft leader elections
3. 30,000 groups simultaneously electing leaders = metadata storm
4. Clients couldn't find leaders → requests piled up → upstream services failed
5. Half of US-EAST-1 went down — S3, Lambda, ECS all affected

**The Critical Insight:** DynamoDB's metadata service demanded strong consistency when it didn't need it. A cache of "which node is the leader" only needs to be fresh within seconds — eventual consistency would have survived the storm.

**The Lesson:** Choosing the wrong consistency level doesn't just slow things down — it creates blast radius that takes down entire regions.

---

## 2. The LeetCode Seed: Majority Element

```python
# LeetCode #169: Majority Element
# Find the element that appears more than n/2 times

def majorityElement(nums):
    """
    Boyer-Moore Voting Algorithm:
    The majority element beats all other elements combined.
    
    This IS quorum-based consistency:
    - Array elements = node responses
    - Majority element = the "true" value
    - Voting process = quorum consensus
    
    If W + R > N, reads and writes overlap:
    at least one node in the read quorum saw the latest write.
    That node's value is the "majority element" — the truth.
    """
    candidate = None
    count = 0
    
    for num in nums:
        if count == 0:
            candidate = num
            count = 1
        elif num == candidate:
            count += 1
        else:
            count -= 1
    
    return candidate

# The quorum connection:
#
# N = 5 replicas
# W = 3 (write to 3 nodes)
# R = 3 (read from 3 nodes)
#
# W + R = 6 > 5 = N
#
# Any 3 nodes we READ from MUST include at least 1 node
# that has the latest write. That write is the "majority"
# in our read responses — the value we return.
#
# This is exactly Boyer-Moore: the latest write "wins"
# because it appears in the majority of read responses.
```

---

## 3. The Consistency Spectrum

```
Strong                                                    Eventual
  |                                                         |
  ├── Linearizable ──┼── Sequential ──┼── Causal ──┼── Eventual ──┤
  │                  │               │            │              │
  │ All ops appear   │  All ops      │ Causally   │  All nodes   │
  │ to execute at    │  in some      │ related    │  converge    │
  │ one point in     │  total order  │  ops in    │  eventually  │
  │ real time        │  (but may not │  right     │  (no time    │
  │                  │  match wall   │  order     │  guarantee)  │
  │ (Spanner)        │  clock)       │ (Session)  │ (DNS, CDN)   │
  └──────────────────┴──────────────┴────────────┴──────────────┘

When to use each:

| Data Type                | Consistency   | Why |
|--------------------------|--------------|-----|
| Bank account balance     | Linearizable | Wrong balance = legal liability |
| Shopping cart             | Eventual     | Temporarily wrong ≪ cart unavailable |
| Social media likes       | Eventual     | 99 vs 100 doesn't matter |
| Inventory count          | Linearizable | Overselling = angry customers |
| User profile             | Session      | User should see own writes |
| Search index             | Eventual     | Slightly stale results OK |
| Leader election metadata | Causal       | Must see latest leader eventually |
```

---

## 4. Production System Build

### Component 1: Quorum Configuration

```python
from dataclasses import dataclass
from typing import Optional, List, Dict, Tuple
from enum import Enum

class ConsistencyLevel(Enum):
    STRONG = "strong"           # Read from leader (linearizable)
    QUORUM = "quorum"           # Read from R nodes, W+R > N
    SESSION = "session"         # Read-your-writes within session
    EVENTUAL = "eventual"       # Read from any replica (fastest)
    BOUNDED_STALENESS = "bounded"  # Read from replica, max X seconds old

@dataclass
class QuorumConfig:
    """
    Quorum configuration for the cluster.
    
    The fundamental rule: W + R > N
    
    Where:
    - N = total replicas per shard (from Episode 3.2)
    - W = number of nodes that must ACK a write
    - R = number of nodes that must respond to a read
    
    If W + R > N, at least one node in every read quorum
    saw the latest write. This ensures consistency.
    """
    n: int = 3  # Total replicas
    
    def strong(self):
        """Read from leader only. Strongest guarantee."""
        return {'w': (self.n // 2) + 1, 'r': self.n}
    
    def quorum(self):
        """Read from majority. Good balance of consistency/speed."""
        majority = (self.n // 2) + 1
        return {'w': majority, 'r': majority}
    
    def fast_write(self):
        """Write to all, read from one. Fast reads, slow writes."""
        return {'w': self.n, 'r': 1}
    
    def fast_read(self):
        """Write to one, read from all. Fast writes, slow reads."""
        return {'w': 1, 'r': self.n}
    
    def eventual(self):
        """Write to majority, read from one. Fastest reads, possibly stale."""
        return {'w': (self.n // 2) + 1, 'r': 1}

# Visual: Why W + R > N works
#
# N = 5 nodes: [A] [B] [C] [D] [E]
#
# Write to W=3:  [A✓] [B✓] [C✓] [D ] [E ]
#                 ↑     ↑     ↑
#                (write acknowledged by 3 nodes)
#
# Read from R=3:        [B✓] [C✓] [D ] [E✓]
#                        ↑     ↑           ↑
#                       (read from 3 nodes)
#
# Overlap: B and C are in BOTH sets.
# B and C have the latest write.
# We return the value with the highest version number → correct!
#
# If R = 2:       [D ] [E ]
# No overlap with write set → might return stale data!
```

### Component 2: Consistency Manager for DistKV

```python
class ConsistencyManager:
    """
    Adds tunable consistency to our sharded KV store.
    
    This wraps the RaftCluster from Episode 3.2 and
    ShardRouter from Episode 3.3 with consistency options.
    
    Architecture:
    
    Client → ConsistencyManager → ShardRouter → RaftCluster
                    ↓
         Choose read strategy based on consistency level
    """
    
    def __init__(self, router, quorum_config: QuorumConfig):
        self.router = router
        self.config = quorum_config
        self.session_markers: Dict[str, int] = {}  # session → last_write_ts
    
    def read(self, key: str, 
             consistency: ConsistencyLevel = ConsistencyLevel.QUORUM,
             session_id: Optional[str] = None,
             max_staleness_ms: int = 5000) -> Tuple[Optional[str], bool]:
        """
        Read with specified consistency level.
        
        Compare to Episode 3.2 (always strong):
        - Ep 3.2: read → leader → consistent (slow)
        - Ep 3.5: read → choose strategy → trade consistency for speed
        """
        
        if consistency == ConsistencyLevel.STRONG:
            return self._read_strong(key)
        
        elif consistency == ConsistencyLevel.QUORUM:
            return self._read_quorum(key)
        
        elif consistency == ConsistencyLevel.SESSION:
            return self._read_session(key, session_id)
        
        elif consistency == ConsistencyLevel.EVENTUAL:
            return self._read_eventual(key)
        
        elif consistency == ConsistencyLevel.BOUNDED_STALENESS:
            return self._read_bounded(key, max_staleness_ms)
        
        return None, False
    
    def write(self, key: str, value: str,
              consistency: ConsistencyLevel = ConsistencyLevel.QUORUM,
              session_id: Optional[str] = None) -> bool:
        """Write with specified consistency level."""
        
        if consistency == ConsistencyLevel.STRONG:
            return self._write_strong(key, value)
        
        elif consistency == ConsistencyLevel.QUORUM:
            return self._write_quorum(key, value, session_id)
        
        elif consistency == ConsistencyLevel.EVENTUAL:
            return self._write_eventual(key, value, session_id)
        
        return False
    
    def _read_strong(self, key: str):
        """
        Read from Raft leader. Linearizable.
        
        Guarantees: You see the most recent committed write.
        Cost: Must contact leader (potentially cross-datacenter).
        Latency: 1-100ms depending on leader location.
        """
        shard_id = self.router.ring.get_shard(key)
        cluster = self.router.clusters[shard_id]
        leader = cluster.get_leader()
        
        if leader is None:
            # No leader — must wait for election (Episode 3.2)
            return None, False
        
        return leader.client_get(key)
    
    def _read_quorum(self, key: str):
        """
        Read from R nodes, return latest version.
        
        Guarantees: See latest write (if W + R > N).
        Cost: R network calls, compare versions.
        Latency: P50 of R responses (wait for fastest R nodes).
        """
        shard_id = self.router.ring.get_shard(key)
        cluster = self.router.clusters[shard_id]
        
        r = self.config.quorum()['r']
        responses = []
        
        for node_id, node in list(cluster.nodes.items())[:r]:
            value, found = node.store.get(key)
            if found:
                # Include version metadata for comparison
                responses.append({
                    'value': value,
                    'version': node.commit_index,
                    'node_id': node_id
                })
        
        if not responses:
            return None, False
        
        # Return the response with the highest version
        # This is the "majority element" from the LeetCode seed
        latest = max(responses, key=lambda r: r['version'])
        return latest['value'], True
    
    def _read_session(self, key: str, session_id: Optional[str]):
        """
        Session consistency: read-your-writes guarantee.
        
        Guarantees: You see at least as recent as your last write.
        Cost: One read from any node, but check version.
        
        How: Track the version of the last write per session.
        Only accept reads with version >= that marker.
        """
        if not session_id:
            return self._read_eventual(key)
        
        min_version = self.session_markers.get(session_id, 0)
        
        shard_id = self.router.ring.get_shard(key)
        cluster = self.router.clusters[shard_id]
        
        # Try any node, check if version is fresh enough
        for node_id, node in cluster.nodes.items():
            if node.commit_index >= min_version:
                return node.store.get(key)
        
        # No node fresh enough — fall back to leader
        return self._read_strong(key)
    
    def _read_eventual(self, key: str):
        """
        Read from any replica. Fastest possible read.
        
        Guarantees: Data will be consistent "eventually."
        Could be milliseconds stale, could be seconds stale.
        Cost: One network call to nearest node.
        """
        shard_id = self.router.ring.get_shard(key)
        cluster = self.router.clusters[shard_id]
        
        # Read from any node (pick the closest/fastest)
        for node_id, node in cluster.nodes.items():
            return node.store.get(key)
        
        return None, False
    
    def _read_bounded(self, key: str, max_staleness_ms: int):
        """
        Read from replica, but guarantee max staleness.
        
        Guarantees: Data is at most X milliseconds old.
        Cost: Check replica's lag, promote to leader read if too stale.
        
        Used by: Spanner (bounded-staleness reads),
                 Cosmos DB (bounded-staleness consistency level)
        """
        import time
        
        shard_id = self.router.ring.get_shard(key)
        cluster = self.router.clusters[shard_id]
        
        leader = cluster.get_leader()
        leader_commit = leader.commit_index if leader else 0
        
        for node_id, node in cluster.nodes.items():
            # Estimate staleness based on commit index lag
            lag = leader_commit - node.commit_index
            estimated_lag_ms = lag * 10  # ~10ms per entry (rough estimate)
            
            if estimated_lag_ms <= max_staleness_ms:
                return node.store.get(key)
        
        # All replicas too stale — read from leader
        return self._read_strong(key)
    
    def _write_strong(self, key: str, value: str) -> bool:
        """Write through Raft leader with majority ACK."""
        shard_id = self.router.ring.get_shard(key)
        cluster = self.router.clusters[shard_id]
        leader = cluster.get_leader()
        
        if leader:
            return leader.client_put(key, value)
        return False
    
    def _write_quorum(self, key: str, value: str, 
                       session_id: Optional[str]) -> bool:
        """Write with quorum acknowledgment."""
        result = self._write_strong(key, value)
        
        if result and session_id:
            # Track version for session consistency
            shard_id = self.router.ring.get_shard(key)
            cluster = self.router.clusters[shard_id]
            leader = cluster.get_leader()
            if leader:
                self.session_markers[session_id] = leader.commit_index
        
        return result
    
    def _write_eventual(self, key: str, value: str,
                         session_id: Optional[str]) -> bool:
        """
        Write to leader, don't wait for all followers.
        Leader ACKs after local WAL write but before full replication.
        
        Faster but risks data loss if leader crashes before replication.
        """
        return self._write_strong(key, value)  # Still goes through Raft
```

### Component 3: CAP Theorem in Practice

```python
class CAPAnalyzer:
    """
    Maps our consistency levels to CAP theorem choices.
    
    CAP: During a network partition, choose Consistency or Availability.
    
    Our DistKV can choose per-operation:
    - Strong reads → CP (reject if can't reach leader)
    - Eventual reads → AP (serve from any available replica)
    """
    
    def analyze_operation(self, consistency, partition_active):
        """What happens to each consistency level during partition?"""
        
        if not partition_active:
            return "All consistency levels work normally"
        
        analysis = {
            ConsistencyLevel.STRONG: {
                'behavior': 'REJECT read if leader unreachable',
                'cap_choice': 'CP — Consistency over Availability',
                'impact': 'Clients see errors during partition'
            },
            ConsistencyLevel.QUORUM: {
                'behavior': 'Succeed if majority reachable, fail otherwise',
                'cap_choice': 'CP — but more available than STRONG',
                'impact': 'Works if partition is minority'
            },
            ConsistencyLevel.SESSION: {
                'behavior': 'Read from available replica if version OK',
                'cap_choice': 'Leans AP — available with bounded guarantee',
                'impact': 'Works unless session node is on wrong side'
            },
            ConsistencyLevel.EVENTUAL: {
                'behavior': 'Read from ANY available replica',
                'cap_choice': 'AP — Availability over Consistency',
                'impact': 'Always works, may return stale data'
            },
            ConsistencyLevel.BOUNDED_STALENESS: {
                'behavior': 'Read from replica if staleness within bounds',
                'cap_choice': 'Between CP and AP',
                'impact': 'Works until bounds exceeded, then fails'
            }
        }
        
        return analysis.get(consistency)
```

---

## 5. Failure Modes

### Failure Mode 1: Stale Read from Dethroned Leader

```python
# Scenario: Leader doesn't know it's been replaced
#
# Timeline:
# T=0: Node A is leader, serving reads
# T=1: Network partition isolates A from B, C
# T=2: B, C elect new leader (B)
# T=3: Client writes to B (new leader)
# T=4: Client reads from A (old leader, doesn't know it's deposed)
# T=5: A returns STALE data!
#
# Fix: Leader lease / leadership confirmation

class LeaderLease:
    """
    Leader confirms it's still leader before serving reads.
    
    Approach 1: Send heartbeat, wait for majority ACK
    Approach 2: Time-based lease (leader is valid for T seconds)
    
    Trade-off:
    - Heartbeat: Stronger but adds latency (1 RTT per read)
    - Lease: Faster but depends on clock synchronization
    """
    
    def __init__(self, lease_duration_ms=5000):
        self.lease_duration = lease_duration_ms / 1000.0
        self.lease_expiry = 0
    
    def refresh_lease(self):
        """Called after successful heartbeat to majority."""
        import time
        self.lease_expiry = time.time() + self.lease_duration
    
    def is_valid(self):
        """Check if this leader's lease is still valid."""
        import time
        return time.time() < self.lease_expiry
    
    def read_with_lease(self, store, key):
        """Only serve reads if lease is valid."""
        if not self.is_valid():
            raise Exception("Leader lease expired — may not be leader")
        return store.get(key)
```

### Failure Mode 2: Monotonic Read Violation

```python
# Scenario: Client reads from different replicas consecutively
#
# Timeline:
# T=0: Write key="likes" value=100 (replicated to A, B, not yet C)
# T=1: Client reads from B → sees 100
# T=2: Client reads from C → sees 99 (stale!)
# T=3: Client confused: "Likes went DOWN?!"
#
# Fix: Monotonic reads — client always reads from same replica
# or requires version ≥ last seen version

class MonotonicReadGuard:
    """
    Ensure reads never go backward.
    
    Client tracks the version of the last read.
    Subsequent reads must return version ≥ previous.
    """
    
    def __init__(self):
        self.last_seen_version: Dict[str, int] = {}  # key → last version
    
    def read_monotonic(self, key, consistency_manager):
        """Read with monotonic read guarantee."""
        min_version = self.last_seen_version.get(key, 0)
        
        value, found, version = consistency_manager.read_with_version(key)
        
        if found and version < min_version:
            # Stale! Must read from a more up-to-date node
            value, found = consistency_manager.read(
                key, ConsistencyLevel.STRONG
            )
            version = float('inf')  # Strong read is always latest
        
        if found:
            self.last_seen_version[key] = version
        
        return value, found
```

---

## 6. How Real Systems Choose Consistency

```python
real_systems = {
    "DynamoDB": {
        "Options": "Eventually consistent (default), Strongly consistent",
        "Default": "Eventual (single-digit ms latency)",
        "Strong cost": "2x read capacity units, higher latency",
        "Mapping": "Our EVENTUAL vs STRONG levels"
    },
    "Cosmos DB": {
        "Options": "Strong, Bounded Staleness, Session, Consistent Prefix, Eventual",
        "Default": "Session consistency",
        "Unique": "5-level spectrum, most granular in production",
        "Mapping": "Matches our 5 consistency levels almost exactly"
    },
    "Cassandra": {
        "Options": "ONE, QUORUM, ALL, LOCAL_QUORUM, EACH_QUORUM",
        "Default": "ONE (eventual)",
        "Unique": "Per-query consistency, tunable W and R separately",
        "Mapping": "Our quorum config with W/R controls"
    },
    "CockroachDB": {
        "Options": "Serializable only (no weaker levels)",
        "Why": "Designed for correctness-critical workloads",
        "Trade-off": "Higher latency, no consistency tuning",
        "Mapping": "Our STRONG level only"
    }
}
```

---

## 7. Interview Cheatsheet

```python
interview_qa = {
    "Q: Explain CAP theorem with an example":
    "A: During a partition between nodes A and B: "
    "CP (consistency): reject reads if leader unreachable. "
    "AP (availability): serve reads from any node, might be stale. "
    "Our DistKV lets clients choose per operation.",
    
    "Q: What's the quorum formula?":
    "A: W + R > N ensures overlap. N=3, W=2, R=2 → at least 1 node "
    "in every read saw the latest write. That node's value wins.",
    
    "Q: Session consistency — what does it guarantee?":
    "A: Read-your-writes: after a write, your subsequent reads "
    "see at least that write. Other clients may see stale data. "
    "Implemented by tracking write version per session.",
    
    "Q: When is eventual consistency acceptable?":
    "A: When temporary staleness has low business impact: "
    "social media likes, search results, CDN content, analytics. "
    "NOT acceptable for: bank balances, inventory, auctions.",
    
    "Q: How does bounded staleness work?":
    "A: Read from any replica, but guarantee data is at most X ms old. "
    "Track replication lag per replica. If lag > bound, redirect to leader. "
    "Cosmos DB offers this as a first-class consistency level."
}
```

---

## 8. What's Next

```
WHAT YOU BUILT TODAY:
DistKV v5 — tunable consistency:
- STRONG: Read from leader (linearizable)
- QUORUM: Read from majority (W + R > N)
- SESSION: Read-your-writes per session
- BOUNDED_STALENESS: At most X ms stale
- EVENTUAL: Read from any replica (fastest)

Clients choose per operation:
  manager.read("likes:post1", ConsistencyLevel.EVENTUAL)  # Fast
  manager.read("balance:alice", ConsistencyLevel.STRONG)    # Correct

BUT: With eventual consistency, replicas can diverge.
     Two clients write to the same key simultaneously
     on different replicas. When replicas sync —
     WHICH version wins?
     
     Last-write-wins by wall clock? Clocks drift.
     Pick the replica with higher ID? Arbitrary.
     Keep both versions? Now the client must merge.

NEXT: Episode 3.6 — Conflict Resolution: When Replicas Disagree
We'll add vector clocks for causal ordering and CRDTs for
automatic, mathematically guaranteed conflict resolution.

The question changes from "How do we read stale data safely?"
to "How do we resolve conflicts when replicas diverge?"
```

---

*"The right consistency level isn't 'the strongest one.' It's the weakest one that your business can tolerate — because every level of consistency you DON'T need is latency you save."*
