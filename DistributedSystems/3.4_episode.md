# Episode 3.4 â€” Distributed Transactions: ACID Across Shards
## From Merge K Sorted Lists to Percolator-Style Commits

**Season 3 â€” Distributed Systems: Building DistKV**

---

## Previously on DistKV...

In Episode 3.1, we built a single-node KV store. In 3.2, we added Raft replication for fault tolerance. In 3.3, we sharded data across multiple Raft groups using consistent hashing â€” write throughput now scales linearly.

**But we broke atomicity.** A bank transfer â€” debit account A on shard 1, credit account B on shard 2 â€” is TWO separate writes to TWO separate Raft groups. If we crash between them, money vanishes. If shard 2's write fails but shard 1's succeeds, the accounts are inconsistent forever.

**Today's fix:** Add a transaction layer that coordinates cross-shard writes. All participating shards commit â€” or all roll back. No partial updates.

**Arc So Far:**
- **3.1**: Single-node KV store âœ…
- **3.2**: Raft replication âœ…
- **3.3**: Sharding âœ…
- **3.4**: Distributed transactions â† YOU ARE HERE
- **3.5**: Consistency models (coming next)

---

## 1. The Hook: Real-World Production Failure

### The 2018 Coinbase Transaction Duplication
**"$100M+ in erroneous transactions due to broken atomic commits"**

**The Incident:**
1. Coinbase built a homegrown 2PC (Two-Phase Commit) for crypto transfers
2. During a network partition: coordinator timed out â†’ aborted transaction
3. But one participant shard had already committed locally
4. Result: Money appeared in recipient's account without being debited from sender
5. Double-spend: the worst possible failure in financial systems

**The Critical Insight:** 2PC's blocking nature creates a fundamental tradeoff:
- Wait for timeout? â†’ User sees 30-second latency
- Proceed without timeout? â†’ Risk double-spend
- Use non-blocking protocol? â†’ More complex, but solvable

**The Lesson:** Distributed transactions need failure RECOVERY, not just happy-path correctness. The hard part isn't committing â€” it's recovering from crashes mid-commit.

---

## 2. The LeetCode Seed: Merge K Sorted Lists

```python
# LeetCode #23: Merge K Sorted Lists
# Real-world: Ordering commits across shards

import heapq

def mergeKLists(lists):
    """
    Merge K sorted lists into one sorted list.
    
    This IS the distributed transaction ordering problem:
    - Each list = one shard's local commit log
    - Sorted order = timestamp ordering
    - Merged list = global transaction order
    - Heap = coordinator that picks the next commit
    
    The challenge: each shard sees its own commits in order,
    but the GLOBAL order across shards must also be consistent.
    """
    heap = []
    
    for shard_id, lst in enumerate(lists):
        if lst:
            heapq.heappush(heap, (lst[0].val, shard_id, 0, lst[0]))
    
    result = []
    
    while heap:
        val, shard_id, idx, node = heapq.heappop(heap)
        result.append(node)
        
        # Advance this shard's pointer
        if node.next:
            heapq.heappush(heap, (node.next.val, shard_id, idx + 1, node.next))
    
    return result

# The mapping to distributed transactions:
#
# K Sorted Lists              Distributed Transactions
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€               â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Each list                   Each shard's commit log
# List ordering               Local timestamp ordering
# Heap comparison             Global timestamp from oracle
# Merged output               Globally ordered commit history
# All elements included       All shards must agree (consensus)
#
# Key insight: Just as merge needs all lists to participate,
# distributed commits need all shards to participate.
# If one list is unavailable â†’ merge is stuck (blocking).
# This is exactly the 2PC blocking problem.
```

---

## 3. The Transaction Protocol Spectrum

Before building, let's understand the three major approaches:

```
Protocol     | Consistency       | Latency    | Blocking? | Use Case
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€|â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€|â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€|â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€|â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2PC          | Serializable      | 2 RTTs     | YES       | Banking
Percolator   | Snapshot Isolation | 1 RTT + TS | NO        | Google Search index
Sagas        | Eventual          | Variable   | NO        | E-commerce workflows

2PC (Two-Phase Commit):
  Coordinator â†’ "Prepare?" â†’ All participants
  All say YES â†’ Coordinator â†’ "Commit" â†’ All participants
  If coordinator crashes between phases â†’ EVERYONE BLOCKS

Percolator (Google's approach):
  Client â†’ Get timestamp â†’ Write with locks â†’ Get commit TS
  â†’ Commit PRIMARY key first â†’ Commit secondaries
  INNOVATION: Primary lock makes crash recovery trivial (O(1) check)

Sagas (Compensating Actions):
  Step 1 â†’ Step 2 â†’ Step 3 â†’ ... 
  If Step 3 fails â†’ Compensate Step 2 â†’ Compensate Step 1
  No atomicity, but no blocking either
```

---

## 4. Production System Build

### Component 1: Timestamp Oracle

```python
import threading
import time

class TimestampOracle:
    """
    Single source of truth for transaction timestamps.
    
    Why? Without a global clock, we can't order transactions
    across shards. Each shard's local clock drifts differently.
    
    The timestamp oracle provides monotonically increasing IDs
    that serve as logical timestamps for transaction ordering.
    
    In production:
    - Google Spanner uses TrueTime (GPS/atomic clocks) â€” Episode 3.7
    - CockroachDB uses Hybrid Logical Clocks â€” Episode 3.7
    - TiDB uses a dedicated PD (Placement Driver) timestamp server
    """
    
    def __init__(self):
        self.counter = 0
        self.lock = threading.Lock()
    
    def get_timestamp(self) -> int:
        """Get a new monotonically increasing timestamp."""
        with self.lock:
            self.counter += 1
            return self.counter
    
    def get_commit_timestamp(self, start_ts: int) -> int:
        """
        Commit timestamp must be > start timestamp.
        This ensures later transactions see earlier commits.
        """
        with self.lock:
            self.counter = max(self.counter, start_ts) + 1
            return self.counter
```

### Component 2: Two-Phase Commit (2PC)

```python
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Set
from enum import Enum

class TxState(Enum):
    ACTIVE = "active"
    PREPARING = "preparing"
    COMMITTED = "committed"
    ABORTED = "aborted"

@dataclass
class TransactionRecord:
    tx_id: int
    state: TxState
    participants: List[str]  # Shard IDs involved
    writes: Dict[str, Dict[str, str]]  # shard_id â†’ {key: value}

class TwoPhaseCommitCoordinator:
    """
    Classic 2PC coordinator.
    
    Layered on top of our sharded KV store from Episode 3.3:
    - Each shard is a participant
    - Coordinator manages the protocol
    - Our Raft groups (Episode 3.2) handle intra-shard durability
    
    Architecture:
    
    Client â†’ Coordinator â†’ Phase 1: Prepare all shards
                         â†’ Phase 2: Commit all shards (if all said YES)
                         â†’ Abort all shards (if any said NO)
    """
    
    def __init__(self, router, ts_oracle: TimestampOracle):
        self.router = router  # ShardRouter from Episode 3.3
        self.ts_oracle = ts_oracle
        self.transactions: Dict[int, TransactionRecord] = {}
        self.wal_path = '/tmp/distkv/coordinator_wal.log'
    
    def begin_transaction(self) -> int:
        """Start a new distributed transaction."""
        tx_id = self.ts_oracle.get_timestamp()
        self.transactions[tx_id] = TransactionRecord(
            tx_id=tx_id,
            state=TxState.ACTIVE,
            participants=[],
            writes={}
        )
        return tx_id
    
    def add_write(self, tx_id: int, key: str, value: str):
        """Buffer a write in the transaction."""
        tx = self.transactions[tx_id]
        shard_id = self.router.ring.get_shard(key)
        
        if shard_id not in tx.writes:
            tx.writes[shard_id] = {}
            tx.participants.append(shard_id)
        
        tx.writes[shard_id][key] = value
    
    def commit(self, tx_id: int) -> bool:
        """
        Execute 2PC commit protocol.
        
        Phase 1 (Prepare):
          Ask each participant: "Can you commit these writes?"
          Participants: Lock keys, validate, write to WAL, respond YES/NO
          
        Phase 2 (Commit or Abort):
          If ALL said YES â†’ tell all to commit
          If ANY said NO â†’ tell all to abort
        
        THE PROBLEM: If coordinator crashes between Phase 1 and Phase 2,
        all participants are STUCK. They've locked keys but don't know
        whether to commit or abort. This is the "blocking" problem.
        """
        tx = self.transactions[tx_id]
        
        # Phase 1: Prepare
        tx.state = TxState.PREPARING
        self._log_state(tx)  # WAL: record we're preparing
        
        prepare_results = {}
        for shard_id in tx.participants:
            result = self._send_prepare(shard_id, tx_id, tx.writes[shard_id])
            prepare_results[shard_id] = result
        
        # Decision
        all_prepared = all(prepare_results.values())
        
        if all_prepared:
            # Phase 2: Commit
            tx.state = TxState.COMMITTED
            self._log_state(tx)  # WAL: record commit decision
            
            for shard_id in tx.participants:
                self._send_commit(shard_id, tx_id)
            
            return True
        else:
            # Phase 2: Abort
            tx.state = TxState.ABORTED
            self._log_state(tx)
            
            for shard_id in tx.participants:
                self._send_abort(shard_id, tx_id)
            
            return False
    
    def _send_prepare(self, shard_id, tx_id, writes) -> bool:
        """Ask a shard to prepare (lock keys, validate)."""
        # In production: RPC to shard leader
        # Shard: acquires locks, writes to WAL, responds
        return True  # Simplified
    
    def _send_commit(self, shard_id, tx_id):
        """Tell a shard to commit (apply writes, release locks)."""
        pass
    
    def _send_abort(self, shard_id, tx_id):
        """Tell a shard to abort (discard writes, release locks)."""
        pass
    
    def _log_state(self, tx):
        """Log transaction state to coordinator WAL."""
        # Critical for crash recovery:
        # If coordinator crashes after logging COMMITTED,
        # recovery knows to push commit to all participants.
        pass

# The 2PC blocking problem visualized:
#
# Time    Coordinator     Shard A          Shard B
# T=0     Prepare A,B     â†’ Lock keys      â†’ Lock keys
# T=1     Both said YES   â† YES            â† YES
# T=2     Log COMMITTED   (writing to WAL...)
# T=3     ðŸ’¥ CRASH!       WAITING...       WAITING...
#
# Now: A and B hold locks but don't know the decision.
# They can't commit (coordinator might have decided abort).
# They can't abort (coordinator might have decided commit).
# Keys are LOCKED FOREVER until coordinator recovers.
#
# This is why Google invented Percolator.
```

### Component 3: Percolator Transaction (Non-Blocking)

```python
class PercolatorTransaction:
    """
    Google's Percolator: Optimistic, non-blocking transactions.
    
    KEY INNOVATION: Primary lock.
    
    Among all keys in the transaction, designate ONE as "primary."
    Commit timestamp is written to the primary key FIRST.
    
    During crash recovery: Check primary's state â†’ know entire 
    transaction's outcome. Recovery is O(1), not O(N).
    
    This solves 2PC's blocking problem:
    - 2PC: Coordinator crash = participants stuck forever
    - Percolator: Primary key IS the coordinator state
    - Any node can check primary and resolve the transaction
    
    Used by: TiDB (TiKV), CockroachDB (write intents)
    """
    
    def __init__(self, ts_oracle: TimestampOracle, storage):
        self.ts_oracle = ts_oracle
        self.storage = storage  # Our sharded store from Episode 3.3
        self.start_ts = ts_oracle.get_timestamp()
        self.writes = {}  # key â†’ value
        self.read_set = {}  # key â†’ value read during transaction
        self.primary_key = None
    
    def get(self, key: str):
        """
        Read a key at our start timestamp (snapshot read).
        
        MVCC (Multi-Version Concurrency Control):
        Find the latest version committed BEFORE our start_ts.
        This gives us a consistent snapshot â€” we see the database
        as it was at one point in time.
        """
        # Find latest committed version â‰¤ start_ts
        version = self.storage.get_version_at(key, self.start_ts)
        
        if version:
            self.read_set[key] = version.value
            return version.value
        return None
    
    def put(self, key: str, value: str):
        """Buffer a write (applied at commit time)."""
        self.writes[key] = value
    
    def commit(self) -> bool:
        """
        Two-phase commit WITHOUT blocking.
        
        Phase 1 (Prewrite): Write all keys with locks.
            - Locks make writes invisible to other transactions.
            - Conflict detection happens here (optimistic).
            
        Phase 2 (Commit): Remove locks, make writes visible.
            - Primary key committed first (single point of truth).
            - Secondary keys committed afterward.
            - If crash during secondaries â†’ recovery fixes them.
        """
        if not self.writes:
            return True
        
        # Choose primary key (any key in write set)
        self.primary_key = next(iter(self.writes))
        
        # â”€â”€ Phase 1: Prewrite â”€â”€
        # Write all keys with locks (invisible to other transactions)
        
        # Prewrite primary first
        if not self._prewrite(self.primary_key, self.writes[self.primary_key], 
                              is_primary=True):
            self._cleanup()
            return False
        
        # Prewrite secondaries
        for key, value in self.writes.items():
            if key == self.primary_key:
                continue
            if not self._prewrite(key, value, is_primary=False):
                self._cleanup()
                return False
        
        # â”€â”€ Get commit timestamp â”€â”€
        commit_ts = self.ts_oracle.get_commit_timestamp(self.start_ts)
        
        # â”€â”€ Phase 2: Commit â”€â”€
        # Commit primary FIRST (this is the atomic commit point)
        if not self._commit_primary(commit_ts):
            self._cleanup()
            return False
        
        # After primary commits, the transaction is committed.
        # Secondary commits can fail â€” recovery will fix them.
        for key in self.writes:
            if key != self.primary_key:
                self._commit_secondary(key, commit_ts)
                # Note: failure here is OK â€” recovery handles it
        
        return True
    
    def _prewrite(self, key: str, value: str, is_primary: bool) -> bool:
        """
        Write data + lock. Detect conflicts.
        
        Conflict cases (optimistic concurrency control):
        1. Another transaction has a lock on this key
           â†’ Abort (or wait if the lock is old/stale)
        2. Another transaction committed a write after our start_ts
           â†’ Abort (write-write conflict)
        """
        # Check for existing lock (another in-progress transaction)
        existing_lock = self.storage.get_lock(key)
        if existing_lock:
            if existing_lock.ts < self.start_ts:
                # Stale lock â€” old transaction might have crashed
                self._resolve_stale_lock(existing_lock, key)
            else:
                # Newer transaction â€” we should abort
                return False
        
        # Check for write-write conflict
        latest_commit = self.storage.get_latest_commit_after(key, self.start_ts)
        if latest_commit:
            # Someone committed a new version after our snapshot
            return False
        
        # Write data + lock
        self.storage.write_with_lock(
            key=key,
            value=value,
            start_ts=self.start_ts,
            primary_key=self.primary_key,
            is_primary=is_primary
        )
        
        return True
    
    def _commit_primary(self, commit_ts: int) -> bool:
        """
        Commit the primary key.
        
        THIS IS THE ATOMIC COMMIT POINT.
        Before: Transaction is uncommitted.
        After: Transaction is committed.
        
        The lock on the primary key is replaced with a commit record.
        """
        return self.storage.commit_lock(
            key=self.primary_key,
            start_ts=self.start_ts,
            commit_ts=commit_ts
        )
    
    def _commit_secondary(self, key: str, commit_ts: int):
        """Commit a secondary key (replace lock with commit record)."""
        self.storage.commit_lock(
            key=key,
            start_ts=self.start_ts,
            commit_ts=commit_ts
        )
    
    def _resolve_stale_lock(self, lock, key):
        """
        Resolve a lock left by a crashed transaction.
        
        THIS IS THE MAGIC: Check the primary key.
        - Primary committed â†’ push commit to this key too
        - Primary NOT committed â†’ transaction aborted, clean up lock
        
        O(1) recovery: Check ONE key to know entire transaction's fate.
        """
        primary_key = lock.primary_key
        primary_committed = self.storage.is_committed(primary_key, lock.ts)
        
        if primary_committed:
            commit_ts = self.storage.get_commit_ts(primary_key, lock.ts)
            self.storage.commit_lock(key, lock.ts, commit_ts)
        else:
            self.storage.remove_lock(key, lock.ts)
    
    def _cleanup(self):
        """Remove all locks written by this transaction."""
        for key in self.writes:
            self.storage.remove_lock(key, self.start_ts)
```

### Component 4: Saga Pattern for Long-Running Workflows

```python
class OrderSaga:
    """
    For operations that span services (not just shards):
    E-commerce order = inventory service + payment service + shipping service
    
    Sagas use COMPENSATING ACTIONS instead of rollback:
    If Step 3 fails â†’ undo Step 2 â†’ undo Step 1
    
    No locking, no blocking, no 2PC coordinator.
    Trade-off: Eventual consistency (not ACID atomicity).
    
    Used by: Amazon, Uber, Airbnb for business workflows
    """
    
    def __init__(self):
        self.steps = []
        self.executed = []
    
    def add_step(self, action, compensation, description):
        """Add a step with its compensation (undo) action."""
        self.steps.append({
            'action': action,
            'compensation': compensation,
            'description': description
        })
    
    def execute(self) -> bool:
        """
        Execute all steps. On failure, compensate in reverse.
        
        Example: Create Order
        1. Reserve inventory    â†” Release inventory
        2. Charge credit card   â†” Refund payment
        3. Create shipping      â†” Cancel shipping
        4. Send confirmation    â†” (no compensation needed)
        """
        for step in self.steps:
            try:
                step['action']()
                self.executed.append(step)
            except Exception as e:
                print(f"[Saga] Step failed: {step['description']}: {e}")
                self._compensate()
                return False
        
        return True
    
    def _compensate(self):
        """Undo executed steps in reverse order."""
        for step in reversed(self.executed):
            if step['compensation']:
                try:
                    step['compensation']()
                except Exception as e:
                    # Compensation failed â€” needs manual intervention
                    print(f"[Saga] CRITICAL: Compensation failed: {e}")
                    # In production: alert on-call, log for manual fix
```

### Component 5: Transaction Layer for DistKV

```python
class DistKVTransactionManager:
    """
    Transaction manager layered on our sharded KV store.
    
    Chooses protocol based on transaction characteristics:
    - Single-shard â†’ local Raft commit (fast, no coordination)
    - Cross-shard, short-lived â†’ Percolator (non-blocking)
    - Cross-shard, needs serializable â†’ 2PC (blocking but safe)
    - Cross-service, long-running â†’ Saga (eventual consistency)
    """
    
    def __init__(self, router, ts_oracle):
        self.router = router
        self.ts_oracle = ts_oracle
    
    def execute_transaction(self, operations):
        """
        Smart routing: choose protocol based on workload.
        
        This is the integration point with Episodes 3.1-3.3.
        """
        # Determine which shards are involved
        shards = set()
        for op in operations:
            shard = self.router.ring.get_shard(op['key'])
            shards.add(shard)
        
        if len(shards) == 1:
            # Single-shard: just use Raft directly (Episode 3.2)
            return self._single_shard_commit(operations, shards.pop())
        
        elif all(op.get('type') != 'long_running' for op in operations):
            # Cross-shard, short-lived: Percolator (non-blocking)
            return self._percolator_commit(operations)
        
        else:
            # Long-running: Saga
            return self._saga_commit(operations)
    
    def _single_shard_commit(self, operations, shard_id):
        """Commit within one Raft group â€” no coordination needed."""
        cluster = self.router.clusters[shard_id]
        leader = cluster.get_leader()
        
        for op in operations:
            if op['type'] == 'put':
                leader.client_put(op['key'], op['value'])
        
        return True
    
    def _percolator_commit(self, operations):
        """Cross-shard commit using Percolator protocol."""
        tx = PercolatorTransaction(self.ts_oracle, self.router)
        
        for op in operations:
            if op['type'] == 'put':
                tx.put(op['key'], op['value'])
            elif op['type'] == 'get':
                tx.get(op['key'])
        
        return tx.commit()
    
    def _saga_commit(self, operations):
        """Long-running workflow using Sagas."""
        saga = OrderSaga()
        # Convert operations to saga steps with compensations
        return saga.execute()
```

---

## 5. Failure Modes

### Failure Mode 1: 2PC Coordinator Crash

```python
# THE most critical failure in distributed transactions.
# Participants hold locks, waiting for commit/abort decision.
#
# Recovery approach: Coordinator WAL
# 1. If WAL says COMMITTED â†’ push commit to all participants
# 2. If WAL says PREPARING (but no COMMITTED) â†’ abort all
# 3. This is why coordinator logs the decision BEFORE telling participants

class CoordinatorRecovery:
    def recover(self, wal_entries):
        for tx in wal_entries:
            if tx.state == TxState.COMMITTED:
                # Push commit â€” some participants may not have heard
                for shard in tx.participants:
                    self._send_commit(shard, tx.tx_id)
            elif tx.state == TxState.PREPARING:
                # Never committed â€” safe to abort
                for shard in tx.participants:
                    self._send_abort(shard, tx.tx_id)
```

### Failure Mode 2: Write-Write Conflict in Percolator

```python
# Two transactions try to write the same key concurrently.
#
# Timeline:
# T=0: Tx1 starts (start_ts=10), reads key A=100
# T=1: Tx2 starts (start_ts=20), reads key A=100
# T=2: Tx1 prewrites A=90 (succeeds, gets lock)
# T=3: Tx2 tries to prewrite A=80 (FAILS â€” sees Tx1's lock)
# T=4: Tx2 aborts, retries with new start_ts=30
# T=5: Tx1 commits (commit_ts=25)
# T=6: Tx2 retry: reads A=90, prewrites A=80 (succeeds)
#
# Key insight: Optimistic concurrency detects conflicts at WRITE time.
# Low contention â†’ few aborts â†’ high throughput (Google Search index)
# High contention â†’ many aborts â†’ use 2PC instead (banking)
```

### Failure Mode 3: Clock Skew Breaking Ordering

```python
# Timestamps from our oracle are logical, not physical.
# But if we used wall clocks instead:
#
# Server A clock: 10:00:00.000
# Server B clock: 10:00:00.500 (500ms ahead)
#
# Tx1 on A at "10:00:01.000" â†’ timestamp 1000
# Tx2 on B at "10:00:00.800" â†’ timestamp 800
#
# Tx2 has LOWER timestamp despite happening LATER in real time!
# If Tx2 reads Tx1's writes, causality is violated.
#
# Our centralized TimestampOracle avoids this â€” single source of truth.
# Episode 3.7 will explore TrueTime and HLC for decentralized timestamps.
```

---

## 6. MVCC Visibility Rules

```python
# How Percolator decides what a transaction can "see":
#
# Key X version history:
# write_ts=10, value="A", commit_ts=20  (Committed)
# write_ts=30, value="B", commit_ts=40  (Committed)
# write_ts=50, value="C", commit_ts=?   (Lock â€” in-progress)

def what_transaction_sees(start_ts):
    """
    MVCC Visibility: A transaction at start_ts sees the latest
    version whose commit_ts â‰¤ start_ts.
    
    Locked (uncommitted) versions are INVISIBLE.
    """
    if start_ts == 25:
        return "A"   # write_ts=10, commit_ts=20 â‰¤ 25 âœ“
    
    if start_ts == 45:
        return "B"   # write_ts=30, commit_ts=40 â‰¤ 45 âœ“
    
    if start_ts == 55:
        return "B"   # write_ts=50 is locked (not committed) â†’ invisible
    
    if start_ts == 65:
        # If C committed at 60: return "C" (commit_ts=60 â‰¤ 65)
        # If C still locked: return "B"
        pass

# Key insight: Each transaction sees a CONSISTENT SNAPSHOT.
# No matter how long the transaction runs, it always sees
# the database as it was at start_ts. This is Snapshot Isolation.
```

---

## 7. Interview Cheatsheet

```python
interview_qa = {
    "Q: 2PC vs Percolator â€” when to use which?":
    "A: 2PC for serializable isolation (banking, financial). "
    "Percolator for snapshot isolation (analytics, indexing). "
    "2PC blocks on coordinator failure; Percolator doesn't.",
    
    "Q: What's the primary lock optimization?":
    "A: Designate one key as 'primary.' Commit to primary first. "
    "During recovery, check ONE key to know entire transaction state. "
    "O(1) recovery vs O(N) for checking all keys.",
    
    "Q: How does Percolator handle conflicts?":
    "A: Optimistic: write conflicts detected at prewrite time. "
    "If another transaction has a lock â†’ abort and retry. "
    "Good for low contention (Google Search). Bad for hot keys.",
    
    "Q: When would you use Sagas?":
    "A: Long-running workflows across services: e-commerce orders, "
    "travel booking (flight + hotel + car). Use compensation instead "
    "of rollback. Eventual consistency, not ACID.",
    
    "Q: How does MVCC work?":
    "A: Multiple versions per key, each with a timestamp. "
    "Transactions read the latest version committed before their "
    "start timestamp. Gives consistent snapshots without locking reads.",
    
    "Q: What isolation level does Percolator provide?":
    "A: Snapshot Isolation. Prevents dirty reads and non-repeatable "
    "reads. Does NOT prevent write skew (for that, need serializable "
    "like CockroachDB's read-refresh â€” Episode 3.5)."
}
```

### Production Checklist

```markdown
âœ… Timestamp oracle for global ordering
âœ… MVCC for snapshot isolation
âœ… Percolator protocol with primary lock optimization
âœ… 2PC for serializable transactions (when needed)
âœ… Saga pattern for long-running workflows
âœ… Smart protocol selection based on workload
âœ… Coordinator WAL for crash recovery (2PC)
âœ… O(1) crash recovery via primary lock (Percolator)
âœ… Write-write conflict detection
âœ… Stale lock resolution
```

---

## 8. What's Next

```
WHAT YOU BUILT TODAY:
DistKV v4 â€” transactions across shards:
- Percolator: non-blocking cross-shard commits (default)
- 2PC: serializable isolation when needed
- Sagas: long-running business workflows
- MVCC: snapshot isolation for reads
- Smart routing: single-shard ops skip coordination

COMPARE TO PREVIOUS EPISODES:
3.1: PUT â†’ WAL â†’ Memtable (single node)
3.2: PUT â†’ Raft â†’ Majority ACK â†’ Apply (replicated)
3.3: PUT â†’ Hash â†’ Shard â†’ Raft â†’ Apply (sharded)
3.4: BEGIN TX â†’ PUT A (shard 1) â†’ PUT B (shard 2) â†’ COMMIT
     â†’ Percolator prewrite both â†’ commit primary â†’ commit secondary

Bank transfer now works:
  tx.put("account:alice", balance - 100)  â†’ shard 1
  tx.put("account:bob", balance + 100)    â†’ shard 2
  tx.commit()  # Both succeed or both roll back. Money never disappears.

BUT: Every read goes to the leader (strong consistency).
     For a social media "likes" counter, that's overkill.
     For analytics queries across all shards, it's too slow.
     
     We need TUNABLE consistency â€” strong when it matters,
     eventual when performance matters more.

NEXT: Episode 3.5 â€” Consistency Models: Choose Your Guarantee
We'll add quorum reads/writes (R+W>N), bounded staleness,
and let clients choose their consistency level per operation.

The question changes from "How do we keep data correct?"
to "How CORRECT does each read actually need to be?"
```

---

*"The hardest part of distributed transactions isn't committing â€” it's recovering from crashes mid-commit. Percolator solved this with a single key."*
