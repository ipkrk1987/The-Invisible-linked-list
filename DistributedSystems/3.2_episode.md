# Episode 3.2 — Replication: Never Lose Data
## From BFS Propagation to Raft Log Replication

**Season 3 — Distributed Systems: Building DistKV**

---

## Previously on DistKV...

In Episode 3.1, we built **DistKV v1** — a single-node key-value store with:
- Write-Ahead Log for durability
- In-memory memtable for fast reads
- TCP server for client access
- Crash recovery via WAL replay

**The fatal flaw:** If the machine dies, everything is gone. Today we fix that by replicating our WAL across multiple machines using the Raft consensus protocol.

**Arc So Far:**
- **3.1**: Single-node KV store ✅
- **3.2**: Replication ← YOU ARE HERE
- **3.3**: Sharding (coming next)

---

## 1. The Hook: Real-World Production Failure

### GitLab, January 31, 2017
**"Accidentally deleted production data. Replication propagated the deletion everywhere."**

**The Incident:**
1. An engineer ran `rm -rf` on a PostgreSQL data directory during maintenance
2. Replication faithfully copied the deletion to all replicas
3. Five backup systems had been silently failing for months
4. 6 hours of production data — gone permanently
5. 300GB of data, serving millions of developers, irrecoverable

**The Critical Insight:**
Replication ≠ Backup. Replication propagates ALL changes — including catastrophic mistakes. Without a consensus protocol that verifies which changes are legitimate, replication is a time bomb.

**What GitLab needed (and later adopted):**
- Multi-node Raft replication where destructive operations require majority agreement
- Point-in-time recovery from immutable snapshots
- Tested backup verification (not just "backup exists")

**The Lesson:** Copying data across machines is easy. Knowing WHICH copy is the truth — that's the engineering challenge Raft solves.

---

## 2. The LeetCode Seed: BFS Flood Fill

```python
# LeetCode #733: Flood Fill
# Core insight: BFS propagates state from a source to all reachable nodes

import collections

def floodFill(image, sr, sc, newColor):
    """
    Starting from (sr, sc), change all connected pixels 
    of the same color to newColor.
    
    This IS leader-based replication:
    - Starting pixel = Leader node
    - Connected pixels = Follower replicas  
    - Color change = Log entry (write/update/delete)
    - BFS queue = Replication send buffer
    - Boundary check = Term/index consistency check
    """
    rows, cols = len(image), len(image[0])
    original = image[sr][sc]
    if original == newColor:
        return image
    
    queue = collections.deque([(sr, sc)])
    visited = set()
    
    while queue:
        r, c = queue.popleft()
        if (r, c) in visited:
            continue
        visited.add((r, c))
        
        image[r][c] = newColor  # ← This IS log replication
        
        for dr, dc in [(1,0),(-1,0),(0,1),(0,-1)]:
            nr, nc = r + dr, c + dc
            if (0 <= nr < rows and 0 <= nc < cols 
                and image[nr][nc] == original
                and (nr, nc) not in visited):
                queue.append((nr, nc))
    
    return image

# The gap between BFS and Raft:
#
# BFS PROPAGATION              RAFT REPLICATION
# ─────────────────           ─────────────────
# Fire and forget              Acknowledged (majority ACK)
# No ordering guarantee        Strict log ordering
# Any node can start           Only leader writes
# Success = all painted        Success = majority replicated
# No failure handling          Term/index safety checks
# One-time operation           Continuous replication stream
#
# BFS gives us the INTUITION. Raft gives us the GUARANTEES.
```

---

## 3. Building Replicated DistKV: Adding Raft

### The Architecture Change

```
EPISODE 3.1 (Single Node):

  Client → WAL → Memtable → SSTable

EPISODE 3.2 (Replicated — NEW):

  Client → Leader → Raft Log (replicate to followers)
                          ↓ (majority ACK)
                   Apply to WAL → Memtable → SSTable
                   
  The Raft log BECOMES the WAL. Every entry is:
  1. Written to local log
  2. Sent to all followers  
  3. Committed only when majority acknowledges
  4. Then applied to the state machine (our KV store from 3.1)
```

### Component 1: The Raft Log Entry

```python
from dataclasses import dataclass, field
from typing import Optional, List, Dict, Any
from enum import Enum
import time
import random
import threading
import hashlib

class NodeRole(Enum):
    FOLLOWER = "follower"
    CANDIDATE = "candidate"
    LEADER = "leader"

@dataclass
class LogEntry:
    """
    Each entry in the Raft log.
    
    This replaces the WAL entries from Episode 3.1.
    The Raft log IS the distributed WAL.
    """
    term: int           # Leader's term when entry was created
    index: int          # Position in log (1-indexed)
    command: str        # "PUT key value" or "DEL key"
    checksum: str = ""  # Data integrity (from Episode 3.1)
    
    def __post_init__(self):
        data = f"{self.term}|{self.index}|{self.command}"
        self.checksum = hashlib.sha256(data.encode()).hexdigest()[:16]

@dataclass
class AppendEntriesRequest:
    """
    RPC from leader to followers.
    
    The leader sends this to replicate log entries.
    Contains consistency checks to ensure follower's log matches.
    """
    term: int                     # Leader's current term
    leader_id: str                # So follower knows who the leader is
    prev_log_index: int           # Index of entry before new entries
    prev_log_term: int            # Term of entry before new entries
    entries: List[LogEntry]       # New entries to replicate (empty = heartbeat)
    leader_commit: int            # Leader's commit index

@dataclass
class AppendEntriesResponse:
    term: int           # Follower's current term
    success: bool       # True if follower accepted entries
    match_index: int    # Highest index follower has (for leader tracking)

@dataclass
class RequestVoteRequest:
    term: int           # Candidate's term
    candidate_id: str   # Who is asking for votes
    last_log_index: int # Index of candidate's last log entry
    last_log_term: int  # Term of candidate's last log entry

@dataclass  
class RequestVoteResponse:
    term: int
    vote_granted: bool
```

### Component 2: The Raft Node

```python
class RaftNode:
    """
    A single node in a Raft cluster.
    
    This WRAPS our DistKV from Episode 3.1.
    The Raft log replaces the WAL — entries are replicated
    to a majority before being applied to the KV store.
    
    Key Raft invariants (memorize for interviews):
    1. Election Safety:     At most one leader per term
    2. Leader Append-Only:  Leaders never overwrite their entries
    3. Log Matching:        Same index + term → identical prefix
    4. Leader Completeness: Committed entries survive leader changes
    5. State Machine Safety: All nodes apply same entries in same order
    """
    
    def __init__(self, node_id: str, peers: List[str], store):
        # Identity
        self.node_id = node_id
        self.peers = peers  # Other node IDs in cluster
        
        # Persistent state (survives crashes)
        self.current_term = 0
        self.voted_for: Optional[str] = None
        self.log: List[LogEntry] = []  # 1-indexed (index 0 is sentinel)
        
        # Volatile state
        self.role = NodeRole.FOLLOWER
        self.commit_index = 0          # Highest committed index
        self.last_applied = 0          # Highest applied to state machine
        
        # Leader-only state
        self.next_index: Dict[str, int] = {}   # Next index to send each peer
        self.match_index: Dict[str, int] = {}  # Highest replicated per peer
        
        # Timers
        self.election_timeout = self._random_timeout()
        self.last_heartbeat = time.time()
        
        # The KV store from Episode 3.1 (our state machine)
        self.store = store
        
        # Network layer (simplified — in production, use gRPC)
        self.send_rpc = None  # Set by cluster manager
        
        # Locks
        self.lock = threading.Lock()
    
    def _random_timeout(self) -> float:
        """
        Random election timeout: 150-300ms
        
        WHY RANDOM? Without randomness, all followers timeout 
        simultaneously → all become candidates → split votes → 
        no leader elected → repeat forever.
        
        Random timeout breaks this symmetry.
        """
        return random.uniform(0.150, 0.300)
    
    # ─── CLIENT-FACING API ───
    
    def client_put(self, key: str, value: str) -> bool:
        """
        Handle a client PUT request.
        
        Flow (compare to Episode 3.1's single-node PUT):
        
        Episode 3.1: WAL.append() → memtable.put() → return OK
        Episode 3.2: Raft.propose() → replicate to majority → 
                     commit → apply to store → return OK
        
        The key difference: We don't return OK until a MAJORITY
        of nodes have the entry. This is what makes it durable 
        across machine failures.
        """
        with self.lock:
            if self.role != NodeRole.LEADER:
                return False  # Only leader accepts writes
            
            # Step 1: Append to local log
            entry = LogEntry(
                term=self.current_term,
                index=len(self.log) + 1,
                command=f"PUT {key} {value}"
            )
            self.log.append(entry)
            
            # Step 2: Replicate to followers
            # (In production: async, with callbacks on majority ACK)
            ack_count = 1  # Leader counts as 1
            
            for peer_id in self.peers:
                success = self._send_append_entries(peer_id)
                if success:
                    ack_count += 1
            
            # Step 3: Commit if majority acknowledged
            majority = (len(self.peers) + 1) // 2 + 1
            
            if ack_count >= majority:
                self.commit_index = entry.index
                self._apply_committed_entries()
                return True
            else:
                # Failed to get majority — entry will be retried
                return False
    
    def client_get(self, key: str):
        """
        Handle a client GET request.
        
        Design choice: Do we need consensus for reads?
        
        Option A: Read from leader only (linearizable, slower)
        Option B: Read from any replica (faster, possibly stale)
        
        We implement Option A for correctness.
        In Episode 3.5, we'll add tunable consistency where
        clients can choose between strong and eventual reads.
        """
        with self.lock:
            if self.role != NodeRole.LEADER:
                return None, False  # Redirect to leader
            
            # Confirm we're still leader (leader lease)
            # In production: Send heartbeat, wait for majority ACK
            return self.store.get(key)
    
    # ─── RAFT PROTOCOL ───
    
    def _send_append_entries(self, peer_id: str) -> bool:
        """
        Send AppendEntries RPC to a follower.
        
        This is the BFS propagation from the LeetCode seed,
        but with consistency checks and acknowledgments.
        """
        next_idx = self.next_index.get(peer_id, len(self.log) + 1)
        
        # Determine prev_log_index and prev_log_term for consistency check
        prev_log_index = next_idx - 1
        prev_log_term = 0
        if prev_log_index > 0 and prev_log_index <= len(self.log):
            prev_log_term = self.log[prev_log_index - 1].term
        
        # Entries to send
        entries = self.log[next_idx - 1:]  # All entries from next_idx onward
        
        request = AppendEntriesRequest(
            term=self.current_term,
            leader_id=self.node_id,
            prev_log_index=prev_log_index,
            prev_log_term=prev_log_term,
            entries=entries,
            leader_commit=self.commit_index
        )
        
        # Send RPC (simplified)
        response = self._rpc_call(peer_id, 'append_entries', request)
        
        if response is None:
            return False  # Network failure
        
        if response.success:
            # Update tracking for this peer
            self.next_index[peer_id] = next_idx + len(entries)
            self.match_index[peer_id] = next_idx + len(entries) - 1
            return True
        else:
            if response.term > self.current_term:
                # We're stale — step down to follower
                self._become_follower(response.term)
            else:
                # Log mismatch — decrement and retry
                # This is the "backtracking" when follower's log diverges
                self.next_index[peer_id] = max(1, next_idx - 1)
            return False
    
    def handle_append_entries(self, request: AppendEntriesRequest) -> AppendEntriesResponse:
        """
        Follower receives AppendEntries from leader.
        
        This is where BFS becomes conditional:
        - BFS: Always accept new color
        - Raft: Only accept if log is consistent (term/index check)
        """
        with self.lock:
            # Rule 1: Reject if leader's term is stale
            if request.term < self.current_term:
                return AppendEntriesResponse(
                    term=self.current_term, 
                    success=False,
                    match_index=0
                )
            
            # Accept leader's authority
            self.current_term = request.term
            self.role = NodeRole.FOLLOWER
            self.last_heartbeat = time.time()
            
            # Rule 2: Log consistency check
            # "Does my log at prev_log_index have the same term?"
            if request.prev_log_index > 0:
                if request.prev_log_index > len(self.log):
                    # We're behind — log is shorter than expected
                    return AppendEntriesResponse(
                        term=self.current_term,
                        success=False,
                        match_index=len(self.log)
                    )
                
                if self.log[request.prev_log_index - 1].term != request.prev_log_term:
                    # Term mismatch — truncate divergent entries
                    self.log = self.log[:request.prev_log_index - 1]
                    return AppendEntriesResponse(
                        term=self.current_term,
                        success=False,
                        match_index=len(self.log)
                    )
            
            # Rule 3: Append new entries (overwrite conflicts)
            for entry in request.entries:
                idx = entry.index - 1
                if idx < len(self.log):
                    if self.log[idx].term != entry.term:
                        self.log = self.log[:idx]  # Truncate conflict
                        self.log.append(entry)
                else:
                    self.log.append(entry)
            
            # Rule 4: Update commit index
            if request.leader_commit > self.commit_index:
                self.commit_index = min(
                    request.leader_commit, 
                    len(self.log)
                )
                self._apply_committed_entries()
            
            return AppendEntriesResponse(
                term=self.current_term,
                success=True,
                match_index=len(self.log)
            )
    
    # ─── LEADER ELECTION ───
    
    def _start_election(self):
        """
        Triggered when follower hasn't heard from leader.
        
        Election protocol:
        1. Increment term (new "election round")
        2. Vote for self
        3. Request votes from all peers
        4. Win if get majority
        5. If nobody wins → random timeout → try again
        """
        with self.lock:
            self.current_term += 1
            self.role = NodeRole.CANDIDATE
            self.voted_for = self.node_id
            
            votes = 1  # Vote for self
            
            request = RequestVoteRequest(
                term=self.current_term,
                candidate_id=self.node_id,
                last_log_index=len(self.log),
                last_log_term=self.log[-1].term if self.log else 0
            )
        
        # Request votes (outside lock — can be slow)
        for peer_id in self.peers:
            response = self._rpc_call(peer_id, 'request_vote', request)
            if response and response.vote_granted:
                votes += 1
        
        with self.lock:
            majority = (len(self.peers) + 1) // 2 + 1
            
            if votes >= majority and self.role == NodeRole.CANDIDATE:
                self._become_leader()
            else:
                # Election failed — reset to follower, try again later
                self.role = NodeRole.FOLLOWER
                self.election_timeout = self._random_timeout()
    
    def handle_request_vote(self, request: RequestVoteRequest) -> RequestVoteResponse:
        """
        Decide whether to vote for a candidate.
        
        Safety rules:
        1. Only vote once per term (prevents multiple leaders)
        2. Only vote for candidates with logs at least as up-to-date as ours
           (prevents losing committed entries)
        """
        with self.lock:
            # Rule 1: Don't vote for stale candidates
            if request.term < self.current_term:
                return RequestVoteResponse(term=self.current_term, vote_granted=False)
            
            # Update term if candidate is newer
            if request.term > self.current_term:
                self.current_term = request.term
                self.voted_for = None
                self.role = NodeRole.FOLLOWER
            
            # Rule 2: Only vote once per term
            if self.voted_for is not None and self.voted_for != request.candidate_id:
                return RequestVoteResponse(term=self.current_term, vote_granted=False)
            
            # Rule 3: Candidate's log must be at least as up-to-date
            my_last_term = self.log[-1].term if self.log else 0
            my_last_index = len(self.log)
            
            candidate_up_to_date = (
                request.last_log_term > my_last_term or
                (request.last_log_term == my_last_term and 
                 request.last_log_index >= my_last_index)
            )
            
            if not candidate_up_to_date:
                return RequestVoteResponse(term=self.current_term, vote_granted=False)
            
            # Grant vote
            self.voted_for = request.candidate_id
            self.last_heartbeat = time.time()  # Reset election timer
            
            return RequestVoteResponse(term=self.current_term, vote_granted=True)
    
    def _become_leader(self):
        """Transition to leader role."""
        self.role = NodeRole.LEADER
        
        # Initialize leader state
        for peer in self.peers:
            self.next_index[peer] = len(self.log) + 1
            self.match_index[peer] = 0
        
        # Send immediate heartbeat to establish authority
        self._send_heartbeats()
        
        print(f"[Raft] Node {self.node_id} became leader for term {self.current_term}")
    
    def _become_follower(self, term: int):
        """Step down to follower."""
        self.current_term = term
        self.role = NodeRole.FOLLOWER
        self.voted_for = None
    
    def _send_heartbeats(self):
        """Send empty AppendEntries as heartbeat."""
        for peer in self.peers:
            self._send_append_entries(peer)
    
    # ─── STATE MACHINE APPLICATION ───
    
    def _apply_committed_entries(self):
        """
        Apply committed log entries to the KV store.
        
        This is where Raft meets Episode 3.1:
        The Raft log entry is "committed" (majority have it),
        so we apply it to our DistKV store.
        """
        while self.last_applied < self.commit_index:
            self.last_applied += 1
            entry = self.log[self.last_applied - 1]
            
            # Parse and apply command
            parts = entry.command.split(' ', 2)
            cmd = parts[0]
            
            if cmd == 'PUT' and len(parts) >= 3:
                self.store.put(parts[1], parts[2])
            elif cmd == 'DEL' and len(parts) >= 2:
                self.store.delete(parts[1])
    
    def _rpc_call(self, peer_id, method, request):
        """Simplified RPC call. In production: gRPC or custom TCP."""
        if self.send_rpc:
            return self.send_rpc(peer_id, method, request)
        return None
    
    # ─── TIMER LOOP ───
    
    def tick(self):
        """
        Called periodically (every ~50ms).
        
        Checks if election timeout has elapsed (followers)
        or sends heartbeats (leaders).
        """
        with self.lock:
            if self.role == NodeRole.LEADER:
                self._send_heartbeats()
            elif time.time() - self.last_heartbeat > self.election_timeout:
                # Haven't heard from leader — start election
                pass  # Will call _start_election() outside lock
        
        if self.role != NodeRole.LEADER:
            if time.time() - self.last_heartbeat > self.election_timeout:
                self._start_election()
```

### Component 3: The Replicated Cluster

```python
class RaftCluster:
    """
    Manages a cluster of RaftNodes.
    
    In production, each node runs on a separate machine.
    Here we simulate a 3-node cluster in one process for testing.
    
    3 nodes → tolerates 1 failure (majority = 2)
    5 nodes → tolerates 2 failures (majority = 3)
    7 nodes → tolerates 3 failures (majority = 4)
    
    Formula: N nodes tolerates ⌊(N-1)/2⌋ failures
    """
    
    def __init__(self, num_nodes: int = 3):
        self.nodes: Dict[str, RaftNode] = {}
        
        node_ids = [f"node_{i}" for i in range(num_nodes)]
        
        for node_id in node_ids:
            peers = [nid for nid in node_ids if nid != node_id]
            store = DistKV(
                data_dir=f'/tmp/distkv/{node_id}',
                node_id=node_id
            )
            node = RaftNode(node_id, peers, store)
            node.send_rpc = self._make_rpc_handler(node_id)
            self.nodes[node_id] = node
    
    def _make_rpc_handler(self, sender_id):
        """Create RPC handler for a node."""
        def send_rpc(peer_id, method, request):
            if peer_id not in self.nodes:
                return None
            
            peer = self.nodes[peer_id]
            
            if method == 'append_entries':
                return peer.handle_append_entries(request)
            elif method == 'request_vote':
                return peer.handle_request_vote(request)
            return None
        
        return send_rpc
    
    def get_leader(self) -> Optional[RaftNode]:
        """Find the current leader."""
        for node in self.nodes.values():
            if node.role == NodeRole.LEADER:
                return node
        return None
    
    def simulate_failure(self, node_id: str):
        """Simulate a node crash (for testing failover)."""
        if node_id in self.nodes:
            del self.nodes[node_id]
            print(f"[Cluster] Node {node_id} CRASHED")
    
    def demo(self):
        """
        Demonstrate replicated KV operations.
        
        This is the payoff: writes survive node failure!
        """
        # Elect a leader
        self.nodes['node_0']._start_election()
        leader = self.get_leader()
        
        if leader:
            # Write through leader
            leader.client_put("user:1", "Alice")
            leader.client_put("user:2", "Bob")
            
            # Verify all nodes have the data
            for node_id, node in self.nodes.items():
                val, found = node.store.get("user:1")
                print(f"  {node_id}: user:1 = {val} (found={found})")
            
            # Crash the leader
            leader_id = leader.node_id
            self.simulate_failure(leader_id)
            
            # New election
            remaining = list(self.nodes.values())[0]
            remaining._start_election()
            
            new_leader = self.get_leader()
            if new_leader:
                # Data survived the crash!
                val, found = new_leader.store.get("user:1")
                print(f"  After leader crash: user:1 = {val} ✓")
                
                # New writes work on new leader
                new_leader.client_put("user:3", "Charlie")
                print(f"  New write after failover: user:3 = Charlie ✓")
```

---

## 4. Failure Modes in Replicated DistKV

### Failure Mode 1: Network Partition (Split Brain)

```python
# Scenario: Network splits the 3-node cluster
#
# {Leader: node_0} | {Follower: node_1, Follower: node_2}
#
# What happens:
# 1. node_0 (old leader) is isolated, can't reach majority
#    → Its writes NEVER commit (no majority ACK)
#    → Clients connected to node_0 see write failures
#
# 2. node_1 or node_2 times out, starts election
#    → They have a majority (2 out of 3)
#    → New leader elected on the other side of partition
#    → Clients connected to node_1/2 can continue writing
#
# 3. When partition heals:
#    → node_0 discovers higher term from new leader
#    → node_0 steps down, rolls back uncommitted entries
#    → node_0 gets caught up with new leader's log
#
# RAFT GUARANTEE: No committed data is ever lost.
# Only uncommitted entries on the isolated leader are rolled back.

class PartitionSimulator:
    def simulate_partition(self, cluster, isolated_nodes):
        """
        Simulate network partition.
        Isolated nodes can't communicate with non-isolated nodes.
        """
        for node_id in isolated_nodes:
            node = cluster.nodes[node_id]
            original_rpc = node.send_rpc
            
            def partitioned_rpc(peer_id, method, request):
                if peer_id not in isolated_nodes:
                    return None  # Partition: can't reach other side
                return original_rpc(peer_id, method, request)
            
            node.send_rpc = partitioned_rpc
```

### Failure Mode 2: Slow Follower

```python
# Scenario: One follower has slow disk I/O (old hardware)
#
# Problem:
# 1. Leader sends AppendEntries at 1000 entries/sec
# 2. Slow follower can only process 100 entries/sec
# 3. Leader keeps decrementing nextIndex, resending old entries
# 4. Eventually leader needs to resend ENTIRE log (GBs of data)
# 5. This saturates the network, slowing down healthy followers too
#
# Solution: Log compaction with snapshots

class SnapshotManager:
    """
    Log compaction: Replace old entries with a snapshot.
    
    When a follower is too far behind, send a snapshot
    instead of replaying the entire log.
    """
    
    def create_snapshot(self, store, last_included_index, last_included_term):
        """
        Snapshot the entire KV store state.
        
        After snapshot, all log entries before last_included_index
        can be discarded.
        """
        snapshot = {
            'last_included_index': last_included_index,
            'last_included_term': last_included_term,
            'data': dict(store.memtable.data)  # Full state
        }
        return snapshot
    
    def install_snapshot(self, store, snapshot):
        """
        Install snapshot on a follower.
        
        Used when follower is too far behind for log replay.
        """
        store.memtable.data = snapshot['data']
        return snapshot['last_included_index']

# Used by: etcd, CockroachDB, TiKV
# etcd takes snapshots every 10,000 entries
# CockroachDB uses incremental snapshots (Raft + RocksDB)
```

### Failure Mode 3: Disruptive Candidate (Pre-Vote)

```python
# Scenario: A node with network issues keeps losing connection
# and restarting elections, bumping the term number each time.
#
# Problem:
# 1. Flaky node loses heartbeats (brief network blip)
# 2. Starts election, increments term to 42
# 3. Network recovers, sends RequestVote with term=42
# 4. Current leader in term 41 sees higher term → steps down!
# 5. Cluster is leaderless until new election completes
# 6. The flaky node loses again → cycle repeats
#
# Solution: Pre-Vote extension

class PreVoteExtension:
    """
    Before starting a real election, ask: "Would you vote for me?"
    
    If majority says no → don't start election (don't bump term).
    This prevents a single flaky node from destabilizing the cluster.
    """
    
    def pre_vote(self, node, peers):
        """
        Phase 0: Check if election would succeed.
        
        Send PreVote RPCs without incrementing term.
        Only proceed to real election if majority responds positively.
        """
        pre_votes = 1  # Self
        
        for peer_id in peers:
            # "If my term were current_term+1, would you vote for me?"
            would_vote = self._ask_pre_vote(
                peer_id,
                proposed_term=node.current_term + 1,
                last_log_index=len(node.log),
                last_log_term=node.log[-1].term if node.log else 0
            )
            if would_vote:
                pre_votes += 1
        
        majority = (len(peers) + 1) // 2 + 1
        
        if pre_votes >= majority:
            # Safe to start real election
            node._start_election()
        else:
            # Don't disrupt cluster — reset timer and wait
            node.election_timeout = node._random_timeout()
            node.last_heartbeat = time.time()
```

### Failure Mode 4: Disk Corruption

```python
# Scenario: Bit flip corrupts a log entry on a follower
#
# Our checksum from Episode 3.1 catches this:
# 1. Leader sends AppendEntries with entries[i]
# 2. Follower verifies checksum before appending
# 3. If checksum mismatch → request snapshot from leader
#
# Production approach (CockroachDB):
# - Checksums on every log entry
# - Periodic background verification of entire log
# - Automatic recovery via snapshot from healthy replicas
```

---

## 5. Production Hardening

### Optimization 1: Flow Control (Backpressure)

```python
class FlowControlledReplicator:
    """
    Prevent leader from overwhelming slow followers.
    
    Without flow control: Leader sends entries as fast as possible.
    A slow follower causes unbounded buffering → OOM.
    
    With flow control: Byte-based window limits in-flight data.
    """
    
    def __init__(self, max_in_flight_bytes: int = 10 * 1024 * 1024):
        self.max_in_flight = max_in_flight_bytes
        self.in_flight: Dict[str, int] = {}  # peer → bytes in flight
    
    def can_send(self, peer_id: str, entry_bytes: int) -> bool:
        """Check if we can send more data to this peer."""
        current = self.in_flight.get(peer_id, 0)
        return current + entry_bytes <= self.max_in_flight
    
    def on_send(self, peer_id: str, entry_bytes: int):
        """Record bytes sent."""
        self.in_flight[peer_id] = self.in_flight.get(peer_id, 0) + entry_bytes
    
    def on_ack(self, peer_id: str, acked_bytes: int):
        """Record bytes acknowledged."""
        self.in_flight[peer_id] = max(0, self.in_flight.get(peer_id, 0) - acked_bytes)
```

### Optimization 2: Batched Replication

```python
class BatchedReplicator:
    """
    Send multiple entries per AppendEntries RPC.
    
    Without batching: 1 entry per RPC = high overhead
    With batching: 100 entries per RPC = lower latency/entry
    
    Dynamic batching: Adjust batch size based on follower speed.
    Fast follower → larger batches
    Slow follower → smaller batches
    """
    
    def __init__(self):
        self.target_batch_bytes = 1024 * 1024  # 1MB
        self.max_batch_entries = 1000
    
    def create_batch(self, log, next_index):
        """Create optimal batch for a follower."""
        entries = []
        batch_bytes = 0
        
        for i in range(next_index - 1, len(log)):
            entry = log[i]
            entry_size = len(entry.command.encode())
            
            if batch_bytes + entry_size > self.target_batch_bytes:
                break
            if len(entries) >= self.max_batch_entries:
                break
            
            entries.append(entry)
            batch_bytes += entry_size
        
        return entries
```

### Optimization 3: Multi-Rack Awareness

```python
class RackAwareQuorum:
    """
    Ensure replicas span physical failure domains.
    
    Problem: All 3 replicas in same rack → rack failure = data loss
    Fix: Quorum must span at least 2 racks/AZs
    
    Used by: CockroachDB across AZs, TiDB across data centers
    """
    
    def __init__(self, node_racks: Dict[str, str]):
        self.node_racks = node_racks  # node_id → rack_id
    
    def is_safe_quorum(self, acking_nodes: List[str]) -> bool:
        """Check if ACKing nodes span multiple racks."""
        racks = set(self.node_racks.get(n, 'unknown') for n in acking_nodes)
        return len(racks) >= 2
```

---

## 6. Interview Cheatsheet

```python
interview_qa = {
    "Q: How does Raft handle leader failure?":
    "A: Followers detect missing heartbeats via random election timeout. "
    "A follower becomes candidate, requests votes, wins with majority. "
    "Random timeout prevents split votes.",
    
    "Q: Can Raft lose committed data?":
    "A: No. Leader Completeness property: a committed entry exists on "
    "a majority of nodes. Any new leader must have a log at least as "
    "up-to-date (election safety), so committed entries are never lost.",
    
    "Q: Raft vs Paxos?":
    "A: Same safety guarantees. Raft decomposes the problem into "
    "leader election + log replication (easier to understand). "
    "Paxos is more theoretically elegant but harder to implement.",
    
    "Q: How to scale Raft reads?":
    "A: Option 1: Read from leader only (linearizable). "
    "Option 2: Follower reads with leader lease (bounded staleness). "
    "Option 3: Multi-Raft — partition data, one Raft group per partition "
    "(this is sharding — Episode 3.3).",
    
    "Q: What happens during a network partition?":
    "A: Minority side can't elect a leader (no majority). "
    "Majority side elects new leader, continues serving. "
    "When partition heals, minority catches up from majority leader.",
    
    "Q: How many nodes should a Raft cluster have?":
    "A: 3 (tolerates 1 failure), 5 (tolerates 2), rarely more. "
    "More nodes = slower commits (more RPCs per write). "
    "For more capacity → shard data across multiple Raft groups (Episode 3.3)."
}
```

### Production Checklist

```markdown
✅ Raft log replication with majority commitment
✅ Leader election with random timeouts
✅ Log consistency checks (term + index)
✅ Pre-vote extension to prevent disruptive elections
✅ Log compaction with snapshots
✅ Flow control (byte-based windows)
✅ Batched replication for throughput
✅ Checksums per entry (from Episode 3.1)
✅ Multi-rack awareness for quorum safety
✅ Leader lease for read optimization
```

---

## 7. What's Next

```
WHAT YOU BUILT TODAY:
DistKV v2 — a replicated key-value store:
- Raft consensus for leader election
- Log replication to all followers
- Majority-based commitment (survives ⌊(N-1)/2⌋ failures)
- Automatic leader failover

This is the foundation of etcd, Consul, TiDB, and CockroachDB.

COMPARE TO EPISODE 3.1:
3.1: PUT → WAL → Memtable → OK (single machine)
3.2: PUT → Raft log → Replicate to majority → Commit → Apply to store → OK

The cost: ~2x latency (extra network round-trip to followers)
The gain: Survives machine death. Zero data loss.

BUT: All writes still go through ONE leader.
     If you need 100K writes/sec, one machine can't handle it.
     If your data exceeds one machine's disk, you're stuck.

NEXT: Episode 3.3 — Sharding: Divide and Conquer
We'll partition our data across multiple Raft groups.
Each shard handles a range of keys, each with its own leader.
Write throughput scales linearly with the number of shards.

The question changes from "How do we survive failure?"
to "How do we handle more data and traffic than one machine allows?"
```

---

*"Replication is the easiest distributed systems concept to explain and the hardest to get right. Today you learned why — and built the hardest version."*
