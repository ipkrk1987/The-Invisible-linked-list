Episode 3.1 — Replication: Never Lose Data

From BFS Propagation to Raft Log Replication

---

1. The Hook: Real-World Production Failure

The 2017 GitLab Database Outage
"Accidentally deleted production data and 5 backups. 6 hours of data loss."

What happened:

1. Engineer deletes a directory of stale PostgreSQL data
2. Misconfigured backup script had been failing silently for months
3. Replication was running but backups weren't verified
4. Cascading failure: replication propagated the deletion everywhere

The Lesson: Replication without proper verification and leadership election is a time bomb. You need more than just copying data — you need consensus on what the correct history is.

---

2. The LeetCode Seed: BFS Propagation

LeetCode Problem 733: Flood Fill (BFS/DFS traversal)

```python
def floodFill(image, sr, sc, newColor):
    original = image[sr][sc]
    if original == newColor: 
        return image
    
    rows, cols = len(image), len(image[0])
    queue = collections.deque([(sr, sc)])
    
    while queue:
        r, c = queue.popleft()
        image[r][c] = newColor
        
        # Propagate to 4 neighbors
        for dr, dc in [(1,0), (-1,0), (0,1), (0,-1)]:
            nr, nc = r + dr, c + dc
            if 0 <= nr < rows and 0 <= nc < cols and image[nr][nc] == original:
                queue.append((nr, nc))
    
    return image
```

Core Insight: BFS propagates state changes from a source to connected nodes, ensuring all reachable nodes get the update.

---

3. Distributed Systems Mapping

BFS → Log Replication:

BFS Concept Distributed Systems Equivalent
Starting pixel Leader node
4-connected neighbors Followers / replicas
Color change operation Log entry (write/update/delete)
Queue of pixels to fill Replication send queue / buffer
Boundary checks Term/index consistency checks
All pixels updated Log entry committed on majority

The Key Contrast: In BFS, propagation is fire-and-forget. In Raft, propagation is acknowledged and conditional — entries only become "real" when a majority has them.

---

4. Production System Build: Mini-Raft Replication

Step 1: The Log Structure

```go
type LogEntry struct {
    Term    int         // leader's term when entry created
    Index   int         // position in log (Raft paper uses 1-based)
    Command interface{} // the state machine command
}

type Node struct {
    id        int
    currentTerm int
    votedFor   int
    log        []LogEntry    // 0-indexed in our implementation
    
    // Volatile state
    commitIndex int  // highest committed entry
    lastApplied int  // highest applied to state machine
    
    // Leader state
    nextIndex  map[int]int  // for each follower, next index to send
    matchIndex map[int]int  // for each follower, highest replicated index
}
```

Note: Raft's paper uses 1-based indices for clarity, but our implementation uses 0-based Python lists internally.

Step 2: Leader Election (Simplified)

```python
class RaftNode:
    def __init__(self, node_id):
        self.node_id = node_id
        self.state = "follower"  # or "candidate", "leader"
        self.current_term = 0
        self.election_timeout = random.randint(150, 300)  # ms
        
    def become_candidate(self):
        self.state = "candidate"
        self.current_term += 1
        self.voted_for = self.node_id
        self.request_votes_from_peers()
        
    def request_votes_from_peers(self):
        # Send RequestVote RPC to all other nodes
        votes_received = 1  # vote for self
        
        for peer in self.peers:
            # In production, these RPCs are asynchronous with timeouts
            # Here we show synchronous logic for clarity
            response = peer.request_vote(
                term=self.current_term,
                candidate_id=self.node_id,
                last_log_index=len(self.log)-1,
                last_log_term=self.log[-1].term if self.log else 0
            )
            
            if response.vote_granted:
                votes_received += 1
                
        # If majority votes received, become leader
        if votes_received > len(self.peers) // 2:
            self.become_leader()
```

Step 3: Log Replication Protocol

```python
def replicate_log_entry(self, command):
    """Leader appends command and replicates to followers"""
    
    # 1. Leader appends to local log
    entry = LogEntry(
        term=self.current_term,
        index=len(self.log),  # 0-based index; Raft paper's index = len(self.log) + 1
        command=command
    )
    self.log.append(entry)
    
    # 2. Send AppendEntries RPC to all followers
    for follower_id in self.followers:
        next_idx = self.next_index[follower_id]
        prev_log_index = next_idx - 1
        prev_log_term = self.log[prev_log_index].term if prev_log_index >= 0 else 0
        
        entries = self.log[next_idx:]  # entries to send
        
        self.send_append_entries(
            follower_id,
            term=self.current_term,
            leader_id=self.node_id,
            prev_log_index=prev_log_index,
            prev_log_term=prev_log_term,
            entries=entries,
            leader_commit=self.commit_index
        )

def handle_append_entries_response(self, follower_id, success, match_index):
    """Leader processes follower's response"""
    
    if success:
        # Update follower's progress
        self.match_index[follower_id] = match_index
        self.next_index[follower_id] = match_index + 1
        
        # Check if we can commit this entry
        self.try_commit_entries()
    else:
        # Decrement next_index and retry (one entry at a time)
        self.next_index[follower_id] -= 1
        self.retry_replication(follower_id)

def try_commit_entries(self):
    """Commit entries replicated to majority of nodes"""
    for N in range(self.commit_index + 1, len(self.log)):
        # Count how many followers have replicated entry N
        count = 1  # leader has it
        for follower_id in self.followers:
            if self.match_index[follower_id] >= N:
                count += 1
        
        # If majority (including leader) have it, commit
        if count > (len(self.followers) + 1) // 2:
            if self.log[N].term == self.current_term:
                self.commit_index = N
    
    # Apply committed entries to state machine
    self.apply_committed_entries()
```

Step 4: Follower Handling

```python
def handle_append_entries(self, leader_term, leader_id, prev_log_index, 
                         prev_log_term, entries, leader_commit):
    """Follower processes AppendEntries RPC"""
    
    # 1. Reject if leader's term is stale
    if leader_term < self.current_term:
        return AppendEntriesResponse(term=self.current_term, success=False)
    
    # 2. Reset election timeout (leader is alive)
    self.reset_election_timeout()
    
    # 3. Check log consistency
    if len(self.log) - 1 < prev_log_index:
        # Follower missing entries
        return AppendEntriesResponse(term=leader_term, success=False)
    
    if prev_log_index >= 0 and self.log[prev_log_index].term != prev_log_term:
        # Log mismatch - delete conflicting entry and all that follow
        # (Matches Raft's "delete entry at prevLogIndex and all that follow it")
        self.log = self.log[:prev_log_index]
        return AppendEntriesResponse(term=leader_term, success=False)
    
    # 4. Append new entries
    if entries:
        self.log = self.log[:prev_log_index + 1] + entries
    
    # 5. Update commit index
    if leader_commit > self.commit_index:
        self.commit_index = min(leader_commit, len(self.log) - 1)
        self.apply_committed_entries()
    
    return AppendEntriesResponse(term=leader_term, success=True)
```

---

5. Failure Modes (What Breaks in Real Life?)

Failure 1: Network Partitions

```python
# Scenario: Leader isolated from majority
leader = Node(id=0, state="leader")
follower1 = Node(id=1, state="follower")
follower2 = Node(id=2, state="follower")

# Network partition: {0} | {1, 2}
# Leader 0 continues accepting writes but can't replicate to majority
# Followers 1 & 2 elect new leader among themselves (they have quorum)
# Result: Temporary split-brain with two leaders

# Raft's safety guarantee: The old leader's writes will be rolled back 
# when partition heals, as they don't have quorum acknowledgment
```

Failure 2: Slow Followers

```python
# Follower 1 is on overloaded hardware, 10x slower disk I/O
# Leader keeps decrementing next_index[1] and resending old entries
# Eventually, next_index[1] = 0, leader resends ENTIRE log (GBs of data)

# This is why Raft keeps nextIndex per follower and backs off one entry
# at a time — but without flow control, a pathological follower can
# still saturate bandwidth and starve other followers
```

Failure 3: Memory Pressure

```python
# Leader buffers unacknowledged entries in memory
# One follower goes offline for maintenance
# Leader's memory fills with unsent entries
# OOM killer terminates the leader
# All writes stop until new leader elected
```

Failure 4: Disk Corruption

```python
# Follower's log file gets corrupted at index 1527
# Leader sends entry 1528 with prev_log_index=1527, prev_log_term=5
# Follower's log[1527].term is corrupted (reads as 8 instead of 5)
# Follower rejects all future AppendEntries
# Manual intervention required: snapshot + restore
```

---

6. Hardening the System: Production-Grade Raft

Fix 1: Pre-Vote Extension (Prevents Disruptive Candidates)

```python
def request_pre_vote(self):
    """Check if peers would vote for me before incrementing term"""
    grant_count = 1
    
    for peer in self.peers:
        # Ask: "Would you vote for me if my term were current_term+1?"
        granted = peer.pre_vote_request(
            term=self.current_term + 1,
            candidate_id=self.node_id,
            last_log_index=len(self.log)-1,
            last_log_term=self.log[-1].term if self.log else 0
        )
        if granted:
            grant_count += 1
    
    # Only become real candidate if majority would support
    # This prevents a slightly-partitioned or slow node from repeatedly
    # bumping the term and forcing everyone into unnecessary elections
    return grant_count > len(self.peers) // 2
```

Fix 2: Log Compaction with Snapshots

```python
def take_snapshot(self, last_included_index, last_included_term, snapshot_data):
    """Replace old log entries with compacted snapshot"""
    
    # In real Raft, snapshots carry lastIncludedIndex and lastIncludedTerm
    # and logs are logically offset by these values.
    # Here we pretend last_included_index maps directly into our 0-based list.
    # Real implementations keep a "log offset" to translate between snapshot
    # indices and in-memory log positions.
    
    # 1. Save snapshot to disk
    snapshot = Snapshot(
        last_included_index=last_included_index,
        last_included_term=last_included_term,
        data=snapshot_data
    )
    self.save_snapshot(snapshot)
    
    # 2. Truncate log
    if last_included_index <= len(self.log) - 1:
        self.log = self.log[last_included_index + 1:]
    
    # 3. Update metadata
    self.snapshot_index = last_included_index
    self.snapshot_term = last_included_term
```

Fix 3: Dynamic Batching & Flow Control

```python
# To prevent one slow follower from forcing the leader to resend
# GBs of data, we add a simple byte-based flow control window.

class FlowControlledReplicator:
    def __init__(self):
        self.in_flight_bytes = 0
        self.max_in_flight = 10 * 1024 * 1024  # 10MB
        self.target_batch_size = 1 * 1024 * 1024  # 1MB
        self.batch_timeout = 10  # ms
        self.last_send = time.time()
        
    def replicate_with_flow_control(self, entries):
        """Batch entries while respecting follower capacity"""
        
        batch = []
        batch_size_bytes = 0
        
        for entry in entries:
            entry_size = self.estimate_size(entry)
            
            # Don't exceed follower's window
            if self.in_flight_bytes + batch_size_bytes + entry_size > self.max_in_flight:
                break
                
            batch.append(entry)
            batch_size_bytes += entry_size
            
            # Send batch if: full batch OR timeout reached
            if batch_size_bytes >= self.target_batch_size or \
               (batch and time.time() - self.last_send > self.batch_timeout):
                self.send_batch(batch)
                self.last_send = time.time()
                batch = []
                batch_size_bytes = 0
        
        # Send any remaining entries
        if batch:
            self.send_batch(batch)
            self.last_send = time.time()
```

Fix 4: Checksums & Data Integrity

```python
class VerifiedLogEntry:
    def __init__(self, term, index, command):
        self.term = term
        self.index = index
        self.command = command
        self.checksum = self.calculate_checksum(command)
        
    def calculate_checksum(self, data):
        # CRC32 or SHA-256 for production
        return hashlib.sha256(pickle.dumps(data)).digest()[:4]
    
    def validate(self):
        return self.checksum == self.calculate_checksum(self.command)

def verify_log_integrity(self):
    """Periodic log verification job"""
    for i, entry in enumerate(self.log):
        if not entry.validate():
            # Corruption detected!
            self.truncate_log_to(i - 1)
            self.request_snapshot_from_leader()
            break
```

Fix 5: Multi-Rack Awareness

```python
class RackAwareReplication:
    def __init__(self, nodes_with_racks):
        # nodes_by_rack = {"us-east-1a": [node1, node2], "us-east-1b": [node3, node4]}
        self.nodes_by_rack = nodes_with_racks
        
        # Build reverse mapping
        self.node_to_rack = {}
        for rack, nodes in nodes_with_racks.items():
            for node in nodes:
                self.node_to_rack[node.id] = rack
        
        self.required_racks_for_quorum = 2  # At least 2 racks must acknowledge
        
    def is_quorum_met(self, acknowledgments):
        """Check if acknowledgments span enough racks"""
        racks_acked = set()
        
        for node_id in acknowledgments:
            racks_acked.add(self.node_to_rack[node_id])
        
        return len(racks_acked) >= self.required_racks_for_quorum
```

---

7. What This Teaches for System Design Interviews

Interview Takeaways:

1. Replication ≠ Backup: Replication propagates errors; backups save you
2. The CAP Triangle Trade-off:
   · Raft chooses CP (Consistency + Partition tolerance)
   · CP doesn't mean "always consistent no matter what" — it means whenever the system responds, the response is consistent
   · During partitions, Raft may simply refuse some writes/reads
   · Know when to choose AP systems (Dynamo) vs CP (Raft)
3. Leader-Based Systems Scale Reads, Not Writes:
   ```python
   # All followers can serve reads (eventually consistent)
   # All writes go through leader (single bottleneck)
   # In Raft, linearizable reads usually go through the leader 
   # (or require extra checks). Serving reads from followers 
   # trades consistency for scale.
   # Solution: Leader leases, read replicas with staleness bounds
   ```
4. Log-Based Replication Enables:
   · Change Data Capture (Debezium, Kafka Connect)
   · Event sourcing architectures
   · Time-travel queries (point-in-time recovery)
   · Secondary indexes (materialized views)
5. Production Nuances:
   · Disk space management (log compaction)
   · Memory management (unacked entry buffers)
   · Network topology (cross-AZ traffic costs)
   · Upgrade strategies (rolling restarts with leadership transfer)

Common Interview Questions:

· "How does Raft compare to Paxos?"
· "What happens during a network partition?"
· "How do you handle a slow follower?"
· "How would you scale a Raft-based system?"
· "When would you choose eventual consistency over strong consistency?"

The Bottom Line:

You've just built the foundation of etcd, Consul, TiDB, and CockroachDB. Every distributed database, coordination service, or replicated state machine at FAANG uses some variant of this log replication pattern.

---

Raft Safety Cheatsheet (Key Invariants)

1. Election Safety: At most one leader per term
2. Leader Append-Only: Leaders never overwrite or delete entries
3. Log Matching: If two logs contain an entry with same index and term, they have identical preceding entries
4. Leader Completeness: A committed entry will be present in future leaders' logs
5. State Machine Safety: If a server applies entry at index i, no other server will apply different entry at i

---

Next Episode Preview: Episode 3.2 — Consensus: When Machines Disagree
We'll add leader election, quorum commits, and make the system safe under crashes and network partitions (but not Byzantine/malicious nodes). We'll see why Raft is easier to reason about than Paxos, and how quorum + terms give you those five safety properties from the cheatsheet.

---

