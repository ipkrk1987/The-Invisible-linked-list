# Episode 3.6 — Conflict Resolution: When Replicas Disagree
## From Course Schedule to Vector Clocks and CRDTs

**Season 3 — Distributed Systems: Building DistKV**

---

## Previously on DistKV...

In Episode 3.5, we added tunable consistency. Clients can now choose eventual consistency for speed — reading from any replica without waiting for the leader.

**But eventual consistency has a dark side.** Two clients write the same key on different replicas simultaneously. When replicas sync, we have two versions of the truth. Who wins?

- Last-Write-Wins by wall clock? **Clocks drift.** Two servers 50ms apart = wrong winner.
- Pick higher replica ID? **Arbitrary.** No semantic meaning.
- Keep both? **Now the client has to merge.**

**Today's fix:** Add vector clocks for causal ordering (know which writes happened before which) and CRDTs for automatic, mathematically-proven conflict resolution.

**Arc So Far:**
- **3.1**: Single-node KV store ✅
- **3.2**: Raft replication ✅
- **3.3**: Sharding ✅
- **3.4**: Distributed transactions ✅
- **3.5**: Consistency models ✅
- **3.6**: Conflict resolution ← YOU ARE HERE

---

## 1. The Hook: Real-World Production Failure

### The 2012 Amazon Shopping Cart Mystery
**"Items keep disappearing from my cart"**

**The Incident:**
1. Amazon's shopping cart used Dynamo (their internal distributed KV store)
2. Dynamo used eventual consistency for availability — shopping cart must ALWAYS work
3. Two replicas: Replica A has cart `{book, pen}`, Replica B has cart `{book, lamp}`
4. User added `pen` on Replica A, added `lamp` on Replica B (during brief partition)
5. Replicas reconcile: Last-Write-Wins → `{book, lamp}` wins. `pen` is LOST.
6. User: "I ADDED a pen! Where did it go?!"

**Amazon's Solution:** Vector clocks + application-level merge. When conflict detected, return ALL versions to the client. Shopping cart merge = **union of all items** (better to have a phantom item than to lose one).

**The Lesson:** Conflict resolution isn't a database problem — it's a domain problem. The system must understand what "merge" means for each data type.

---

## 2. The LeetCode Seed: Course Schedule

```python
# LeetCode #207: Course Schedule
# Can you finish all courses given prerequisite pairs?
# This IS causality detection in disguise.

from collections import defaultdict, deque

def canFinish(numCourses, prerequisites):
    """
    Topological sort: detect if directed graph has cycles.
    
    The connection to distributed systems:
    - Courses = events (writes to our KV store)
    - Prerequisites = causal relationships
    - "Course A before Course B" = "Write A happened before Write B"
    - Cycle = causality violation (impossible ordering)
    
    A vector clock IS a causality DAG:
    - If VC(A) < VC(B), A happened before B (A is prerequisite for B)
    - If neither VC(A) < VC(B) nor VC(B) < VC(A), they're concurrent
      (no prerequisite relationship — THESE are the conflicts)
    """
    graph = defaultdict(list)
    in_degree = [0] * numCourses
    
    for course, prereq in prerequisites:
        graph[prereq].append(course)
        in_degree[course] += 1
    
    # Start with courses that have no prerequisites = root events
    queue = deque([i for i in range(numCourses) if in_degree[i] == 0])
    completed = 0
    
    while queue:
        course = queue.popleft()
        completed += 1
        
        for next_course in graph[course]:
            in_degree[next_course] -= 1
            if in_degree[next_course] == 0:
                queue.append(next_course)
    
    return completed == numCourses

# Mapping to DistKV:
#
# Events: Write("x", 1, node_A), Write("x", 2, node_B), Write("x", 3, node_A)
#
# If Write1 → Write3 (causal: same node, sequential)
#    but Write1 ∥ Write2 (concurrent: different nodes, no causal link)
#
# Topological sort resolves what we CAN order.
# Concurrent events (cycles in comparison) need conflict resolution.
```

---

## 3. The Causality Problem

```
WHY WALL CLOCKS FAIL:

Server A (clock: 10:00:00.100)    Server B (clock: 10:00:00.050)
        |                                  |
 Write("x", 1)                      Write("x", 2)
  at 10:00:00.100                    at 10:00:00.150
        |                                  |
        └──────── reconcile ───────────────┘
                       |
           Server B's clock is 50ms behind.
           B's timestamp: 10:00:00.150
           A's event was at 10:00:00.100
           
           LWW picks B's write (higher timestamp).
           But B's real time might have been 10:00:00.100 too!
           
           Winner depends on clock accuracy, not causality.
           THIS IS WHY WE NEED VECTOR CLOCKS.
```

---

## 4. Production System Build

### Component 1: Vector Clock

```python
from typing import Dict, Optional, Any
import copy

class VectorClock:
    """
    A logical clock that tracks causality across distributed nodes.
    
    The key insight:
    - Each node maintains a counter
    - On local event: increment own counter
    - On send: attach current vector clock
    - On receive: merge (take max per node), then increment own
    
    This gives us three relationships:
    1. VC(A) < VC(B)  → A HAPPENED BEFORE B
    2. VC(B) < VC(A)  → B HAPPENED BEFORE A
    3. Neither         → A and B are CONCURRENT (conflict!)
    """
    
    def __init__(self, node_id: str):
        self.node_id = node_id
        self.clock: Dict[str, int] = {}
    
    def increment(self):
        """Increment this node's counter (local event)."""
        self.clock[self.node_id] = self.clock.get(self.node_id, 0) + 1
        return self
    
    def merge(self, other: 'VectorClock'):
        """
        Merge with another vector clock (on receive).
        Take element-wise maximum.
        """
        all_nodes = set(self.clock.keys()) | set(other.clock.keys())
        for node in all_nodes:
            self.clock[node] = max(
                self.clock.get(node, 0),
                other.clock.get(node, 0)
            )
        self.increment()  # Increment own counter after merge
        return self
    
    def __lt__(self, other: 'VectorClock') -> bool:
        """
        VC(A) < VC(B) if:
        - For ALL nodes: A[node] <= B[node]
        - For AT LEAST ONE node: A[node] < B[node]
        
        This means A "happened before" B.
        """
        all_nodes = set(self.clock.keys()) | set(other.clock.keys())
        at_least_one_less = False
        
        for node in all_nodes:
            a_val = self.clock.get(node, 0)
            b_val = other.clock.get(node, 0)
            
            if a_val > b_val:
                return False  # A has a component > B → not "before"
            if a_val < b_val:
                at_least_one_less = True
        
        return at_least_one_less
    
    def concurrent_with(self, other: 'VectorClock') -> bool:
        """
        Two events are concurrent if neither happened before the other.
        These are the CONFLICTS that need resolution.
        """
        return not (self < other) and not (other < self) and self.clock != other.clock
    
    def copy(self):
        """Create a deep copy of this vector clock."""
        vc = VectorClock(self.node_id)
        vc.clock = copy.deepcopy(self.clock)
        return vc
    
    def __repr__(self):
        return f"VC({dict(sorted(self.clock.items()))})"

# Visual example:
#
# Node A: VC = {A:0}     Node B: VC = {B:0}
#
# A writes "x"=1:   VC = {A:1}
# B writes "x"=2:   VC = {B:1}
#
# Compare: {A:1} vs {B:1}
#   A[A]=1 > B[A]=0 → not A < B
#   A[B]=0 < B[B]=1 → not B < A
#   → CONCURRENT! This is a CONFLICT.
#
# A receives B's write:  merged VC = {A:2, B:1}
# Now {A:1} < {A:2, B:1} → the write at {A:1} happened before merge
```

### Component 2: Versioned Store with Conflict Detection

```python
from dataclasses import dataclass, field
from typing import List, Tuple

@dataclass
class VersionedValue:
    """A value with its vector clock and metadata."""
    value: Any
    vector_clock: VectorClock
    node_id: str
    timestamp: float = 0.0  # Wall clock for tiebreaking only

class ConflictDetector:
    """
    Integrates vector clocks into DistKV's storage layer.
    
    On write: attach current vector clock.
    On read: return all concurrent versions (siblings).
    On reconcile: detect conflicts, invoke resolution strategy.
    """
    
    def __init__(self, node_id: str):
        self.node_id = node_id
        self.vc = VectorClock(node_id)
        self.store: Dict[str, List[VersionedValue]] = {}
    
    def put(self, key: str, value: Any) -> VectorClock:
        """
        Write a key with vector clock tracking.
        """
        self.vc.increment()
        
        versioned = VersionedValue(
            value=value,
            vector_clock=self.vc.copy(),
            node_id=self.node_id
        )
        
        if key not in self.store:
            self.store[key] = [versioned]
        else:
            # Remove any values that this write supersedes
            surviving = []
            for existing in self.store[key]:
                if not existing.vector_clock < versioned.vector_clock:
                    surviving.append(existing)
            surviving.append(versioned)
            self.store[key] = surviving
        
        return self.vc.copy()
    
    def get(self, key: str) -> Tuple[List[VersionedValue], bool]:
        """
        Read a key. May return MULTIPLE values (siblings)
        if there are unresolved concurrent writes.
        """
        if key not in self.store:
            return [], False
        
        return self.store[key], True
    
    def receive_write(self, key: str, versioned: VersionedValue):
        """
        Receive a write from another node (during replication).
        Detect if it's a successor, predecessor, or concurrent.
        """
        self.vc.merge(versioned.vector_clock)
        
        if key not in self.store:
            self.store[key] = [versioned]
            return "NEW"
        
        new_siblings = []
        relationship = "SUCCESSOR"
        
        for existing in self.store[key]:
            if versioned.vector_clock < existing.vector_clock:
                # Incoming is OLDER — discard it
                return "STALE"
            
            elif existing.vector_clock < versioned.vector_clock:
                # Existing is OLDER — incoming supersedes
                continue  # Don't keep old version
            
            else:
                # CONCURRENT — both survive as siblings
                new_siblings.append(existing)
                relationship = "CONFLICT"
        
        new_siblings.append(versioned)
        self.store[key] = new_siblings
        
        return relationship
```

### Component 3: CRDTs — Conflict-Free Replicated Data Types

```python
class GCounter:
    """
    Grow-only Counter CRDT.
    
    The magic: Instead of one counter, each node has its OWN counter.
    Total = sum of all node counters.
    Merge = take max of each node's counter.
    
    This ALWAYS converges — no conflicts possible!
    
    Use case: "Likes" counter, view counter, visit counter.
    """
    
    def __init__(self, node_id: str):
        self.node_id = node_id
        self.counts: Dict[str, int] = {}
    
    def increment(self, amount: int = 1):
        """Increment this node's counter."""
        self.counts[self.node_id] = self.counts.get(self.node_id, 0) + amount
    
    def value(self) -> int:
        """Total count across all nodes."""
        return sum(self.counts.values())
    
    def merge(self, other: 'GCounter'):
        """
        Merge with another replica's counter.
        Take max per node — commutative, associative, idempotent.
        
        These three properties = CRDT guarantee.
        No matter what order merges happen, result is the same.
        """
        all_nodes = set(self.counts.keys()) | set(other.counts.keys())
        for node in all_nodes:
            self.counts[node] = max(
                self.counts.get(node, 0),
                other.counts.get(node, 0)
            )
    
    def __repr__(self):
        return f"GCounter({self.counts}) = {self.value()}"


class PNCounter:
    """
    Positive-Negative Counter CRDT.
    
    A counter that supports both increment AND decrement.
    Implemented as two G-Counters: one for increments, one for decrements.
    Value = P.value() - N.value()
    
    Use case: Upvotes/downvotes, inventory count, balance.
    """
    
    def __init__(self, node_id: str):
        self.p = GCounter(node_id)  # Positive (increments)
        self.n = GCounter(node_id)  # Negative (decrements)
    
    def increment(self, amount: int = 1):
        self.p.increment(amount)
    
    def decrement(self, amount: int = 1):
        self.n.increment(amount)
    
    def value(self) -> int:
        return self.p.value() - self.n.value()
    
    def merge(self, other: 'PNCounter'):
        self.p.merge(other.p)
        self.n.merge(other.n)


class ORSet:
    """
    Observed-Remove Set CRDT.
    
    The "add wins" set: if one replica adds and another removes
    concurrently, the ADD wins or the REMOVE can only remove
    specific observed versions.
    
    This solves the Amazon shopping cart problem:
    - Replica A: add("pen")
    - Replica B: remove("pen") — but only removes the version B observed
    - Merge: "pen" survives! (A's add wasn't observed by B's remove)
    
    Implementation: Each element gets a unique tag on add.
    Remove only removes specific tags, not the element concept.
    """
    
    def __init__(self, node_id: str):
        self.node_id = node_id
        self.tag_counter = 0
        # element → set of (tag, node_id) pairs
        self.elements: Dict[Any, set] = {}
        # Tombstone: removed tags
        self.removed: set = set()
    
    def add(self, element):
        """Add an element with a unique tag."""
        self.tag_counter += 1
        tag = (self.node_id, self.tag_counter)
        
        if element not in self.elements:
            self.elements[element] = set()
        self.elements[element].add(tag)
    
    def remove(self, element):
        """
        Remove an element — but ONLY the tags we've observed.
        
        If another replica added the same element with a tag
        we haven't seen, that add survives the remove.
        """
        if element in self.elements:
            for tag in self.elements[element]:
                self.removed.add(tag)
            self.elements[element] = set()
    
    def contains(self, element) -> bool:
        """Check if element is in the set (has non-removed tags)."""
        if element not in self.elements:
            return False
        active_tags = self.elements[element] - self.removed
        return len(active_tags) > 0
    
    def value(self) -> set:
        """Return all elements with at least one active tag."""
        result = set()
        for element, tags in self.elements.items():
            if tags - self.removed:
                result.add(element)
        return result
    
    def merge(self, other: 'ORSet'):
        """
        Merge two OR-Sets.
        - Union of all elements and tags
        - Union of all removed tags
        - Elements with surviving (non-removed) tags remain
        """
        # Merge elements
        for element, tags in other.elements.items():
            if element not in self.elements:
                self.elements[element] = set()
            self.elements[element] |= tags
        
        # Merge tombstones
        self.removed |= other.removed
```

### Component 4: Conflict Resolution Strategies for DistKV

```python
class ConflictResolver:
    """
    Pluggable conflict resolution for DistKV.
    
    The right strategy depends on the DATA TYPE:
    - Counters → Use CRDTs (no conflict possible)
    - Shopping carts → Use OR-Set CRDT (add-wins)
    - Arbitrary KV → Use vector clocks + application merge
    - Simple values → Last-Write-Wins (if you accept data loss)
    """
    
    @staticmethod
    def last_write_wins(siblings: List[VersionedValue]) -> VersionedValue:
        """
        Simple: highest timestamp wins. Loses data.
        Acceptable for: session data, caches, ephemeral state.
        
        Cassandra uses this as default.
        """
        return max(siblings, key=lambda s: s.timestamp)
    
    @staticmethod
    def highest_node_id_wins(siblings: List[VersionedValue]) -> VersionedValue:
        """
        Deterministic: higher node ID wins. Also loses data.
        At least consistent across all nodes (no clock dependency).
        """
        return max(siblings, key=lambda s: s.node_id)
    
    @staticmethod
    def merge_values(siblings: List[VersionedValue]) -> VersionedValue:
        """
        Application-specific merge. Keeps ALL data.
        
        Examples:
        - Shopping cart: union of items
        - Document: merge text (like git merge)
        - Set: union of elements
        """
        # Example: merge as union (for set-valued keys)
        merged_set = set()
        for sibling in siblings:
            if isinstance(sibling.value, (set, list)):
                merged_set |= set(sibling.value)
            else:
                merged_set.add(sibling.value)
        
        # New version supersedes all siblings
        merged_vc = VectorClock("merger")
        for sibling in siblings:
            merged_vc.merge(sibling.vector_clock)
        
        return VersionedValue(
            value=merged_set,
            vector_clock=merged_vc,
            node_id="merger"
        )
    
    @staticmethod
    def return_all(siblings: List[VersionedValue]) -> List[VersionedValue]:
        """
        Return ALL versions to the client. Client decides.
        
        This is what Amazon Dynamo does.
        The application knows the domain semantics.
        """
        return siblings
```

---

## 5. Failure Modes

### Failure Mode 1: Vector Clock Explosion

```python
# Scenario: 1000 nodes, each writes once
# Vector clock has 1000 entries: {A:1, B:1, C:1, ..., ZZ:1}
# Metadata is now larger than the value!
#
# Fix: Limit vector clock size with pruning

class BoundedVectorClock(VectorClock):
    """
    Prune vector clock entries when size exceeds threshold.
    
    Strategy: Remove entries with lowest counters
    (least recently active nodes).
    
    Trade-off: May lose causal ordering info → false conflicts.
    But false conflicts are SAFE (we resolve them, just unnecessarily).
    False ordering would be UNSAFE (we'd drop a write).
    """
    
    MAX_ENTRIES = 10
    
    def prune(self):
        if len(self.clock) <= self.MAX_ENTRIES:
            return
        
        # Keep the MAX_ENTRIES most active nodes
        sorted_entries = sorted(
            self.clock.items(), key=lambda x: x[1], reverse=True
        )
        self.clock = dict(sorted_entries[:self.MAX_ENTRIES])
```

### Failure Mode 2: Sibling Explosion

```python
# Scenario: Many concurrent writes → many siblings
# Key has 100 versions → read returns 100 values → client overwhelmed
#
# Fix: Sibling limit + automatic resolution

class SiblingManager:
    """
    Manage sibling (concurrent version) explosion.
    
    Cassandra limit: 100,000 siblings before warning.
    DynamoDB: Automatically resolves with LWW after threshold.
    Our approach: Configurable limit with fallback resolution.
    """
    
    MAX_SIBLINGS = 32
    
    def check_siblings(self, key, siblings, resolver):
        if len(siblings) > self.MAX_SIBLINGS:
            print(f"WARNING: {key} has {len(siblings)} siblings. "
                  f"Auto-resolving with LWW.")
            resolved = resolver.last_write_wins(siblings)
            return [resolved]
        return siblings
```

---

## 6. Interview Cheatsheet

```python
interview_qa = {
    "Q: How do vector clocks work?":
    "A: Each node maintains a counter per known node. "
    "Increment own counter on local events. Merge (take max) on receive. "
    "If VC(A) < VC(B): A happened before B. "
    "If neither < the other: concurrent (conflict!).",
    
    "Q: What are CRDTs?":
    "A: Data types designed so merge is always conflict-free. "
    "Merge is commutative (A+B = B+A), associative ((A+B)+C = A+(B+C)), "
    "and idempotent (A+A = A). Examples: G-Counter, OR-Set. "
    "Trade-off: limited operations (no arbitrary KV).",
    
    "Q: Last-Write-Wins vs Vector Clocks — when to use each?":
    "A: LWW: simple, lossy, good for caches/sessions. "
    "Vector clocks: complex, preserves causality, good for critical data. "
    "CRDTs: best of both (automatic + correct) but only for specific types.",
    
    "Q: How does Amazon handle shopping cart conflicts?":
    "A: Vector clocks detect concurrent writes. Return all versions "
    "to the client. Shopping cart merge = union (add-wins). "
    "Better to have a phantom item than to lose one."
}
```

---

## 7. What's Next

```
WHAT YOU BUILT TODAY:
DistKV v6 — conflict resolution:
- VectorClock: causal ordering across nodes
- ConflictDetector: detect concurrent writes as siblings
- GCounter, PNCounter: conflict-free counters
- ORSet: conflict-free sets (add-wins)
- ConflictResolver: pluggable strategies (LWW, merge, return-all)

The shopping cart is safe:
  cart_a.add("pen")     # Replica A
  cart_b.add("lamp")    # Replica B (concurrent)
  cart_a.merge(cart_b)  # → {"pen", "lamp"} — nothing lost!

BUT: Our DistKV runs in ONE region. Users in Tokyo hit our
     US servers with 200ms latency. We need MULTI-REGION
     replication. But if replicas are in different time zones,
     how do we ORDER events? Wall clocks differ. Vector clocks
     don't give real-time ordering.

NEXT: Episode 3.7 — Going Global: Multi-Region and Time
We'll add Hybrid Logical Clocks (HLC) for cross-region ordering
and Spanner-style TrueTime for external consistency.

The question changes from "Who wrote first?"
to "What TIME is it, really, across the planet?"
```

---

*"Conflict resolution is where distributed systems stop being computer science and start being philosophy — the answer to 'which write wins' depends not on algorithms but on what the data MEANS."*
