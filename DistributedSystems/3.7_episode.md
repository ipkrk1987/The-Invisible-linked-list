# Episode 3.7 — Going Global: Multi-Region and Time
## From Meeting Rooms II to Hybrid Logical Clocks and Spanner-Style TrueTime

**Season 3 — Distributed Systems: Building DistKV**

---

## Previously on DistKV...

Across Episodes 3.1-3.6, we built a replicated, sharded, transactional KV store with tunable consistency and conflict resolution. All running in **one data center.**

Users in Tokyo hit your US-East servers with 200ms round-trip latency. Users in Frankfurt? 90ms. Your SLA promises under 50ms. **You need replicas on every continent.**

But multi-region introduces a problem we've been dodging: **TIME.** Vector clocks from Episode 3.6 tell us causality but not real-world ordering. If a write in Tokyo and a write in Virginia happen "simultaneously," which one is at timestamp T=0? Their clocks differ by milliseconds — or seconds if GPS glitches.

**Today's fix:** Add Hybrid Logical Clocks (HLC) for affordable cross-region ordering and understand Google Spanner's TrueTime for external consistency. We'll make DistKV truly global.

**Arc So Far:**
- **3.1**: Single-node KV store ✅
- **3.2**: Raft replication ✅
- **3.3**: Sharding ✅
- **3.4**: Distributed transactions ✅
- **3.5**: Consistency models ✅
- **3.6**: Conflict resolution ✅
- **3.7**: Going global ← YOU ARE HERE

---

## 1. The Hook: Real-World Production System

### Google Spanner: The Database That Keeps Time
**"We put GPS receivers and atomic clocks in every data center"**

**The Problem Google Faced:**
1. Google needed globally consistent transactions (Ads, Cloud SQL)
2. Traditional approach: single leader, all writes route to one region
3. But single-leader = 200ms+ latency for remote writes = unacceptable
4. Multi-leader across regions: fast writes, but how to ORDER them?

**Google's Answer (2012):**
Install GPS receivers and atomic clocks in every data center. Create TrueTime API: instead of "the time is T," it says "the time is between [T-ε, T+ε]." Then WAIT for the uncertainty to pass before committing. This guarantees that if commit A returns before commit B starts, then A's timestamp < B's timestamp: **external consistency.**

**The Cost:** Google deployed custom hardware. Each commit costs ~7ms waiting. The trade-off: hardware investment + commit latency → globally consistent timestamps.

**Why This Matters for Us:** Most companies can't install atomic clocks. We need something cheaper. That's Hybrid Logical Clocks — 95% of the benefit, 0% of the special hardware.

---

## 2. The LeetCode Seed: Meeting Rooms II

```python
# LeetCode #253: Meeting Rooms II
# Find the minimum number of conference rooms needed.
# Intervals may overlap — overlaps represent uncertainty.

import heapq

def minMeetingRooms(intervals):
    """
    The connection to distributed time:
    - Meetings = transactions across regions
    - Overlap = clock uncertainty window
    - Rooms = serialization lanes
    
    TrueTime: Each timestamp is an INTERVAL [earliest, latest].
    If two transaction intervals DON'T overlap, we know their order.
    If they DO overlap, we must either:
      (a) WAIT for uncertainty to pass (Spanner's commit-wait)
      (b) Accept that ordering is ambiguous (HLC's approach)
    
    Just like Meeting Rooms: overlapping intervals need 
    separate "rooms" (serialization points) to avoid conflicts.
    """
    if not intervals:
        return 0
    
    intervals.sort(key=lambda x: x[0])
    heap = []  # End times of active meetings
    
    for start, end in intervals:
        # If earliest ending meeting ends before this one starts
        # → reuse that room (no overlap = ordered transactions)
        if heap and heap[0] <= start:
            heapq.heappop(heap)
        
        heapq.heappush(heap, end)
    
    return len(heap)  # Rooms needed = max concurrent overlapping intervals

# Mapping to Spanner:
#
# Transaction A: timestamp interval [100, 107]  (7ms uncertainty)
# Transaction B: timestamp interval [105, 112]
# Transaction C: timestamp interval [120, 127]
#
# A and B OVERLAP → ambiguous ordering → need commit-wait
# B and C DON'T OVERLAP → B definitely before C
#
# Rooms needed = max overlapping = 2
# This is the "cost" of clock uncertainty
```

---

## 3. The Clock Problem

```
WHY CLOCKS ARE HARD:

Three types of clocks:

1. WALL CLOCK (time.time() / System.currentTimeMillis())
   - Can jump backward (NTP correction)
   - Can jump forward (leap second)
   - Different machines differ by 1-250ms
   - NOT SAFE for ordering events

2. MONOTONIC CLOCK (time.monotonic() / System.nanoTime())
   - Never goes backward
   - Only meaningful on ONE machine
   - Cannot compare across machines
   - SAFE for durations, NOT for ordering

3. LOGICAL CLOCK (Lamport / Vector / HLC)
   - No wall-clock dependency
   - Captures causal ordering
   - Comparable across machines
   - SAFE for ordering, but no real-time meaning
   
Our goal: Combine wall clock (real-time meaning) + logical clock
(causal ordering) = Hybrid Logical Clock
```

---

## 4. Production System Build

### Component 1: Hybrid Logical Clock (HLC)

```python
import time
from dataclasses import dataclass
from typing import Tuple

@dataclass 
class HLCTimestamp:
    """
    A Hybrid Logical Clock timestamp.
    
    Components:
    - physical: wall clock time (milliseconds)
    - logical: counter for events at same physical time
    - node_id: source node (for tiebreaking)
    
    Ordering: Compare physical first, then logical, then node_id.
    This gives us total ordering with causal consistency.
    """
    physical: int   # Wall clock ms
    logical: int    # Sub-millisecond counter
    node_id: str    # Tiebreaker
    
    def __lt__(self, other):
        if self.physical != other.physical:
            return self.physical < other.physical
        if self.logical != other.logical:
            return self.logical < other.logical
        return self.node_id < other.node_id
    
    def __le__(self, other):
        return self == other or self < other
    
    def __eq__(self, other):
        return (self.physical == other.physical and 
                self.logical == other.logical and
                self.node_id == other.node_id)
    
    def __repr__(self):
        return f"HLC({self.physical}.{self.logical}@{self.node_id})"


class HybridLogicalClock:
    """
    Hybrid Logical Clock — combines wall clock with logical clock.
    
    Properties:
    1. Timestamps are always >= wall clock (no backward jumps)
    2. Causal ordering preserved (send before receive)
    3. Timestamps stay close to wall clock (bounded drift)
    4. NO special hardware required (unlike TrueTime)
    
    Algorithm (from Kulkarni et al., 2014):
    
    On LOCAL event or SEND:
      physical = max(local_wall_clock, current_physical)
      if physical == current_physical:
        logical += 1      # Same physical → increment logical
      else:
        logical = 0        # New physical → reset logical
    
    On RECEIVE(msg_timestamp):
      physical = max(local_wall_clock, current_physical, msg_physical)
      if physical == current_physical == msg_physical:
        logical = max(current_logical, msg_logical) + 1
      elif physical == current_physical:
        logical = current_logical + 1
      elif physical == msg_physical:
        logical = msg_logical + 1
      else:
        logical = 0
    """
    
    def __init__(self, node_id: str):
        self.node_id = node_id
        self.physical = 0
        self.logical = 0
    
    def _wall_clock_ms(self) -> int:
        """Get current wall clock in milliseconds."""
        return int(time.time() * 1000)
    
    def now(self) -> HLCTimestamp:
        """
        Generate a timestamp for a local event.
        Called on: every write, every transaction start.
        """
        wall = self._wall_clock_ms()
        
        if wall > self.physical:
            # Wall clock advanced — use it, reset logical
            self.physical = wall
            self.logical = 0
        else:
            # Wall clock hasn't advanced (fast events) — increment logical
            self.logical += 1
        
        return HLCTimestamp(self.physical, self.logical, self.node_id)
    
    def receive(self, msg_ts: HLCTimestamp) -> HLCTimestamp:
        """
        Update clock on receiving a message from another node.
        
        Ensures: returned timestamp > msg_ts (causal ordering)
        Ensures: returned timestamp >= wall clock (bounded drift)
        """
        wall = self._wall_clock_ms()
        
        if wall > self.physical and wall > msg_ts.physical:
            # Wall clock is ahead of both — use it
            self.physical = wall
            self.logical = 0
        
        elif self.physical > msg_ts.physical:
            # Our physical is ahead — increment logical
            self.logical += 1
        
        elif msg_ts.physical > self.physical:
            # Message physical is ahead — adopt it
            self.physical = msg_ts.physical
            self.logical = msg_ts.logical + 1
        
        else:
            # Same physical — take max logical + 1
            self.physical = self.physical  # unchanged
            self.logical = max(self.logical, msg_ts.logical) + 1
        
        return HLCTimestamp(self.physical, self.logical, self.node_id)
    
    def drift_from_wall_clock(self) -> int:
        """
        How far is our HLC from the wall clock?
        Should be bounded (< max network delay).
        Alert if drift > threshold.
        """
        return abs(self.physical - self._wall_clock_ms())

# Why HLC beats vector clocks for multi-region:
#
# Vector Clock: {tokyo:5, virginia:3, frankfurt:7}
#   - 3 entries per write (grows with nodes)
#   - No real-time meaning ("what time did this happen?")
#   - Can't do "give me all writes after 3pm UTC"
#
# HLC: HLC(1703001234567.3@tokyo)
#   - Fixed size (physical + logical + node_id)
#   - Has real-time meaning ("approximately 3pm UTC")
#   - CAN do "give me all writes after 3pm UTC"
#   - Still preserves causal ordering!
```

### Component 2: Multi-Region Replicator

```python
from enum import Enum
from typing import Dict, List, Optional
from dataclasses import dataclass, field

class ReplicationMode(Enum):
    SYNC = "synchronous"       # Wait for remote ACK (strong, slow)
    ASYNC = "asynchronous"     # Don't wait (fast, may lose data)
    SEMI_SYNC = "semi-sync"    # Wait for at least one remote region

@dataclass
class RegionConfig:
    region_id: str
    location: str          # e.g., "us-east-1", "ap-northeast-1"
    latency_ms: int        # Estimated latency to other regions
    is_primary: bool       # Is this the primary write region?
    
class GlobalReplicator:
    """
    Multi-region replication for DistKV.
    
    Architecture:
    
    Region: US-East        Region: EU-West        Region: AP-Tokyo
    ┌──────────────┐       ┌──────────────┐       ┌──────────────┐
    │ ShardRouter  │       │ ShardRouter  │       │ ShardRouter  │
    │ ┌──────────┐ │  ←──→ │ ┌──────────┐ │  ←──→ │ ┌──────────┐ │
    │ │RaftGroup │ │ async │ │RaftGroup │ │ async │ │RaftGroup │ │
    │ │(shard 1) │ │  ──→  │ │(shard 1) │ │  ──→  │ │(shard 1) │ │
    │ └──────────┘ │       │ └──────────┘ │       │ └──────────┘ │
    │ ┌──────────┐ │       │ ┌──────────┐ │       │ ┌──────────┐ │
    │ │RaftGroup │ │       │ │RaftGroup │ │       │ │RaftGroup │ │
    │ │(shard 2) │ │       │ │(shard 2) │ │       │ │(shard 2) │ │
    │ └──────────┘ │       │ └──────────┘ │       │ └──────────┘ │
    └──────────────┘       └──────────────┘       └──────────────┘
    
    Within region: Raft consensus (Episode 3.2) — synchronous
    Across regions: Async replication with HLC ordering
    """
    
    def __init__(self, local_region: RegionConfig, 
                 remote_regions: List[RegionConfig],
                 mode: ReplicationMode = ReplicationMode.SEMI_SYNC):
        self.local_region = local_region
        self.remote_regions = remote_regions
        self.mode = mode
        self.hlc = HybridLogicalClock(local_region.region_id)
        self.replication_lag: Dict[str, int] = {}  # region → lag in ms
        self.pending_acks: Dict[str, list] = {}
    
    def replicate_write(self, key: str, value: str, 
                        local_ts: HLCTimestamp) -> bool:
        """
        Replicate a write to remote regions.
        
        Called AFTER local Raft consensus succeeds.
        This is the cross-region layer.
        """
        if self.mode == ReplicationMode.SYNC:
            return self._sync_replicate(key, value, local_ts)
        
        elif self.mode == ReplicationMode.ASYNC:
            self._async_replicate(key, value, local_ts)
            return True  # Don't wait
        
        elif self.mode == ReplicationMode.SEMI_SYNC:
            return self._semi_sync_replicate(key, value, local_ts)
        
        return False
    
    def _sync_replicate(self, key, value, ts) -> bool:
        """
        Wait for ALL remote regions to ACK.
        
        Latency: max(all region latencies) × 2 (round trip)
        For US-East → Tokyo: ~200ms × 2 = 400ms per write
        
        When to use: Financial data, legal documents.
        """
        acks = 0
        for region in self.remote_regions:
            if self._send_to_region(region, key, value, ts):
                acks += 1
        
        return acks == len(self.remote_regions)
    
    def _async_replicate(self, key, value, ts):
        """
        Fire-and-forget to remote regions.
        
        Latency: 0ms additional (write completes immediately)
        Risk: Data loss if local region fails before replication
        
        When to use: Social media, analytics, non-critical data.
        """
        for region in self.remote_regions:
            # Queue for async send (background thread)
            if region.region_id not in self.pending_acks:
                self.pending_acks[region.region_id] = []
            self.pending_acks[region.region_id].append((key, value, ts))
    
    def _semi_sync_replicate(self, key, value, ts) -> bool:
        """
        Wait for at least ONE remote region to ACK.
        
        Latency: min(all region latencies) × 2
        For US-East: min(EU ~90ms, Tokyo ~200ms) = ~180ms total
        
        When to use: Most production workloads.
        Survives single-region failure without data loss.
        """
        for region in sorted(self.remote_regions, 
                           key=lambda r: r.latency_ms):
            if self._send_to_region(region, key, value, ts):
                # Got one ACK — replicate to rest async
                remaining = [r for r in self.remote_regions 
                           if r.region_id != region.region_id]
                for r in remaining:
                    self._async_replicate(key, value, ts)
                return True
        
        return False
    
    def _send_to_region(self, region, key, value, ts) -> bool:
        """
        Send a write to a remote region.
        Remote region applies via its local Raft consensus.
        """
        # In production: gRPC call with TLS, compression
        # Remote side: receive → HLC merge → Raft propose → ACK
        print(f"Replicating {key}={value} to {region.region_id} "
              f"(~{region.latency_ms}ms)")
        return True  # Simulated success
    
    def receive_remote_write(self, key, value, remote_ts: HLCTimestamp):
        """
        Receive a write from a remote region.
        
        1. Merge HLC (ensures causal ordering)
        2. Propose to local Raft group
        3. Apply when committed
        """
        local_ts = self.hlc.receive(remote_ts)
        
        print(f"Received remote write: {key}={value}")
        print(f"  Remote TS: {remote_ts}")
        print(f"  Local TS:  {local_ts}")
        print(f"  Drift:     {self.hlc.drift_from_wall_clock()}ms")
        
        # Propose to local Raft (Episode 3.2)
        # shard = router.ring.get_shard(key)
        # cluster = router.clusters[shard]
        # cluster.get_leader().client_put(key, value)
        
        return local_ts


class RegionRouter:
    """
    Route client requests to the optimal region.
    
    Strategy:
    - Reads: route to nearest region (lowest latency)
    - Writes: route to primary region for the key's shard
    - OR: route writes to local region (multi-leader)
    
    Single-leader vs Multi-leader:
    
    Single-leader (simpler):
      All writes → one primary region → replicate to others
      + No conflicts
      - Write latency = distance to primary
    
    Multi-leader (faster writes):
      Writes → any region → replicate to others
      + Low write latency everywhere  
      - Conflicts possible (Episode 3.6's CRDTs help)
    """
    
    def __init__(self, regions: List[RegionConfig],
                 multi_leader: bool = False):
        self.regions = {r.region_id: r for r in regions}
        self.multi_leader = multi_leader
    
    def route_read(self, key: str, client_region: str,
                   consistency: str = "eventual") -> str:
        """Route read to best region."""
        if consistency == "strong":
            # Must read from primary (or shard leader's region)
            for r in self.regions.values():
                if r.is_primary:
                    return r.region_id
        
        # For eventual/session: read from client's nearest region
        return client_region
    
    def route_write(self, key: str, client_region: str) -> str:
        """Route write to appropriate region."""
        if self.multi_leader:
            # Write to local region (lowest latency)
            return client_region
        
        # Single-leader: route to primary
        for r in self.regions.values():
            if r.is_primary:
                return r.region_id
        
        return client_region
```

### Component 3: Spanner-Style TrueTime (Conceptual)

```python
class TrueTime:
    """
    Google Spanner's TrueTime API.
    
    Instead of "the time is T", TrueTime says:
    "the time is between [earliest, latest]"
    
    Uncertainty (ε) comes from:
    - GPS receiver accuracy: ~1μs
    - Atomic clock drift: ~200μs/30s
    - Network delay to time server: variable
    
    Spanner's ε is typically 1-7ms.
    
    NOTE: This requires GPS receivers + atomic clocks in every DC.
    We implement this conceptually. For real production without
    special hardware, use HLC (Component 1) instead.
    """
    
    def __init__(self, epsilon_ms: float = 7.0):
        self.epsilon = epsilon_ms / 1000.0
    
    def now(self):
        """Return time interval [earliest, latest]."""
        t = time.time()
        return {
            'earliest': t - self.epsilon,
            'latest': t + self.epsilon,
            'epsilon': self.epsilon
        }
    
    def after(self, timestamp) -> bool:
        """Is the given timestamp definitely in the past?"""
        return self.now()['earliest'] > timestamp
    
    def before(self, timestamp) -> bool:
        """Is the given timestamp definitely in the future?"""
        return self.now()['latest'] < timestamp
    
    def commit_wait(self, commit_timestamp):
        """
        Spanner's commit-wait: after assigning a timestamp,
        WAIT until we're sure the timestamp is in the past.
        
        This ensures: if transaction A commits before B starts,
        then A's timestamp < B's timestamp. (External consistency)
        
        Cost: Average wait = ε ≈ 7ms per transaction.
        """
        while not self.after(commit_timestamp):
            time.sleep(0.001)  # Wait 1ms, check again
        
        # Now we're certain: commit_timestamp is in the past
        # No other transaction can get an earlier timestamp


# Comparison:
#
# | Feature              | Wall Clock | Vector Clock | HLC      | TrueTime  |
# |---------------------|-----------|-------------|---------|----------|
# | Causal ordering     | ✗         | ✓           | ✓       | ✓        |
# | Real-time meaning   | ✓         | ✗           | ~✓      | ✓        |
# | Fixed size          | ✓         | ✗ (grows)   | ✓       | ✓        |
# | Special hardware    | ✗         | ✗           | ✗       | ✓ (GPS)  |
# | External consistency| ✗         | ✗           | ✗       | ✓        |
# | Bounded uncertainty | ✗         | N/A         | ✗       | ✓ (~7ms) |
# | Cost                | Free      | Free        | Free    | $$$      |
# | Used by             | Most DBs  | Riak, Dynamo| CockroachDB | Spanner |
```

---

## 5. Failure Modes

### Failure Mode 1: Region-Level Failure

```python
# Scenario: US-East region goes completely offline
# (AWS us-east-1 outage — happens ~2x/year)
#
# Impact on DistKV:
# - If US-East is primary: ALL writes fail (single-leader)
# - Async-replicated data not yet sent to other regions: LOST
# - Semi-sync: data is in at least one other region
#
# Fix: Automatic region failover

class RegionFailoverManager:
    """
    Detect region failure and promote a new primary.
    
    Similar to Raft election (Episode 3.2) but at region level:
    - Regions heartbeat to each other
    - If primary region unreachable for T seconds, survivors elect
    - New primary takes over writes
    - When failed region recovers, it becomes a follower
    """
    
    def __init__(self, regions: List[RegionConfig]):
        self.regions = {r.region_id: r for r in regions}
        self.primary_region = next(
            r.region_id for r in regions if r.is_primary
        )
        self.failover_timeout_s = 30
    
    def detect_failure(self, failed_region: str):
        """Handle region failure detection."""
        if failed_region == self.primary_region:
            # Primary is down — must failover
            survivors = [r for r in self.regions.values() 
                        if r.region_id != failed_region]
            
            # Pick new primary: region with lowest replication lag
            new_primary = min(survivors, key=lambda r: r.latency_ms)
            self.primary_region = new_primary.region_id
            
            print(f"FAILOVER: {failed_region} → {new_primary.region_id}")
            print(f"Data loss window: up to {new_primary.latency_ms}ms")
            print(f"of async-replicated writes")
```

### Failure Mode 2: Clock Skew in HLC

```python
# Scenario: Node's wall clock jumps forward 1 hour (NTP misconfiguration)
# HLC adopts the bad time → all subsequent timestamps are 1 hour ahead
# Other nodes now "import" the bad time via HLC merge
#
# Fix: Bounded clock skew detection

class ClockSkewGuard:
    """
    Reject HLC updates with suspicious clock values.
    
    If a received timestamp is more than MAX_DRIFT ahead of
    our wall clock, reject it — the sender's clock is wrong.
    """
    
    MAX_DRIFT_MS = 60_000  # Reject if >60 seconds ahead
    
    def validate_remote_timestamp(self, remote_ts: HLCTimestamp):
        """Check if remote timestamp is plausible."""
        wall = int(time.time() * 1000)
        drift = remote_ts.physical - wall
        
        if drift > self.MAX_DRIFT_MS:
            raise Exception(
                f"Clock skew detected: remote is {drift}ms ahead. "
                f"Rejecting timestamp to prevent clock contamination. "
                f"Check NTP on {remote_ts.node_id}!"
            )
        
        return True
```

---

## 6. Multi-Region Design Patterns

```python
patterns = {
    "Follow-the-Sun": {
        "How": "Primary region follows business hours. "
               "US primary during US day, EU primary during EU day.",
        "Pro": "Low write latency during active hours",
        "Con": "Planned failover complexity",
        "Used_by": "Trading systems, customer support"
    },
    "Active-Active": {
        "How": "All regions accept writes (multi-leader). "
               "Conflicts resolved with CRDTs (Episode 3.6).",
        "Pro": "Lowest write latency globally",
        "Con": "Conflict resolution complexity",
        "Used_by": "DynamoDB Global Tables, Cassandra"
    },
    "Active-Passive": {
        "How": "One primary region, others are read replicas. "
               "Failover to passive on primary failure.",
        "Pro": "Simplest, no conflicts",
        "Con": "Write latency from remote regions",
        "Used_by": "Most traditional databases"
    },
    "Geo-Partitioned": {
        "How": "Data is partitioned by region. "
               "EU data stays in EU, US data in US.",
        "Pro": "Data sovereignty compliance (GDPR)",
        "Con": "Cross-region queries expensive",
        "Used_by": "CockroachDB, Spanner"
    }
}
```

---

## 7. Interview Cheatsheet

```python
interview_qa = {
    "Q: How does Spanner achieve global consistency?":
    "A: TrueTime API with GPS + atomic clocks provides bounded "
    "time uncertainty (ε ≈ 7ms). Commit-wait: after assigning a "
    "timestamp, wait 2ε before acknowledging. Guarantees external "
    "consistency: if commit A returns before B starts, ts(A) < ts(B).",
    
    "Q: What is a Hybrid Logical Clock?":
    "A: Combines wall clock (real-time meaning) with logical clock "
    "(causal ordering). Three components: physical (wall clock ms), "
    "logical (counter within same ms), node_id (tiebreaker). "
    "Stays close to wall clock, preserves causality, fixed size.",
    
    "Q: Single-leader vs Multi-leader replication?":
    "A: Single-leader: simpler, no conflicts, but write latency = "
    "distance to primary. Multi-leader: low write latency everywhere, "
    "but must handle conflicts (CRDTs, LWW, or app-level merge).",
    
    "Q: How do you handle region failover?":
    "A: Detect failure via heartbeats. Promote replica region with "
    "lowest replication lag. Accept RPO (data loss window) = "
    "replication lag of promoted region. Once failed region recovers, "
    "catch up and rejoin as follower."
}
```

---

## 8. What's Next

```
WHAT YOU BUILT TODAY:
DistKV v7 — globally distributed:
- HybridLogicalClock: cross-region ordering without GPS
- GlobalReplicator: sync/async/semi-sync replication modes
- RegionRouter: route reads to nearest, writes to primary
- RegionFailoverManager: automatic primary promotion
- ClockSkewGuard: reject contaminated timestamps

Your system now runs in 3 regions:
  US-East (primary) ←→ EU-West ←→ AP-Tokyo
  
  User in Tokyo: reads local (5ms), writes route to US (200ms)
  Region failover: US dies → EU promoted in 30s

BUT: We've built 7 episodes of capabilities one at a time.
     How do they all fit together? What happens when a request
     arrives and must flow through EVERY layer?

NEXT: Episode 3.8 — The Grand Assembly
A single PUT("user:alice", "{}") traverses:
  Client → Region Router → Shard Router → Consistency Manager
  → Raft Leader → WAL → Memtable → Replication → Cross-Region

We'll trace the FULL lifecycle, compare DistKV to real systems
(Cassandra, CockroachDB, Spanner, DynamoDB), and run failure
scenarios that cascade through every layer.

Season finale: Everything connects.
```

---

*"The hardest problem in distributed systems isn't algorithms — it's physics. Light takes 67ms to cross the Atlantic. No protocol can fix the speed of light."*
