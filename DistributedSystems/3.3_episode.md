# Episode 3.3 ‚Äî Sharding: Divide and Conquer
## From Binary Search to Consistent Hash Rings

**Season 3 ‚Äî Distributed Systems: Building DistKV**

---

## Previously on DistKV...

In Episode 3.1, we built a single-node KV store with WAL and crash recovery. In Episode 3.2, we added Raft replication ‚Äî every write is now replicated to a majority before commitment, surviving machine death.

**But we hit a ceiling:** All writes go through ONE leader. One machine's disk, CPU, and network limit our capacity. If our dataset grows to 10TB, one machine can't hold it. If we need 100K writes/sec, one leader can't process them all.

**Today's fix:** Split the keyspace across multiple Raft groups. Each group owns a slice of the keys. This is **sharding** ‚Äî and consistent hashing is how we decide which shard owns which key.

**Arc So Far:**
- **3.1**: Single-node KV store ‚úÖ
- **3.2**: Raft replication ‚úÖ
- **3.3**: Sharding ‚Üê YOU ARE HERE
- **3.4**: Distributed transactions (coming next)

---

## 1. The Hook: Real-World Production Failure

### The 2020 Twitch Sharding Catastrophe
**"A single streamer broke the entire platform"**

**The Incident:**
1. Twitch used modulo hashing to route chat messages: `shard = hash(channel_id) % num_shards`
2. A mega-streamer with 400K concurrent viewers was assigned to shard 7
3. Shard 7 received 100x the traffic of other shards ‚Äî a **hot shard**
4. Shard 7's server ran out of memory, crashed
5. Re-hashing moved data to other shards ‚Üí cascading failures
6. Platform-wide chat outage during a major esports event

**Why Modulo Hashing Fails:**
```python
# Before adding a server:
hash("channel_12345") % 3 = 1  ‚Üí Shard 1

# After adding one server for scaling:
hash("channel_12345") % 4 = 0  ‚Üí Shard 0  üîÑ MOVED!

# With modulo hashing, adding 1 server moves ~75% of keys!
# In production: 75% of keys become cache misses simultaneously
# Result: "Thundering herd" ‚Üí all shards overloaded ‚Üí outage
```

**The Lesson:** Consistent hashing solves this ‚Äî adding a server moves only `1/N` keys, not `(N-1)/N`.

---

## 2. The LeetCode Seed: Search Insert Position

```python
# LeetCode #35: Search Insert Position
# Find where a value should be inserted in a sorted array

def searchInsert(nums, target):
    """
    Given sorted array and target, return index where target 
    would be inserted to maintain sorted order.
    
    This IS consistent hash ring lookup:
    - nums = sorted shard positions on the ring
    - target = hash of the key
    - Result = which shard owns this key
    """
    left, right = 0, len(nums)
    
    while left < right:
        mid = (left + right) // 2
        if nums[mid] < target:
            left = mid + 1
        else:
            right = mid
    
    return left

# Example: Ring positions = [100, 350, 700]
# Key hash = 400 ‚Üí searchInsert([100, 350, 700], 400) = 2
# Key goes to shard at position 700 (next clockwise)
#
# Key hash = 800 ‚Üí searchInsert([100, 350, 700], 800) = 3
# Wraps around ‚Üí shard at position 100 (circular ring)
#
# This O(log N) lookup is the heart of every consistent hash ring.
# Redis Cluster, Cassandra, and DynamoDB all use this pattern.
```

---

## 3. Building Sharded DistKV

### The Architecture Change

```
EPISODE 3.2 (Single Raft Group):

  All keys ‚Üí Leader ‚Üí Followers
  Capacity limited to one machine's resources

EPISODE 3.3 (Sharded ‚Äî NEW):

  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ            CONSISTENT HASH RING              ‚îÇ
  ‚îÇ                                             ‚îÇ
  ‚îÇ   hash("user:1") ‚Üí Shard A                 ‚îÇ
  ‚îÇ   hash("user:2") ‚Üí Shard B                 ‚îÇ
  ‚îÇ   hash("user:3") ‚Üí Shard A                 ‚îÇ
  ‚îÇ   hash("order:1") ‚Üí Shard C                ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ          ‚îÇ          ‚îÇ
  ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê
  ‚îÇShard‚îÇ   ‚îÇShard‚îÇ   ‚îÇShard‚îÇ
  ‚îÇ  A  ‚îÇ   ‚îÇ  B  ‚îÇ   ‚îÇ  C  ‚îÇ
  ‚îÇ(Raft‚îÇ   ‚îÇ(Raft‚îÇ   ‚îÇ(Raft‚îÇ
  ‚îÇgroup)‚îÇ   ‚îÇgroup)‚îÇ   ‚îÇgroup)‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  
  Each shard is the replicated KV store from Episode 3.2!
  Each shard has its own Raft leader and followers.
```

### Component 1: The Consistent Hash Ring

```python
import hashlib
import bisect
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field

@dataclass
class ShardInfo:
    """Metadata about a shard (Raft group)."""
    shard_id: str
    nodes: List[str]      # Node IDs in this Raft group
    leader: Optional[str]  # Current leader node
    key_range: Tuple[int, int]  # (start_hash, end_hash) owned

class ConsistentHashRing:
    """
    Consistent hash ring with virtual nodes.
    
    Why virtual nodes?
    - 3 physical nodes on a ring ‚Üí likely uneven distribution
    - 3 physical √ó 150 virtual each = 450 points ‚Üí nearly uniform
    
    This is how Cassandra, DynamoDB, and Riak distribute data.
    
    Key insight from LeetCode #35:
    The ring is a sorted array of positions.
    Finding which shard owns a key = binary search (searchInsert).
    """
    
    def __init__(self, num_virtual_nodes: int = 150):
        self.num_virtual_nodes = num_virtual_nodes
        self.ring: List[int] = []           # Sorted positions
        self.ring_map: Dict[int, str] = {}  # Position ‚Üí shard_id
        self.shards: Dict[str, ShardInfo] = {}
    
    def _hash(self, key: str) -> int:
        """Hash a key to a position on the ring (0 to 2^32-1)."""
        digest = hashlib.md5(key.encode()).hexdigest()
        return int(digest[:8], 16)
    
    def add_shard(self, shard_id: str, nodes: List[str]):
        """
        Add a shard to the ring with virtual nodes.
        
        Each shard gets num_virtual_nodes positions on the ring.
        Adding a shard moves only ~1/N keys (much better than modulo!).
        """
        self.shards[shard_id] = ShardInfo(
            shard_id=shard_id,
            nodes=nodes,
            leader=nodes[0] if nodes else None,
            key_range=(0, 0)  # Updated below
        )
        
        for i in range(self.num_virtual_nodes):
            virtual_key = f"{shard_id}:vnode:{i}"
            position = self._hash(virtual_key)
            
            self.ring.append(position)
            self.ring_map[position] = shard_id
        
        self.ring.sort()
    
    def remove_shard(self, shard_id: str):
        """
        Remove a shard from the ring.
        
        Only keys owned by this shard need to move.
        All other keys stay on their current shard.
        """
        positions_to_remove = [
            pos for pos, sid in self.ring_map.items() 
            if sid == shard_id
        ]
        
        for pos in positions_to_remove:
            self.ring.remove(pos)
            del self.ring_map[pos]
        
        if shard_id in self.shards:
            del self.shards[shard_id]
    
    def get_shard(self, key: str) -> Optional[str]:
        """
        Find which shard owns a key.
        
        This is LeetCode #35 on a circular sorted array:
        1. Hash the key to a ring position
        2. Binary search for the next clockwise shard position 
        3. That shard owns this key
        
        Time complexity: O(log V) where V = total virtual nodes
        """
        if not self.ring:
            return None
        
        key_hash = self._hash(key)
        
        # Binary search: find first position >= key_hash
        idx = bisect.bisect_left(self.ring, key_hash)
        
        # Wrap around if past the last position (circular ring)
        if idx >= len(self.ring):
            idx = 0
        
        position = self.ring[idx]
        return self.ring_map[position]
    
    def get_shard_for_replication(self, key: str, num_replicas: int = 3) -> List[str]:
        """
        Get N distinct shards for replication.
        
        Walk clockwise from key position, collecting unique shards.
        This is how Dynamo replicates across physical nodes.
        
        Note: This is CROSS-SHARD replication (different from Raft
        intra-shard replication from Episode 3.2). Each shard already
        replicates internally via Raft.
        """
        if not self.ring:
            return []
        
        key_hash = self._hash(key)
        idx = bisect.bisect_left(self.ring, key_hash)
        
        shards = []
        seen = set()
        
        for i in range(len(self.ring)):
            pos_idx = (idx + i) % len(self.ring)
            shard_id = self.ring_map[self.ring[pos_idx]]
            
            if shard_id not in seen:
                shards.append(shard_id)
                seen.add(shard_id)
                
                if len(shards) >= num_replicas:
                    break
        
        return shards
    
    def get_distribution(self) -> Dict[str, float]:
        """
        Show how evenly keys are distributed across shards.
        
        With 150 virtual nodes per shard, standard deviation is ~5%.
        Without virtual nodes, it could be 50%+ imbalanced.
        """
        if not self.ring:
            return {}
        
        shard_ranges: Dict[str, int] = {}
        total_range = 2**32
        
        for i in range(len(self.ring)):
            shard_id = self.ring_map[self.ring[i]]
            next_pos = self.ring[(i + 1) % len(self.ring)]
            curr_pos = self.ring[i]
            
            range_size = (next_pos - curr_pos) % total_range
            shard_ranges[shard_id] = shard_ranges.get(shard_id, 0) + range_size
        
        # Convert to percentages
        return {
            sid: (range_size / total_range) * 100 
            for sid, range_size in shard_ranges.items()
        }


# Demo: Show consistent hashing vs modulo hashing
def demo_comparison():
    """
    Show why consistent hashing is better than modulo.
    
    Modulo: Add 1 server ‚Üí 75% of keys move
    Consistent: Add 1 server ‚Üí ~33% of keys move (1/N)
    """
    ring = ConsistentHashRing(num_virtual_nodes=150)
    ring.add_shard("shard_A", ["node_1", "node_2", "node_3"])
    ring.add_shard("shard_B", ["node_4", "node_5", "node_6"])
    ring.add_shard("shard_C", ["node_7", "node_8", "node_9"])
    
    # Track 10,000 keys
    key_to_shard = {}
    for i in range(10000):
        key = f"user:{i}"
        key_to_shard[key] = ring.get_shard(key)
    
    # Add a new shard
    ring.add_shard("shard_D", ["node_10", "node_11", "node_12"])
    
    # How many keys moved?
    moved = 0
    for key, old_shard in key_to_shard.items():
        new_shard = ring.get_shard(key)
        if new_shard != old_shard:
            moved += 1
    
    print(f"Keys moved: {moved}/10000 ({moved/100:.1f}%)")
    # Expected: ~25% (1/4 of keys, since we went from 3 to 4 shards)
    
    print(f"Distribution: {ring.get_distribution()}")
    # Expected: ~25% per shard (with some variation)
```

### Component 2: The Shard Router

```python
class ShardRouter:
    """
    Routes client requests to the correct shard.
    
    This sits in front of our replicated KV stores from Episode 3.2.
    Each shard is a full Raft group with its own leader.
    
    Architecture:
    
    Client ‚Üí ShardRouter ‚Üí ConsistentHashRing ‚Üí Shard Leader ‚Üí Raft Group
    
    The router is stateless ‚Äî any number of routers can run
    in parallel for client-side load balancing.
    """
    
    def __init__(self, ring: ConsistentHashRing, 
                 shard_clusters: Dict[str, 'RaftCluster']):
        self.ring = ring
        self.clusters = shard_clusters  # shard_id ‚Üí RaftCluster from Ep 3.2
    
    def put(self, key: str, value: str) -> bool:
        """
        Route a PUT to the correct shard.
        
        Compare to Episode 3.2:
        - Ep 3.2: client.put(key, val) ‚Üí single Raft group
        - Ep 3.3: router.put(key, val) ‚Üí hash ‚Üí correct Raft group
        """
        shard_id = self.ring.get_shard(key)
        if shard_id is None:
            return False
        
        cluster = self.clusters.get(shard_id)
        if cluster is None:
            return False
        
        leader = cluster.get_leader()
        if leader is None:
            return False
        
        return leader.client_put(key, value)
    
    def get(self, key: str):
        """Route a GET to the correct shard."""
        shard_id = self.ring.get_shard(key)
        if shard_id is None:
            return None, False
        
        cluster = self.clusters.get(shard_id)
        if cluster is None:
            return None, False
        
        leader = cluster.get_leader()
        if leader is None:
            return None, False
        
        return leader.client_get(key)
    
    def delete(self, key: str) -> bool:
        """Route a DELETE to the correct shard."""
        shard_id = self.ring.get_shard(key)
        if shard_id is None:
            return False
        
        cluster = self.clusters.get(shard_id)
        if cluster is None:
            return False
        
        leader = cluster.get_leader()
        if leader is None:
            return False
        
        # Reuse Raft put with a tombstone command
        return leader.client_put(key, "__DELETED__")
    
    def multi_get(self, keys: List[str]) -> Dict[str, Optional[str]]:
        """
        Get multiple keys, potentially from different shards.
        
        Groups keys by shard, then queries each shard in parallel.
        This is a common optimization in production KV stores.
        """
        from collections import defaultdict
        from concurrent.futures import ThreadPoolExecutor
        
        # Group keys by shard
        shard_keys: Dict[str, List[str]] = defaultdict(list)
        for key in keys:
            shard_id = self.ring.get_shard(key)
            shard_keys[shard_id].append(key)
        
        # Query each shard in parallel
        results = {}
        
        with ThreadPoolExecutor() as executor:
            futures = {}
            for shard_id, shard_key_list in shard_keys.items():
                for key in shard_key_list:
                    futures[key] = executor.submit(self.get, key)
            
            for key, future in futures.items():
                value, found = future.result()
                results[key] = value if found else None
        
        return results
```

### Component 3: Shard Rebalancing

```python
class ShardRebalancer:
    """
    Handle adding/removing shards without downtime.
    
    When a new shard joins:
    1. Add to consistent hash ring
    2. Identify keys that should move to the new shard
    3. Stream those keys from old shards to new shard
    4. Update routing (atomic switch)
    5. Old shards delete moved keys
    
    This is the same pattern used by:
    - Redis Cluster (MIGRATE command)
    - Cassandra (streaming during join)
    - DynamoDB (split/merge partitions)
    """
    
    def __init__(self, router: ShardRouter):
        self.router = router
        self.migrating = False
    
    def add_shard(self, new_shard_id: str, new_nodes: List[str],
                  new_cluster: 'RaftCluster'):
        """
        Add a new shard to the cluster.
        
        Steps:
        1. Compute which keys will move to the new shard
        2. Stream those keys
        3. Switch routing atomically
        """
        self.migrating = True
        
        # Step 1: Snapshot current key ownership
        old_ownership = {}
        for key in self._get_all_keys():
            old_ownership[key] = self.router.ring.get_shard(key)
        
        # Step 2: Add new shard to ring
        self.router.ring.add_shard(new_shard_id, new_nodes)
        self.router.clusters[new_shard_id] = new_cluster
        
        # Step 3: Find keys that moved
        keys_to_migrate = []
        for key, old_shard in old_ownership.items():
            new_shard = self.router.ring.get_shard(key)
            if new_shard != old_shard:
                keys_to_migrate.append((key, old_shard, new_shard))
        
        # Step 4: Stream keys to new shard
        for key, old_shard, new_shard in keys_to_migrate:
            value, found = self._read_from_shard(old_shard, key)
            if found and value is not None:
                self._write_to_shard(new_shard, key, value)
        
        # Step 5: Cleanup old keys (async, in background)
        for key, old_shard, new_shard in keys_to_migrate:
            self._delete_from_shard(old_shard, key)
        
        self.migrating = False
        
        print(f"[Rebalancer] Migrated {len(keys_to_migrate)} keys to {new_shard_id}")
    
    def _get_all_keys(self):
        """Get all keys across all shards (for migration planning)."""
        # In production: use shard metadata, not full key scan
        return []
    
    def _read_from_shard(self, shard_id, key):
        cluster = self.router.clusters.get(shard_id)
        if cluster:
            leader = cluster.get_leader()
            if leader:
                return leader.client_get(key)
        return None, False
    
    def _write_to_shard(self, shard_id, key, value):
        cluster = self.router.clusters.get(shard_id)
        if cluster:
            leader = cluster.get_leader()
            if leader:
                leader.client_put(key, value)
    
    def _delete_from_shard(self, shard_id, key):
        cluster = self.router.clusters.get(shard_id)
        if cluster:
            leader = cluster.get_leader()
            if leader:
                leader.client_put(key, "__DELETED__")
```

---

## 4. Failure Modes in Sharded DistKV

### Failure Mode 1: Hot Shard

```python
# Scenario: Celebrity's profile is accessed 1M times/sec
# All requests hash to the same shard ‚Üí shard overloaded
#
# Detection:
# - Monitor per-shard request rate (P99 latency spikes)
# - Alert when any shard exceeds 2x average load
#
# Solutions:

class HotShardHandler:
    """Three strategies for hot shards."""
    
    def strategy_1_read_replicas(self, hot_shard_id):
        """
        Add read-only replicas for the hot shard.
        Reads spread across replicas, writes still go to leader.
        """
        # Each Raft follower from Episode 3.2 can serve reads
        # Add more followers to the hot shard's Raft group
        pass
    
    def strategy_2_shard_splitting(self, hot_shard_id):
        """
        Split hot shard into two smaller shards.
        Each gets half the key range.
        
        Used by: DynamoDB (adaptive capacity), CockroachDB (range splitting)
        """
        # Split the key range at midpoint
        # Move half the keys to a new shard
        # Update consistent hash ring
        pass
    
    def strategy_3_client_caching(self, hot_key):
        """
        Cache hot keys at the client/router level.
        Short TTL (1-5 seconds) dramatically reduces shard load.
        
        Used by: Every CDN ever built
        """
        # Router caches GET responses for hot keys
        # TTL-based invalidation
        # Solves read hot spots (not write hot spots)
        pass
```

### Failure Mode 2: Resharding Avalanche

```python
# Scenario: Add a new shard ‚Üí 25% of keys need to migrate
# All migrating keys become temporarily unavailable
# Clients flood other shards with retries ‚Üí cascading overload
#
# Fix: Gradual migration with "double-read" during transition

class GradualMigration:
    """
    During migration, reads check both old and new shard.
    Writes go to new shard only. Migration happens in background.
    """
    
    def __init__(self):
        self.migrating_keys = set()
        self.migration_map = {}  # key ‚Üí (old_shard, new_shard)
    
    def get_during_migration(self, key):
        """
        During migration, try new shard first, fall back to old.
        
        This avoids "key not found" errors during migration.
        """
        if key in self.migration_map:
            old_shard, new_shard = self.migration_map[key]
            
            # Try new shard first
            value, found = self._read_from_shard(new_shard, key)
            if found:
                return value, True
            
            # Fall back to old shard
            return self._read_from_shard(old_shard, key)
        
        # Not migrating ‚Äî normal read
        return self._normal_read(key)
```

### Failure Mode 3: Metadata Service Failure

```python
# Scenario: The service that tracks "which shard owns which key range" crashes
# Without metadata: router doesn't know where to send requests
#
# Solutions:
# 1. Client-side caching of ring state (Redis Cluster approach)
# 2. Gossip protocol for metadata propagation (Cassandra approach)
# 3. Replicated metadata service ‚Äî use Raft! (etcd/ZooKeeper approach)

class MetadataService:
    """
    Stores cluster metadata: shard assignments, node health, ring state.
    
    This is itself a replicated KV store (our Episode 3.2 system!).
    It's "turtles all the way down" ‚Äî the metadata service uses
    the same Raft replication we built earlier.
    """
    
    def __init__(self):
        # Metadata stored in a small, dedicated Raft group
        self.ring_state = None
        self.shard_assignments = {}
        self.node_health = {}
    
    def get_ring(self) -> ConsistentHashRing:
        """Get current ring state (cached by clients)."""
        return self.ring_state
    
    def update_ring(self, new_ring: ConsistentHashRing):
        """
        Update ring state (requires Raft consensus).
        All routers will pick up the new ring via polling/subscription.
        """
        self.ring_state = new_ring
```

---

## 5. Production Hardening

### Optimization 1: Range-Based Sharding (Alternative to Hash)

```python
class RangeBasedSharding:
    """
    Alternative to consistent hashing: range-based sharding.
    
    Hash-based: hash(key) ‚Üí shard (good for point lookups)
    Range-based: key range ‚Üí shard (good for range queries)
    
    CockroachDB, TiDB, Spanner use range-based sharding.
    DynamoDB, Cassandra use hash-based sharding.
    
    Trade-offs:
    - Hash: Even distribution, no range queries
    - Range: Range queries supported, risk of hot spots on sequential keys
    """
    
    def __init__(self):
        self.ranges = []  # [(start_key, end_key, shard_id), ...]
    
    def get_shard(self, key: str) -> Optional[str]:
        """Binary search for the range containing this key."""
        left, right = 0, len(self.ranges) - 1
        
        while left <= right:
            mid = (left + right) // 2
            start, end, shard_id = self.ranges[mid]
            
            if key < start:
                right = mid - 1
            elif key > end:
                left = mid + 1
            else:
                return shard_id
        
        return None
    
    def range_query(self, start_key: str, end_key: str) -> List[str]:
        """
        Find all shards that overlap with the query range.
        
        This is impossible with hash-based sharding!
        Range-based sharding enables queries like:
        "Get all users with IDs between 1000 and 2000"
        """
        shards = []
        for range_start, range_end, shard_id in self.ranges:
            if range_start <= end_key and range_end >= start_key:
                shards.append(shard_id)
        return shards
```

### Optimization 2: Bounded-Load Consistent Hashing

```python
class BoundedLoadHashRing(ConsistentHashRing):
    """
    Google's improvement to consistent hashing (2017 paper).
    
    Problem: Even with virtual nodes, some shards get 2x load
    Solution: Cap each shard's load at (1 + Œµ) √ó average
    
    If a shard is over-capacity, redirect to next shard on ring.
    This guarantees no shard gets more than (1 + Œµ) of fair share.
    """
    
    def __init__(self, epsilon: float = 0.25, **kwargs):
        super().__init__(**kwargs)
        self.epsilon = epsilon
        self.shard_load: Dict[str, int] = {}
    
    def get_shard_bounded(self, key: str) -> Optional[str]:
        """Get shard with bounded load guarantee."""
        if not self.ring:
            return None
        
        key_hash = self._hash(key)
        idx = bisect.bisect_left(self.ring, key_hash)
        
        avg_load = sum(self.shard_load.values()) / max(len(self.shards), 1)
        max_load = avg_load * (1 + self.epsilon)
        
        # Walk clockwise until we find a shard under capacity
        for i in range(len(self.ring)):
            pos_idx = (idx + i) % len(self.ring)
            shard_id = self.ring_map[self.ring[pos_idx]]
            
            current_load = self.shard_load.get(shard_id, 0)
            if current_load < max_load:
                self.shard_load[shard_id] = current_load + 1
                return shard_id
        
        # Fallback: all shards at capacity (shouldn't happen in practice)
        return self.ring_map[self.ring[idx % len(self.ring)]]
```

### Optimization 3: Jump Hash (Minimal Memory)

```python
def jump_consistent_hash(key: int, num_buckets: int) -> int:
    """
    Google's Jump Consistent Hash (2014 paper).
    
    Properties:
    - O(ln N) time, O(1) memory (no ring structure!)
    - Near-perfect balance
    - Minimal disruption on bucket count change
    
    Limitation: Only works with sequential bucket IDs (0, 1, 2, ...)
    Can't remove a bucket from the middle.
    """
    b = -1
    j = 0
    
    while j < num_buckets:
        b = j
        key = ((key * 2862933555777941757) + 1) & 0xFFFFFFFFFFFFFFFF
        j = int((b + 1) * (1 << 31) / ((key >> 33) + 1))
    
    return b

# Used by: Google's internal systems
# Trade-off: Elegant but can't remove arbitrary shards
# Best for: Caching layers where shards are numbered 0..N-1
```

---

## 6. How Real Systems Shard

```python
real_systems = {
    "Redis Cluster": {
        "Method": "Hash slots (16,384 fixed slots assigned to nodes)",
        "Lookup": "CRC16(key) % 16384 ‚Üí slot ‚Üí node",
        "Rebalance": "Migrate slots between nodes",
        "Range queries": "No (hash-based)",
        "Episode parallel": "Consistent hash ring (our approach)"
    },
    "Cassandra": {
        "Method": "Consistent hashing with virtual nodes (vnodes)",
        "Lookup": "Murmur3Hash(partition key) ‚Üí token range ‚Üí node",
        "Rebalance": "Add node ‚Üí takes ranges from neighbors",
        "Range queries": "Within partition only",
        "Episode parallel": "Our ConsistentHashRing"
    },
    "CockroachDB": {
        "Method": "Range-based sharding (ordered key ranges)",
        "Lookup": "Key ‚Üí range ‚Üí Raft group ‚Üí leader",
        "Rebalance": "Split range when too large, merge when too small",
        "Range queries": "Yes (ordered key ranges)",
        "Episode parallel": "Our RangeBasedSharding + Raft from Ep 3.2"
    },
    "DynamoDB": {
        "Method": "Hash-based with adaptive capacity",
        "Lookup": "Hash(partition key) ‚Üí partition",
        "Rebalance": "Split hot partitions automatically",
        "Range queries": "Within partition (sort key)",
        "Episode parallel": "Our Hot Shard strategies"
    }
}
```

---

## 7. Interview Cheatsheet

```python
interview_qa = {
    "Q: Why not just use modulo hashing?":
    "A: Adding/removing servers moves ~N-1/N keys. "
    "Consistent hashing moves only ~1/N keys. "
    "At scale, this is the difference between '5 minutes of migration' "
    "and '30 minutes of downtime.'",
    
    "Q: How do you handle hot shards?":
    "A: Three strategies: (1) Read replicas for read-heavy hot spots, "
    "(2) Shard splitting to divide the key range, "
    "(3) Client-side caching with short TTL for extremely hot keys.",
    
    "Q: Hash-based vs range-based sharding?":
    "A: Hash-based: uniform distribution, no range queries (DynamoDB, Cassandra). "
    "Range-based: supports range queries, risk of hot spots on sequential keys "
    "(CockroachDB, Spanner). Choose based on access patterns.",
    
    "Q: How do you rebalance without downtime?":
    "A: Gradual migration with double-read during transition. "
    "New writes go to new shard, reads check both. "
    "Background migration moves remaining data.",
    
    "Q: What about cross-shard queries?":
    "A: Single-key operations route to one shard (fast). "
    "Multi-key operations scatter-gather across shards (slower). "
    "Cross-shard ACID transactions require coordination ‚Äî Episode 3.4.",
    
    "Q: Virtual nodes ‚Äî why 150 per physical node?":
    "A: Empirically gives <5% standard deviation in load distribution. "
    "10 vnodes ‚Üí ~20% deviation. 150+ ‚Üí ~5%. More vnodes = more memory for ring."
}
```

### Production Checklist

```markdown
‚úÖ Consistent hash ring with virtual nodes
‚úÖ Binary search shard lookup (O(log V))
‚úÖ Graceful shard addition/removal
‚úÖ Double-read during migration for zero downtime
‚úÖ Hot shard detection and mitigation
‚úÖ Bounded-load hashing for fairness guarantee
‚úÖ Per-shard Raft replication (from Episode 3.2)
‚úÖ Metadata service for ring state
‚úÖ Client-side ring caching
‚úÖ Multi-key scatter-gather support
```

---

## 8. What's Next

```
WHAT YOU BUILT TODAY:
DistKV v3 ‚Äî a sharded, replicated key-value store:
- Consistent hash ring distributes keys across shards
- Each shard is a full Raft group (from Episode 3.2)
- Client router hashes keys and sends to correct shard
- Rebalancing adds/removes shards with minimal disruption

COMPARE TO PREVIOUS EPISODES:
3.1: Single node: PUT ‚Üí WAL ‚Üí Memtable
3.2: Replicated: PUT ‚Üí Raft ‚Üí Majority ACK ‚Üí Apply
3.3: Sharded: PUT ‚Üí Hash ‚Üí Correct Shard ‚Üí Raft ‚Üí Apply

You now have horizontal scaling:
- 3 shards √ó 3 replicas each = 9 nodes
- Each shard handles 1/3 of keys
- Write throughput scales linearly with shard count

BUT: What if a bank transfer moves money between two accounts
     on DIFFERENT shards? PUT to shard A must succeed AND
     PUT to shard B must succeed ‚Äî atomically.
     
     A crash between the two PUTs leaves money in limbo.
     
     This is the distributed transaction problem.

NEXT: Episode 3.4 ‚Äî Distributed Transactions: ACID Across Shards
We'll add a transaction coordinator that ensures cross-shard 
operations either ALL succeed or ALL roll back.
No money disappears. No inventory oversold.

The question changes from "How do we handle more data?"
to "How do we keep data correct across multiple shards?"
```

---

*"Sharding is how you scale beyond one machine. Consistent hashing is how you do it without breaking everything every time you add a server."*
