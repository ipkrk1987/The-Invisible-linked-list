# Episode 3.8 — The Grand Assembly: Everything Connects
## From LRU Cache to DistKV — The Full Request Lifecycle

**Season 3 — Distributed Systems: Building DistKV**

---

## Previously on DistKV... (ALL of it)

Across seven episodes, we built a distributed key-value store from scratch:

| Episode | What We Built | Why We Needed It |
|---------|--------------|-----------------|
| 3.1 | Single-node KV with WAL | Foundation — survive crashes |
| 3.2 | Raft replication | Survive node failures |
| 3.3 | Consistent hash sharding | Scale beyond one machine |
| 3.4 | Distributed transactions | ACID across shards |
| 3.5 | Tunable consistency | Trade consistency for speed |
| 3.6 | Vector clocks + CRDTs | Resolve conflicts when replicas diverge |
| 3.7 | Multi-region with HLC | Serve users globally |

**Today:** We trace a SINGLE request through every layer, compare DistKV to four real-world databases, and run cascading failure scenarios that test every component we've built.

**Arc So Far: COMPLETE**
- **3.1**: Single-node KV store ✅
- **3.2**: Raft replication ✅
- **3.3**: Sharding ✅
- **3.4**: Distributed transactions ✅
- **3.5**: Consistency models ✅
- **3.6**: Conflict resolution ✅
- **3.7**: Going global ✅
- **3.8**: Grand Assembly ← THE FINALE

---

## 1. The Hook: The Full Architecture

### What We've Actually Built

```
                         ┌─────────────────────────────────────────┐
                         │           DistKV v8 — Final             │
                         └─────────────────────────────────────────┘
                         
 Client (Tokyo)                     Client (Virginia)
     │                                    │
     ▼                                    ▼
 ┌──────────┐                       ┌──────────┐
 │  Region   │   Cross-Region       │  Region   │
 │  Router   │──Replication (3.7)───│  Router   │
 │ (AP-Tokyo)│   HLC ordering       │(US-East)  │
 └────┬──────┘                      └────┬──────┘
      │                                  │
      ▼                                  ▼
 ┌──────────┐                       ┌──────────┐
 │Consistency│                      │Consistency│
 │ Manager   │   (3.5 + 3.6)       │ Manager   │
 │           │   Quorum / CRDTs    │           │
 └────┬──────┘                      └────┬──────┘
      │                                  │
      ▼                                  ▼
 ┌──────────┐                       ┌──────────┐
 │  Shard    │                      │  Shard    │
 │  Router   │   (3.3)             │  Router   │
 │Hash Ring  │   Consistent Hash   │Hash Ring  │
 └────┬──────┘                      └────┬──────┘
      │                                  │
      ▼                                  ▼
 ┌─────────────────────┐           ┌─────────────────────┐
 │    Shard 0          │           │    Shard 0           │
 │ ┌───────┐ ┌───────┐ │          │ ┌───────┐ ┌───────┐  │
 │ │Leader │ │Follow │ │          │ │Leader │ │Follow │  │
 │ │(Raft) │ │ er(s) │ │          │ │(Raft) │ │ er(s) │  │
 │ └──┬────┘ └───────┘ │          │ └──┬────┘ └───────┘  │
 │    │     (3.2)       │          │    │     (3.2)        │
 │    ▼                 │          │    ▼                  │
 │ ┌───────┐            │          │ ┌───────┐             │
 │ │  WAL  │ → Memtable │          │ │  WAL  │ → Memtable  │
 │ │(3.1)  │ → SSTable  │          │ │(3.1)  │ → SSTable   │
 │ └───────┘            │          │ └───────┘              │
 └─────────────────────┘           └─────────────────────┘
```

---

## 2. The LeetCode Seed: LRU Cache

```python
# LeetCode #146: LRU Cache
# Design a cache with O(1) get and put, evicting least recently used.
# This is the FINAL optimization layer atop everything we've built.

from collections import OrderedDict

class LRUCache:
    """
    The connection to DistKV:
    
    After 7 episodes of distributed infrastructure, the most 
    impactful optimization is often the simplest: DON'T hit the
    distributed system at all. Cache hot keys locally.
    
    This is why every production KV store has a caching layer:
    - DynamoDB Accelerator (DAX)
    - Redis as a cache in front of Cassandra
    - Memcached in front of MySQL at Facebook
    
    The LRU cache sits AT THE CLIENT or AT THE ROUTER,
    intercepting reads before they traverse the entire stack.
    """
    
    def __init__(self, capacity: int):
        self.capacity = capacity
        self.cache = OrderedDict()
        self.hits = 0
        self.misses = 0
    
    def get(self, key: int) -> int:
        if key in self.cache:
            self.hits += 1
            self.cache.move_to_end(key)
            return self.cache[key]
        self.misses += 1
        return -1
    
    def put(self, key: int, value: int) -> None:
        if key in self.cache:
            self.cache.move_to_end(key)
        self.cache[key] = value
        if len(self.cache) > self.capacity:
            self.cache.popitem(last=False)
    
    def hit_rate(self) -> float:
        total = self.hits + self.misses
        return self.hits / total if total > 0 else 0.0


# Where caching fits in DistKV:
#
# WITHOUT cache:
#   Client → Router → Raft Leader → WAL → Memtable → Response
#   Latency: 5-200ms (depending on region)
#
# WITH cache:
#   Client → [LRU Cache] → HIT? → Response (0.01ms!)
#                        → MISS? → Router → Raft → ... (full path)
#
# For read-heavy workloads (95% reads), a cache with 80% hit rate
# reduces average latency by 4x and backend load by 5x.
```

---

## 3. The Complete Request Lifecycle

### Tracing a Single Write: `PUT("user:alice", "{name: Alice, city: Tokyo}")`

```python
def trace_write_lifecycle():
    """
    Every layer from our 7 episodes, in sequence.
    
    Client is in Tokyo. Primary region is US-East.
    Key "user:alice" maps to Shard 2.
    Consistency level: QUORUM.
    """
    
    # ═══════════════════════════════════════════════════
    # STEP 1: Client-Side Cache Check (Episode 3.8)
    # ═══════════════════════════════════════════════════
    # 
    # On WRITE: invalidate cache entry (if present)
    # Cache invalidation is one of the "two hard things in CS"
    #
    # client_cache.invalidate("user:alice")
    # Latency so far: 0ms
    
    # ═══════════════════════════════════════════════════
    # STEP 2: Region Router (Episode 3.7)
    # ═══════════════════════════════════════════════════
    #
    # Client is in Tokyo. For QUORUM write, must route to
    # primary region (US-East) because that's where the
    # shard leader is.
    #
    # region = router.route_write("user:alice", "ap-tokyo")
    # → "us-east" (primary)
    #
    # Network hop: Tokyo → US-East = ~150ms
    # Latency so far: ~150ms
    
    # ═══════════════════════════════════════════════════
    # STEP 3: Consistency Manager (Episode 3.5)
    # ═══════════════════════════════════════════════════
    #
    # QUORUM consistency: W = 2 (for N=3)
    # Write must be acknowledged by 2 of 3 replicas.
    #
    # consistency_mgr.write("user:alice", value, 
    #                       ConsistencyLevel.QUORUM)
    # → Delegates to Raft (which guarantees majority ACK)
    # Latency added: ~0ms (just routing logic)
    
    # ═══════════════════════════════════════════════════
    # STEP 4: Shard Router (Episode 3.3)
    # ═══════════════════════════════════════════════════
    #
    # Consistent hash ring determines which shard owns the key.
    #
    # shard_id = ring.get_shard("user:alice")
    # → Shard 2 (hash("user:alice") lands in Shard 2's range)
    # → Route to Shard 2's Raft leader
    # Latency added: ~0.01ms (hash + binary search)
    
    # ═══════════════════════════════════════════════════
    # STEP 5: Transaction Check (Episode 3.4)
    # ═══════════════════════════════════════════════════
    #
    # Single-key write → no cross-shard transaction needed.
    # If this were a multi-key write spanning shards:
    #   → Use Percolator or 2PC (Episode 3.4)
    # 
    # tx_mgr.classify_transaction(["user:alice"])
    # → "single-shard" → direct Raft write
    # Latency added: ~0ms
    
    # ═══════════════════════════════════════════════════
    # STEP 6: Raft Consensus (Episode 3.2)
    # ═══════════════════════════════════════════════════
    #
    # Shard 2 Leader receives the write:
    # 1. Append to Raft log
    # 2. Send AppendEntries to 2 followers
    # 3. Wait for 1 follower ACK (majority = 2/3)
    # 4. Commit the entry
    # 5. Apply to state machine (DistKV)
    #
    # Latency added: ~2-5ms (intra-datacenter replication)
    
    # ═══════════════════════════════════════════════════
    # STEP 7: WAL + Memtable (Episode 3.1)
    # ═══════════════════════════════════════════════════
    #
    # The Raft state machine application:
    # 1. Append to WAL (fsync for durability)
    # 2. Write to Memtable (sorted in-memory map)
    # 3. If Memtable full → flush to SSTable on disk
    #
    # Latency added: ~1-2ms (fsync dominates)
    
    # ═══════════════════════════════════════════════════
    # STEP 8: Conflict Check (Episode 3.6)
    # ═══════════════════════════════════════════════════
    #
    # For QUORUM writes through Raft: conflicts handled by
    # Raft's leader-only writes. No concurrent write conflicts.
    #
    # For EVENTUAL consistency (multi-leader):
    #   → Vector clock updated
    #   → Conflict detection deferred to read time
    #
    # Latency added: ~0ms (Raft eliminates write conflicts)
    
    # ═══════════════════════════════════════════════════
    # STEP 9: HLC Timestamp (Episode 3.7)
    # ═══════════════════════════════════════════════════
    #
    # Assign HLC timestamp for cross-region ordering:
    # ts = hlc.now()  → HLC(1703001234567.0@us-east)
    #
    # Latency added: ~0ms
    
    # ═══════════════════════════════════════════════════
    # STEP 10: Cross-Region Replication (Episode 3.7)
    # ═══════════════════════════════════════════════════
    #
    # Semi-sync: replicate to closest region first.
    # US-East → EU-West: ~90ms
    # EU-West ACKs → replicate to AP-Tokyo async.
    #
    # For QUORUM write: don't wait for cross-region ACK
    # (local Raft quorum is sufficient)
    # For STRONG write: wait for semi-sync ACK (+90ms)
    #
    # Latency added: 0ms (QUORUM) or ~90ms (STRONG)
    
    # ═══════════════════════════════════════════════════
    # STEP 11: Response to Client
    # ═══════════════════════════════════════════════════
    #
    # Total latency breakdown:
    #
    # Tokyo → US-East network:       ~150ms
    # Shard routing:                  ~0.01ms
    # Raft consensus:                 ~3ms
    # WAL fsync:                      ~1ms
    # Response back Tokyo → US-East:  ~150ms
    # ─────────────────────────────────────────
    # Total (QUORUM):                 ~304ms
    #
    # With local region write (multi-leader):
    # Total:                          ~4ms (!)
    # But: must handle conflicts (Episode 3.6)
    
    print("Write lifecycle complete:")
    print("  Quorum (cross-region):  ~304ms")
    print("  Multi-leader (local):   ~4ms + conflict risk")
    print("  Strong (semi-sync):     ~394ms")
```

### Tracing a Single Read: `GET("user:alice")` with Different Consistency Levels

```python
def trace_read_lifecycle():
    """
    Same key, four different consistency levels.
    Client is in Tokyo.
    """
    
    reads = {
        "STRONG": {
            "path": "Tokyo → US-East → Shard 2 Leader → Response → Tokyo",
            "latency": "~304ms",
            "guarantee": "Sees the absolute latest committed value",
            "when": "Bank balance, inventory count"
        },
        "QUORUM": {
            "path": "Tokyo → US-East → 2/3 Shard 2 nodes → Compare → Response → Tokyo",
            "latency": "~306ms (slightly more than STRONG due to comparison)",
            "guarantee": "Sees latest if W+R > N (which it is: 2+2 > 3)",
            "when": "Important data, but leader might be unavailable"
        },
        "SESSION": {
            "path": "Tokyo → AP-Tokyo replica → Check version ≥ last write → Response",
            "latency": "~5ms (local replica!)",
            "guarantee": "Sees at least your own latest write",
            "when": "User profile (user should see their own changes)"
        },
        "EVENTUAL": {
            "path": "Tokyo → AP-Tokyo replica → Response (any version)",
            "latency": "~3ms (local replica, no version check!)",
            "guarantee": "May see stale data (milliseconds to seconds old)",
            "when": "Social media likes, search results, analytics"
        }
    }
    
    for level, details in reads.items():
        print(f"\n{level}:")
        print(f"  Path:      {details['path']}")
        print(f"  Latency:   {details['latency']}")
        print(f"  Guarantee: {details['guarantee']}")
        print(f"  Use when:  {details['when']}")
```

---

## 4. DistKV vs Real-World Systems

```python
comparison = {
    "Feature": [
        "Storage engine",
        "Replication",
        "Sharding",
        "Transactions",
        "Consistency",
        "Conflict Resolution",
        "Global distribution",
        "Time model"
    ],
    "DistKV (Ours)": [
        "WAL + Memtable + SSTable (3.1)",
        "Raft consensus (3.2)",
        "Consistent hash ring (3.3)",
        "2PC + Percolator + Saga (3.4)",
        "Tunable: Strong→Eventual (3.5)",
        "Vector clocks + CRDTs (3.6)",
        "Multi-region + failover (3.7)",
        "HLC (3.7)"
    ],
    "Cassandra": [
        "Memtable + SSTable (LSM-tree)",
        "Leaderless, gossip protocol",
        "Consistent hashing (virtual nodes)",
        "Lightweight transactions (Paxos)",
        "Tunable: ONE → ALL",
        "Last-Write-Wins (wall clock)",
        "Multi-DC async replication",
        "Wall clock (NTP)"
    ],
    "CockroachDB": [
        "RocksDB (LSM-tree)",
        "Raft (per range)",
        "Range-based sharding",
        "Serializable (Percolator-based)",
        "Serializable only",
        "Not needed (single-leader per range)",
        "Multi-region, locality-aware",
        "HLC (exactly what we built!)"
    ],
    "Google Spanner": [
        "SSTable + B-tree hybrid",
        "Paxos (per split)",
        "Range-based splits",
        "2PC across Paxos groups",
        "External consistency (linearizable+)",
        "Not needed (single-leader per split)",
        "True global distribution",
        "TrueTime (GPS + atomic clocks)"
    ],
    "DynamoDB": [
        "B-tree (proprietary)",
        "Multi-Paxos",
        "Hash + range key partitioning",
        "Single-item only (no cross-partition)",
        "Eventually consistent (default) or strong",
        "Last-Write-Wins",
        "Global Tables (multi-active)",
        "Wall clock + logical"
    ]
}

# What DistKV got RIGHT (matches production systems):
# ✓ LSM-tree storage (same as Cassandra, CockroachDB)
# ✓ Raft consensus (same as CockroachDB, TiKV)
# ✓ Consistent hashing (same as Cassandra, DynamoDB)
# ✓ Percolator transactions (same as CockroachDB, TiDB)
# ✓ Tunable consistency (same as Cassandra, DynamoDB, Cosmos DB)
# ✓ HLC (same as CockroachDB)
# 
# What production systems have that DistKV doesn't:
# ✗ Compaction strategies (leveled, tiered, FIFO)
# ✗ Bloom filters per SSTable for read optimization
# ✗ Snapshots and point-in-time recovery
# ✗ Schema / SQL layer (CockroachDB, Spanner add this)
# ✗ Connection pooling, load balancing, observability
# ✗ Years of battle-testing and edge-case handling
```

---

## 5. Cascading Failure Scenarios

### Scenario 1: "The Prime Day Meltdown"

```python
def prime_day_scenario():
    """
    An e-commerce sale causes cascading failures.
    Tests every layer of DistKV.
    """
    
    timeline = """
    T=0:    Sale starts. Traffic 10x spike.
    
    T+5s:   HOT SHARD (Episode 3.3)
            Key "inventory:popular-item" gets 50K reads/sec.
            Shard 3 is overwhelmed.
            
            Fix: Read from followers (Episode 3.5, EVENTUAL read)
            Result: Shard leader offloaded, followers serve reads.
    
    T+30s:  STALE READ STAMPEDE
            1000 users see "in stock" (stale data from followers).
            All 1000 click "Buy Now" simultaneously.
            
            Fix: Inventory decrement uses STRONG consistency.
            Only ONE gets the last item. 999 get "out of stock."
            
    T+45s:  CROSS-SHARD TRANSACTION STORM (Episode 3.4)
            "Buy" touches inventory shard + order shard + payment shard.
            Percolator transactions create lock contention.
            
            Fix: Batch writes, exponential backoff on conflicts.
            Write-write conflict rate: 2% → manageable.
            
    T+2m:   RAFT LEADER DIES (Episode 3.2)
            Shard 3 leader OOM-killed (too many connections).
            
            Fix: Raft election. New leader in ~1.5 seconds.
            During election: writes to Shard 3 fail → client retries.
            
    T+3m:   REPLICATION LAG SPIKE (Episode 3.7)
            Cross-region replication falls 10 seconds behind.
            Tokyo users see items that are already sold.
            
            Fix: Bounded-staleness reads (Episode 3.5).
            If staleness > 2s, redirect to primary region.
            
    T+5m:   REGION FAILOVER (Episode 3.7)
            US-East overloaded → health check fails repeatedly.
            Failover to EU-West.
            
            Fix: RegionFailoverManager promotes EU-West.
            Lost writes: any async-replicated writes in last ~100ms.
            Recovery: reconcile with CRDTs when US-East recovers (3.6).
            
    T+10m:  STABILIZATION
            Traffic normalizes. All regions healthy.
            Replication catches up. CRDTs merge diverged state.
            Post-mortem: shard Shard 3, add capacity, adjust quorum.
    """
    print(timeline)
```

### Scenario 2: "The Network Partition"

```python
def network_partition_scenario():
    """
    A fiber cut splits US-East from EU-West and AP-Tokyo.
    Tests CAP theorem choices per consistency level.
    """
    
    timeline = """
    Partition: US-East <──X──> (EU-West, AP-Tokyo)
    
    US-East has:  Shard 0 Leader, Shard 2 Leader
    EU-West has:  Shard 1 Leader, replicas of Shard 0 & 2
    AP-Tokyo has: Replicas of all shards
    
    STRONG consistency clients:
      US-East: Can read Shard 0, 2 (leaders here). ✓
               Cannot read Shard 1 (leader in EU-West, unreachable). ✗
      EU-West: Can read Shard 1 (leader here). ✓
               Cannot read Shard 0, 2 (leaders in US-East). ✗
      → CP: Consistent but partially unavailable.
    
    EVENTUAL consistency clients:
      All regions: Can read ALL shards from local replicas. ✓
      But: Writes during partition diverge. ⚠
      → AP: Available but potentially inconsistent.
    
    SESSION consistency clients:
      If session node is in same partition as shard leader: ✓
      If session node is in different partition: Falls back to STRONG → ✗
    
    After partition heals:
      1. Cross-region replication catches up
      2. Conflicting writes resolved by Episode 3.6 strategies
      3. CRDT counters merge automatically
      4. Non-CRDT conflicts: LWW or return siblings to client
      5. System returns to normal within seconds
    """
    print(timeline)
```

---

## 6. The Season 3 Recap: One Data Structure to Rule Them All

```python
season_3_summary = """
╔══════════════════════════════════════════════════════════════╗
║                  SEASON 3: BUILDING DistKV                  ║
║          A Distributed Key-Value Store from Scratch         ║
╠══════════════════════════════════════════════════════════════╣
║                                                              ║
║  Every episode answered one question:                        ║
║                                                              ║
║  3.1: How do I store data and survive crashes?              ║
║       → WAL + Memtable + SSTable                            ║
║                                                              ║
║  3.2: How do I survive machine failures?                    ║
║       → Raft consensus: replicate to 3+ nodes               ║
║                                                              ║
║  3.3: How do I handle more data than one machine holds?     ║
║       → Consistent hashing: split keys across shards         ║
║                                                              ║
║  3.4: How do I update data across multiple shards atomically?║
║       → 2PC, Percolator, Saga patterns                       ║
║                                                              ║
║  3.5: How do I make reads faster when I don't need perfection?║
║       → Tunable consistency: quorum reads, eventual reads    ║
║                                                              ║
║  3.6: What happens when replicas have different data?        ║
║       → Vector clocks detect conflicts, CRDTs auto-resolve  ║
║                                                              ║
║  3.7: How do I serve users on every continent?              ║
║       → Multi-region replication, HLC for ordering           ║
║                                                              ║
║  3.8: How does it all fit together?                          ║
║       → Full lifecycle, real comparisons, failure cascades   ║
║                                                              ║
║  LeetCode to Production — the invisible distributed system: ║
║                                                              ║
║  #706 Design HashMap     → Production KV store              ║
║  #733 Flood Fill         → Leader → Follower replication     ║
║  #35  Search Insert Pos  → Hash ring binary search           ║
║  #23  Merge K Sorted     → Ordering commits across shards   ║
║  #169 Majority Element   → Quorum-based reads               ║
║  #207 Course Schedule    → Causality detection               ║
║  #253 Meeting Rooms II   → Clock uncertainty intervals       ║
║  #146 LRU Cache          → Client-side caching layer         ║
║                                                              ║
╚══════════════════════════════════════════════════════════════╝
"""
```

---

## 7. What You Can Build With This Knowledge

```python
career_applications = {
    "System Design Interviews": {
        "Topics covered": [
            "Design a distributed cache (Redis Cluster)",
            "Design a key-value store (DynamoDB/Cassandra)",
            "Design a URL shortener (sharding + consistent hashing)",
            "Design a distributed lock service (Raft consensus)",
            "Design a global chat system (multi-region + CRDTs)",
            "Design a distributed counter (G-Counter CRDT)",
            "Design a distributed transaction system (2PC + Saga)"
        ],
        "Secret": "Every system design question decomposes into "
                  "subsets of what we built in DistKV."
    },
    
    "Production Engineering": {
        "Skills gained": [
            "Debugging Raft election storms",
            "Diagnosing hot shards and resharding",
            "Choosing consistency levels per use case",
            "Understanding cross-region replication lag",
            "Analyzing cascading failure scenarios",
            "Reading CockroachDB/Cassandra architecture docs"
        ]
    },
    
    "Beyond Databases": {
        "Raft/Paxos": "etcd, Consul, ZooKeeper — all use consensus",
        "Sharding": "Kafka partitions, Elasticsearch shards",
        "CRDTs": "Figma real-time collaboration, Redis CRDTs",
        "Vector clocks": "Riak, Amazon Dynamo",
        "WAL": "PostgreSQL, SQLite, every serious database",
        "LSM-trees": "RocksDB, LevelDB, Cassandra, HBase",
        "HLC": "CockroachDB, YugabyteDB, TiDB"
    }
}
```

---

## 8. The Full Interview Cheatsheet

```python
final_interview_qa = {
    # ======================
    # STORAGE (Episode 3.1)
    # ======================
    "Q: Why use a WAL?":
    "A: Write-ahead logging ensures durability — write to an append-only "
    "log before updating in-memory state. On crash, replay the log. "
    "Append-only writes are sequential I/O = fast. Used by PostgreSQL, "
    "SQLite, and every serious database.",
    
    "Q: What is an LSM-tree?":
    "A: Log-Structured Merge-tree. Writes go to memory (Memtable), "
    "flush to sorted files (SSTables) when full, merge SSTables in "
    "background (compaction). Write-optimized — O(1) amortized writes "
    "vs B-tree's O(log N). Used by RocksDB, Cassandra, LevelDB.",
    
    # ======================
    # REPLICATION (Episode 3.2)
    # ======================
    "Q: How does Raft work?":
    "A: Leader election + log replication. Leader accepts writes, "
    "replicates to followers, commits when majority ACK. On leader failure, "
    "followers with most up-to-date log win election. 5 safety invariants "
    "guarantee that committed entries are never lost.",
    
    # ======================
    # SHARDING (Episode 3.3)
    # ======================
    "Q: How does consistent hashing work?":
    "A: Hash ring with virtual nodes. Each shard gets V positions on ring. "
    "Key's hash → walk clockwise to nearest shard. Adding/removing shard "
    "only moves ~1/N keys. Virtual nodes balance load. "
    "Used by DynamoDB, Cassandra, Memcached.",
    
    # ======================
    # TRANSACTIONS (Episode 3.4)
    # ======================
    "Q: What is 2PC and why is it problematic?":
    "A: Two-Phase Commit: Prepare (all participants lock and say YES/NO), "
    "then Commit (coordinator tells all to commit or abort). Problem: "
    "if coordinator crashes after Prepare but before Commit, participants "
    "are BLOCKED with locks held. Percolator avoids this with primary-lock.",
    
    # ======================
    # CONSISTENCY (Episode 3.5)
    # ======================
    "Q: What does 'tunable consistency' mean?":
    "A: Client chooses per-operation: Strong (read from leader), "
    "Quorum (W+R>N overlap), Session (read-your-writes), "
    "Eventual (any replica, fastest). Weaker consistency = lower latency. "
    "Choose the weakest level your business can tolerate.",
    
    # ======================
    # CONFLICTS (Episode 3.6)
    # ======================
    "Q: What are CRDTs?":
    "A: Conflict-Free Replicated Data Types. Merge is commutative, "
    "associative, idempotent — always converges regardless of merge order. "
    "Examples: G-Counter (grow-only counter), OR-Set (add-wins set). "
    "Used by Redis, Riak, Figma for real-time collaboration.",
    
    # ======================
    # GLOBAL (Episode 3.7)
    # ======================
    "Q: Spanner vs CockroachDB time models?":
    "A: Spanner: TrueTime with GPS+atomic clocks, bounded uncertainty, "
    "commit-wait guarantees external consistency. CockroachDB: HLC "
    "(wall clock + logical counter), no special hardware, same causal "
    "ordering but without commit-wait. HLC is 'good enough' for most.",
    
    # ======================
    # ARCHITECTURE (Episode 3.8)
    # ======================
    "Q: Design a distributed KV store":
    "A: Start with single-node (WAL + Memtable). Add Raft for HA. "
    "Shard with consistent hashing for scale. Add transactions if needed. "
    "Tune consistency per use case. Add CRDTs for multi-leader if going "
    "global. Use HLC for cross-region ordering. Cache at the client. "
    "This is literally what Cassandra, CockroachDB, and DynamoDB do."
}
```

---

## 9. Closing

```
WHAT WE BUILT ACROSS SEASON 3:

Episode 3.1: DistKV v1 — WAL, Memtable, TCP server
Episode 3.2: DistKV v2 + Raft — replicated, fault-tolerant
Episode 3.3: DistKV v3 + Sharding — horizontally scalable
Episode 3.4: DistKV v4 + Transactions — cross-shard ACID
Episode 3.5: DistKV v5 + Consistency — tunable per operation
Episode 3.6: DistKV v6 + Conflict Resolution — eventual consistency safe
Episode 3.7: DistKV v7 + Global — multi-region, HLC ordering
Episode 3.8: DistKV v8 + Cache + Full Assembly

From:  dict() in Python
To:    A system that rivals the architecture of DynamoDB,
       CockroachDB, and Cassandra.

From:  LeetCode #706 (Design HashMap)
To:    Handling 100K requests/sec across 3 continents.

The "invisible distributed system" was always there —
inside every LeetCode problem, waiting to be built.
```

---

*"You don't build a distributed system by reading about one. You build it by watching it BREAK — over and over — and adding exactly the layer that would have prevented the last failure. That's what we did for 8 episodes. That's what engineers do in production, every single day."*
