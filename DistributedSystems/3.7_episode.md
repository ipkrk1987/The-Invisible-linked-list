# Episode 3.7: Eventual Consistency & Conflict Resolution
## How DAGs and Vector Clocks Power Amazon Dynamo's "Always Writable" Database

**Season 3 — Distributed Systems: From Single Machine to Global Scale**

---

## The Hook: Real-World Production Failure

### The 2011 Amazon Shopping Cart Bug
**"Items randomly disappearing from carts, costing millions in lost sales"**

**The Incident**:
- Black Friday 2011: Amazon's shopping cart service had intermittent failures
- Users added items to cart → refreshed page → items gone
- Root cause: **Concurrent writes resolved incorrectly**
- Network partition caused replicas to diverge, then merge incorrectly

**What Actually Happened**:
```
Timeline:
10:00:00 - User adds iPhone to cart → writes to replica A
10:00:01 - Network partition splits A and B
10:00:02 - User adds MacBook to cart → writes to replica B
10:00:05 - Partition heals → A and B try to merge
          → Last-write-wins used wall-clock time
          → MacBook write "wins" (more recent timestamp)
          → iPhone silently discarded!
```

**The Critical Insight**: 
- Wall-clock timestamps are **not causally ordered** (clocks drift, NTP can go backward)
- "Last write wins" (LWW) **loses data silently** when concurrent writes happen
- **Strong consistency** (Spanner-style) would block writes during partition → violates "always writable" SLA

**The Lesson**: Eventual consistency needs **causal ordering** and **conflict detection**, not just timestamps.

---

## Part 1: The Theory of Eventual Consistency

### The CAP Theorem Trade-off

**CAP Theorem**: In a distributed system with network partitions, pick 2 of 3:
1. **Consistency**: All nodes see the same data
2. **Availability**: Every request gets a response
3. **Partition tolerance**: System works despite network failures

**Amazon's choice for shopping carts**:
- ✅ **Availability**: Must always accept writes (revenue loss otherwise)
- ✅ **Partition tolerance**: Network failures happen constantly at scale
- ❌ **Consistency**: Accept temporary divergence, resolve conflicts later

This is **eventual consistency**: replicas diverge temporarily, then converge.

### Why Strong Consistency Isn't Always the Answer

| System | Consistency Model | Why |
|--------|------------------|-----|
| **Bank account balance** | Strong (Spanner) | Wrong balance = legal liability |
| **Shopping cart** | Eventual (Dynamo) | Temporarily wrong cart ≪ cart unavailable |
| **Social media likes** | Eventual | Seeing 99 vs 100 likes doesn't matter |
| **Inventory count** | Strong | Overselling products = customer complaints |
| **Search index** | Eventual | Slightly stale results acceptable |

**The insight**: Different data has different consistency requirements.

### The Problem with Wall-Clock Timestamps

**Wall clocks are not monotonic**:
```python
# Server A
time.time()  # 1701950400.123
# NTP sync happens, clock jumps backward
time.time()  # 1701950395.000  # EARLIER!

# Now "later" events have "earlier" timestamps
# Last-write-wins loses the newer write
```

**Wall clocks drift**:
```
Server A clock: 10:00:00.000
Server B clock: 10:00:00.500  # 500ms ahead

Event on A at "10:00:01.000" (server A time)
Event on B at "10:00:01.200" (server B time)

Which happened first? Can't tell!
B's clock is ahead, so B's timestamp is always "later"
Even if A's event happened first causally
```

**The solution**: Use **logical clocks** (Lamport timestamps, vector clocks) that track **causality**, not wall-clock time.

---

## Part 2: Causality and Happens-Before Relation

### The Happens-Before Relation (→)

Event `a` **happens-before** event `b` (written `a → b`) if:
1. **Local ordering**: `a` and `b` on same process, and `a` occurs before `b` in that process
2. **Message passing**: `a` is sending a message, `b` is receiving that message
3. **Transitivity**: If `a → b` and `b → c`, then `a → c`

**Example**:
```
Process 1:  [Write x=1] ----send msg---→ (receive msg) [Read x]
Process 2:                               [Write y=2]

Events:
- Write x=1 → send msg (local ordering)
- send msg → receive msg (message passing)
- Write x=1 → Read x (transitivity)
- Write x=1 and Write y=2 are CONCURRENT (neither → the other)
```

**Concurrent events** (`a ∥ b`): Neither `a → b` nor `b → a`. These are **conflicts** that need resolution.

### Lamport Timestamps: Partial Solution

**Lamport timestamp** is a counter that increments on each event:

```python
class LamportClock:
    def __init__(self):
        self.timestamp = 0
    
    def increment(self):
        """Local event"""
        self.timestamp += 1
        return self.timestamp
    
    def send(self):
        """Sending message"""
        self.timestamp += 1
        return self.timestamp  # Include in message
    
    def receive(self, received_timestamp):
        """Receiving message"""
        self.timestamp = max(self.timestamp, received_timestamp) + 1
```

**Property**: If `a → b`, then `timestamp(a) < timestamp(b)`

**Problem**: The converse is NOT true! If `timestamp(a) < timestamp(b)`, we can't conclude `a → b`.

**Example**:
```
Process A: [Write x=1 @ TS=5]
Process B: [Write x=2 @ TS=6]

TS=5 < TS=6, but events are CONCURRENT (no message passed)
Can't use Lamport timestamps to detect concurrency!
```

### Vector Clocks: Complete Solution

A **vector clock** is a map from process IDs to timestamps:

```python
VectorClock = {
    "ProcessA": 5,
    "ProcessB": 3,
    "ProcessC": 7
}
```

**Rules**:
1. Each process maintains a vector clock
2. On local event: increment own counter
3. On send: increment own counter, send entire vector
4. On receive: merge vectors (take max for each process), then increment own counter

**Comparison**:
```python
def compare(vc1, vc2):
    """
    Returns:
      'before'     if vc1 < vc2 (vc1 → vc2)
      'after'      if vc1 > vc2 (vc2 → vc1)
      'concurrent' if vc1 ∥ vc2
    """
    less_or_equal = all(vc1.get(p, 0) <= vc2.get(p, 0) for p in vc2)
    greater_or_equal = all(vc1.get(p, 0) >= vc2.get(p, 0) for p in vc2)
    
    if less_or_equal and not greater_or_equal:
        return 'before'  # vc1 → vc2
    elif greater_or_equal and not less_or_equal:
        return 'after'   # vc2 → vc1
    else:
        return 'concurrent'  # vc1 ∥ vc2
```

**Example**:
```
VC1 = {A: 5, B: 3, C: 2}
VC2 = {A: 6, B: 3, C: 2}

VC1 < VC2 for all processes? NO (A: 5 ≤ 6, B: 3 ≤ 3, C: 2 ≤ 2) ✓
VC1 ≠ VC2? YES

Result: VC1 → VC2 (VC1 happened before VC2)
```

**Concurrent example**:
```
VC1 = {A: 5, B: 3, C: 2}
VC2 = {A: 4, B: 4, C: 2}

Neither VC1 < VC2 nor VC2 < VC1 (A is greater in VC1, B is greater in VC2)

Result: VC1 ∥ VC2 (concurrent, CONFLICT!)
```

Now let's build the system.

---

## Part 3: The LeetCode Seed — Topological Sort for Causality

The mapping: **Causal ordering = Topological sort of the happens-before DAG**

```python
# LeetCode #207: Course Schedule (topological sort)
def can_finish(num_courses, prerequisites):
    """
    Prerequisites define happens-before relation:
    [1, 0] means course 0 → course 1 (must take 0 before 1)
    
    Distributed systems: Events are courses, messages are prerequisites
    """
    from collections import defaultdict, deque
    
    graph = defaultdict(list)
    in_degree = [0] * num_courses
    
    for course, prereq in prerequisites:
        graph[prereq].append(course)
        in_degree[course] += 1
    
    # Topological sort (Kahn's algorithm)
    queue = deque([i for i in range(num_courses) if in_degree[i] == 0])
    order = []
    
    while queue:
        node = queue.popleft()
        order.append(node)
        
        for neighbor in graph[node]:
            in_degree[neighbor] -= 1
            if in_degree[neighbor] == 0:
                queue.append(neighbor)
    
    return len(order) == num_courses  # No cycles?

# In distributed systems:
# - Nodes = events (writes to shopping cart)
# - Edges = happens-before relations
# - Topological order = causal order
# - Cycle detection = circular causality (impossible, but bugs can create it)
```

**Key insight**: Vector clocks let us build the happens-before DAG dynamically, then topologically sort to find causal order.

---

## Part 4: Production System — Dynamo-Style Eventual Consistency

### Component 1: Vector Clock Implementation

```python
from typing import Dict, Optional, Any
from dataclasses import dataclass, field
from copy import deepcopy
import json

@dataclass
class VectorClock:
    """Vector clock for tracking causality."""
    
    clocks: Dict[str, int] = field(default_factory=dict)
    
    def increment(self, node_id: str):
        """Increment this node's counter."""
        self.clocks[node_id] = self.clocks.get(node_id, 0) + 1
    
    def merge(self, other: 'VectorClock'):
        """Merge with another vector clock (take max)."""
        for node_id, timestamp in other.clocks.items():
            self.clocks[node_id] = max(self.clocks.get(node_id, 0), timestamp)
    
    def compare(self, other: 'VectorClock') -> str:
        """
        Compare two vector clocks.
        Returns: 'before', 'after', 'concurrent', or 'equal'
        """
        if self.clocks == other.clocks:
            return 'equal'
        
        # Check if self < other
        self_less = True
        self_greater = True
        
        all_nodes = set(self.clocks.keys()) | set(other.clocks.keys())
        
        for node_id in all_nodes:
            self_val = self.clocks.get(node_id, 0)
            other_val = other.clocks.get(node_id, 0)
            
            if self_val > other_val:
                self_less = False
            if self_val < other_val:
                self_greater = False
        
        if self_less and not self_greater:
            return 'before'  # self → other
        elif self_greater and not self_less:
            return 'after'   # other → self
        else:
            return 'concurrent'  # self ∥ other (CONFLICT!)
    
    def copy(self) -> 'VectorClock':
        """Deep copy of vector clock."""
        return VectorClock(clocks=deepcopy(self.clocks))
    
    def __repr__(self):
        return f"VC({json.dumps(self.clocks)})"


@dataclass
class VersionedValue:
    """Value with vector clock for conflict detection."""
    
    value: Any
    vector_clock: VectorClock
    node_id: str  # Which node wrote this version
    
    def __repr__(self):
        return f"Ver(val={self.value}, vc={self.vector_clock}, node={self.node_id})"
```

### Component 2: Conflict Detection and Versioning

```python
from typing import List, Set, Tuple

class VersionedStore:
    """
    Multi-version store that keeps concurrent versions.
    Like Dynamo's versioned key-value store.
    """
    
    def __init__(self, node_id: str):
        self.node_id = node_id
        self.data: Dict[str, List[VersionedValue]] = {}  # key -> list of concurrent versions
        self.local_clock = VectorClock()
    
    def write(self, key: str, value: Any, context_vc: Optional[VectorClock] = None) -> VersionedValue:
        """
        Write a value with vector clock.
        
        If context_vc provided, this is a "causal write" (client saw previous version).
        Otherwise, it's a blind write.
        """
        # Increment local clock
        self.local_clock.increment(self.node_id)
        
        # If context provided, merge it
        if context_vc:
            self.local_clock.merge(context_vc)
            self.local_clock.increment(self.node_id)  # Increment again after merge
        
        # Create new version
        new_version = VersionedValue(
            value=value,
            vector_clock=self.local_clock.copy(),
            node_id=self.node_id
        )
        
        # Get existing versions
        if key not in self.data:
            self.data[key] = [new_version]
        else:
            # Remove versions that are causally dominated by new version
            self.data[key] = self._add_version(self.data[key], new_version)
        
        return new_version
    
    def _add_version(self, versions: List[VersionedValue], new_version: VersionedValue) -> List[VersionedValue]:
        """
        Add new version, removing causally dominated versions.
        Keep only concurrent versions (siblings).
        """
        kept_versions = []
        
        for existing in versions:
            comparison = existing.vector_clock.compare(new_version.vector_clock)
            
            if comparison == 'before':
                # existing → new_version (new dominates)
                # Discard existing
                continue
            elif comparison == 'after':
                # new_version → existing (existing dominates)
                # This shouldn't happen in normal operation
                # But keep existing
                kept_versions.append(existing)
            elif comparison == 'concurrent':
                # Conflict! Keep both versions
                kept_versions.append(existing)
            # 'equal' case: versions are identical, keep one
        
        # Add new version
        kept_versions.append(new_version)
        
        return kept_versions
    
    def read(self, key: str) -> Tuple[List[VersionedValue], VectorClock]:
        """
        Read all concurrent versions of a key.
        
        Returns:
          - List of versions (siblings if conflicts exist)
          - Context vector clock (for next write)
        """
        if key not in self.data:
            return [], VectorClock()
        
        versions = self.data[key]
        
        # Compute context: merge all version clocks
        context = VectorClock()
        for version in versions:
            context.merge(version.vector_clock)
        
        return versions, context
    
    def has_conflicts(self, key: str) -> bool:
        """Check if key has concurrent versions (conflict)."""
        if key not in self.data:
            return False
        return len(self.data[key]) > 1


# Example: Shopping cart with conflicts
store = VersionedStore(node_id="replica_A")

# Write 1: Add iPhone
v1 = store.write("cart:user123", ["iPhone"])
print(f"v1: {v1}")  # VC({A: 1})

# Write 2: Add MacBook (using v1 as context)
v2 = store.write("cart:user123", ["iPhone", "MacBook"], context_vc=v1.vector_clock)
print(f"v2: {v2}")  # VC({A: 2})

# Now simulate concurrent write from another replica
store2 = VersionedStore(node_id="replica_B")
store2.local_clock.clocks = {"A": 1, "B": 0}  # Saw v1

# Concurrent write: Add AirPods
v3 = store2.write("cart:user123", ["iPhone", "AirPods"], context_vc=v1.vector_clock)
print(f"v3: {v3}")  # VC({A: 1, B: 1})

# Merge: Replica A receives v3
store.data["cart:user123"] = store._add_version(store.data["cart:user123"], v3)

# Check for conflicts
versions, context = store.read("cart:user123")
print(f"Versions: {versions}")
# Versions: [Ver(val=['iPhone', 'MacBook'], vc=VC({"A": 2}), node=replica_A),
#            Ver(val=['iPhone', 'AirPods'], vc=VC({"A": 1, "B": 1}), node=replica_B)]

if store.has_conflicts("cart:user123"):
    print("CONFLICT DETECTED! Must resolve.")
```

### Component 3: Conflict Resolution Strategies

```python
from typing import Callable

class ConflictResolver:
    """
    Strategies for resolving conflicts in versioned store.
    """
    
    @staticmethod
    def last_write_wins(versions: List[VersionedValue]) -> Any:
        """
        LWW: Pick version with largest vector clock sum.
        
        WARNING: Loses data! Only use for commutative/idempotent data.
        """
        if not versions:
            return None
        
        def clock_sum(v: VersionedValue) -> int:
            return sum(v.vector_clock.clocks.values())
        
        winner = max(versions, key=clock_sum)
        return winner.value
    
    @staticmethod
    def union_merge(versions: List[VersionedValue]) -> Any:
        """
        Union: Merge all versions (for sets/shopping carts).
        
        Example: ["iPhone", "MacBook"] ∪ ["iPhone", "AirPods"] = ["iPhone", "MacBook", "AirPods"]
        """
        if not versions:
            return []
        
        merged = set()
        for version in versions:
            if isinstance(version.value, list):
                merged.update(version.value)
            else:
                merged.add(version.value)
        
        return list(merged)
    
    @staticmethod
    def custom_merge(versions: List[VersionedValue], merge_fn: Callable[[List[Any]], Any]) -> Any:
        """
        Custom merge: Let application define merge logic.
        
        Example: For counters, sum them.
        """
        if not versions:
            return None
        
        values = [v.value for v in versions]
        return merge_fn(values)
    
    @staticmethod
    def prompt_user(versions: List[VersionedValue]) -> Any:
        """
        Manual resolution: Show conflicts to user.
        
        Amazon shopping cart does this: "You have conflicting carts, merge?"
        """
        print("Conflict detected! Multiple versions exist:")
        for i, version in enumerate(versions):
            print(f"  {i+1}. {version.value} (from {version.node_id})")
        
        choice = input("Choose version (or 'merge' to combine all): ")
        
        if choice.lower() == 'merge':
            return ConflictResolver.union_merge(versions)
        else:
            idx = int(choice) - 1
            return versions[idx].value


# Example: Resolving shopping cart conflict
versions, context = store.read("cart:user123")

if store.has_conflicts("cart:user123"):
    # Strategy 1: Union merge (Amazon's approach)
    merged_cart = ConflictResolver.union_merge(versions)
    print(f"Merged cart: {merged_cart}")  # ['iPhone', 'MacBook', 'AirPods']
    
    # Write back merged result
    store.write("cart:user123", merged_cart, context_vc=context)
    
    # Now no conflicts
    versions, _ = store.read("cart:user123")
    print(f"After merge: {len(versions)} version(s)")  # 1 version
```

---

## Part 5: CRDTs — Conflict-Free Replicated Data Types

### What Are CRDTs?

**CRDTs** are data structures that **automatically resolve conflicts** by design. They guarantee **strong eventual consistency**: if all replicas receive all updates, they converge to the same state.

**Two flavors**:
1. **CvRDT** (Convergent): Send full state, merge with commutative/associative operation
2. **CmRDT** (Commutative): Send operations, operations commute

### CRDT Example: G-Counter (Grow-Only Counter)

```python
class GCounter:
    """
    Grow-only counter CRDT.
    Each node maintains its own counter.
    Total = sum of all node counters.
    """
    
    def __init__(self, node_id: str):
        self.node_id = node_id
        self.counts: Dict[str, int] = {}
    
    def increment(self):
        """Increment local counter."""
        self.counts[self.node_id] = self.counts.get(self.node_id, 0) + 1
    
    def value(self) -> int:
        """Get total count."""
        return sum(self.counts.values())
    
    def merge(self, other: 'GCounter'):
        """Merge with another G-Counter (take max for each node)."""
        for node_id, count in other.counts.items():
            self.counts[node_id] = max(self.counts.get(node_id, 0), count)
    
    def __repr__(self):
        return f"GCounter({self.counts}, total={self.value()})"


# Example: Distributed page view counter
counter_A = GCounter("server_A")
counter_B = GCounter("server_B")

# Server A gets 5 views
for _ in range(5):
    counter_A.increment()

# Server B gets 3 views
for _ in range(3):
    counter_B.increment()

print(f"A: {counter_A}")  # GCounter({'server_A': 5}, total=5)
print(f"B: {counter_B}")  # GCounter({'server_B': 3}, total=3)

# Merge (eventually consistent replication)
counter_A.merge(counter_B)
print(f"A after merge: {counter_A}")  # GCounter({'server_A': 5, 'server_B': 3}, total=8)

counter_B.merge(counter_A)
print(f"B after merge: {counter_B}")  # GCounter({'server_A': 5, 'server_B': 3}, total=8)

# Both converged to same state!
```

### CRDT Example: OR-Set (Add-Wins Set)

```python
import uuid

class ORSet:
    """
    Observed-Remove Set CRDT.
    Adds win over removes (good for shopping carts).
    """
    
    def __init__(self, node_id: str):
        self.node_id = node_id
        self.added: Dict[Any, Set[str]] = {}  # element -> set of unique add IDs
        self.removed: Set[str] = set()  # set of removed IDs
    
    def add(self, element: Any):
        """Add element with unique ID."""
        add_id = f"{self.node_id}:{uuid.uuid4()}"
        
        if element not in self.added:
            self.added[element] = set()
        
        self.added[element].add(add_id)
    
    def remove(self, element: Any):
        """Remove element (mark all add IDs as removed)."""
        if element in self.added:
            self.removed.update(self.added[element])
    
    def contains(self, element: Any) -> bool:
        """Check if element is in set."""
        if element not in self.added:
            return False
        
        # Element is in set if it has add IDs not in removed set
        active_ids = self.added[element] - self.removed
        return len(active_ids) > 0
    
    def elements(self) -> List[Any]:
        """Get all elements in set."""
        result = []
        for element, add_ids in self.added.items():
            if self.contains(element):
                result.append(element)
        return result
    
    def merge(self, other: 'ORSet'):
        """Merge with another OR-Set."""
        # Union add IDs
        for element, add_ids in other.added.items():
            if element not in self.added:
                self.added[element] = set()
            self.added[element].update(add_ids)
        
        # Union removed IDs
        self.removed.update(other.removed)
    
    def __repr__(self):
        return f"ORSet({self.elements()})"


# Example: Shopping cart as CRDT
cart_A = ORSet("replica_A")
cart_B = ORSet("replica_B")

# User adds iPhone on replica A
cart_A.add("iPhone")

# User adds MacBook on replica A
cart_A.add("MacBook")

# Concurrent: User adds AirPods on replica B
cart_B.add("AirPods")

print(f"A before merge: {cart_A}")  # ['iPhone', 'MacBook']
print(f"B before merge: {cart_B}")  # ['AirPods']

# Replicas sync
cart_A.merge(cart_B)
cart_B.merge(cart_A)

print(f"A after merge: {cart_A}")  # ['iPhone', 'MacBook', 'AirPods']
print(f"B after merge: {cart_B}")  # ['iPhone', 'MacBook', 'AirPods']

# Both converged! No conflicts, no data loss.
```

---

## Part 6: Failure Modes & Hardening

### Failure Mode 1: Vector Clock Explosion

**Problem**: Vector clocks grow with number of nodes. 1000 nodes = 1000 counters per version.

**Solution**: Prune old entries
```python
def prune_vector_clock(vc: VectorClock, active_nodes: Set[str]):
    """Remove entries for nodes that are no longer active."""
    vc.clocks = {node: ts for node, ts in vc.clocks.items() if node in active_nodes}
```

**Production approach (Riak)**: Limit vector clock size to 100 entries, keep only most recent.

### Failure Mode 2: Sibling Explosion

**Problem**: Frequent concurrent writes create many sibling versions. Each read returns 100+ versions.

**Solution**: Aggressive client-side resolution
```python
def aggressive_resolve(key: str):
    """Force resolution after every write."""
    versions, context = store.read(key)
    
    if len(versions) > 1:
        # Immediately resolve and write back
        resolved = ConflictResolver.union_merge(versions)
        store.write(key, resolved, context_vc=context)
```

### Failure Mode 3: Tombstone Accumulation

**Problem**: Removed elements leave tombstones forever (for CRDT correctness).

**Solution**: Garbage collect tombstones after causal stability
```python
def gc_tombstones(orset: ORSet, stable_vc: VectorClock):
    """Remove tombstones that are causally dominated by stable_vc."""
    # Only remove tombstones older than stable point
    # Requires coordination to determine stable point
    pass
```

---

## Part 7: What This Teaches for System Design Interviews

### When Asked: "Design a Shopping Cart Service"

**Key points**:
1. **Identify consistency requirements**: Shopping cart tolerates eventual consistency
2. **Choose replication strategy**: Multi-master for availability (Dynamo-style)
3. **Conflict detection**: Use vector clocks to detect concurrent updates
4. **Conflict resolution**: Union merge for shopping carts (add-wins)
5. **Alternative**: Use CRDT (OR-Set) to avoid conflicts entirely

### When Asked: "CAP Theorem Trade-offs"

**Strong answer**:
- "For shopping carts, we prioritize Availability and Partition tolerance"
- "Use eventual consistency with vector clocks for causality tracking"
- "Conflicts resolved client-side with union merge (all items preserved)"
- "Better to show user merged cart than fail the request"

### When Asked: "How does Dynamo work?"

**Key components**:
1. **Consistent hashing**: Partition data across nodes
2. **Quorum replication**: Write to W nodes, read from R nodes, W+R > N
3. **Vector clocks**: Track causality, detect conflicts
4. **Conflict resolution**: Application-specific merge (union for carts)
5. **Hinted handoff**: Temporary storage when replica down
6. **Merkle trees**: Anti-entropy for replica sync

---

## Summary: The Distributed Systems Stack So Far

**Episodes 3.1-3.7** gave you:

1. **Episode 3.1**: Replication (Raft log replication)
2. **Episode 3.2**: Consensus (Raft leader election)
3. **Episode 3.3**: Sharding (Consistent hashing)
4. **Episode 3.4**: Distributed indexing (Bigtable tablets)
5. **Episode 3.5**: Cross-shard transactions (Percolator)
6. **Episode 3.6**: Global databases (Spanner + TrueTime)
7. **Episode 3.7**: Eventual consistency (Dynamo + CRDTs) ✅

**Next**: Episode 3.8 — Distributed Caching (Redis Cluster)

You now understand:
- ✅ Strong consistency (Spanner)
- ✅ Eventual consistency (Dynamo)
- ✅ When to use each
- ✅ How to handle conflicts without losing data
