Episode 3.5 — Cross-Shard Transactions: ACID Across a Thousand Machines

From Two-Pointer Synchronization to Percolator-Style Distributed Transactions

In this episode, you will learn:
.Why 2PC blocks and how Percolator avoids it with timestamps
.How to achieve snapshot isolation across 1000 shards with O(1) commit latency
.When to use 2PC vs OCC vs Sagas in production systems
.How Google, CockroachDB, and Aurora actually implement distributed transactions

---

1. The Hook: Real-World Production Failure

The 2018 Coinbase Transaction Duplication
"$100M+ in erroneous transactions due to broken atomic commits"

The Actual Bug:

1. Coinbase's homegrown 2PC had a bug in timeout handling
2. During network partition: Coordinator timed out → aborted transaction
3. But one participant had already committed locally
4. Result: Money appeared in one account without debiting another

The Critical Insight: 2PC's blocking nature creates a fundamental tradeoff:

· Wait for timeout? → User sees 30s latency
· Proceed without timeout? → Risk double-spend
· Use non-blocking protocol? → More complex, still imperfect

The Lesson: Distributed transactions need failure recovery, not just happy-path correctness.

---

2. The LeetCode Seed: Two-Pointer Synchronization

```python
# LeetCode 23: Merge k Sorted Lists
# Real-world application: Ordering commits across shards

def merge_transaction_logs(logs):
    """
    Problem: Each shard has local commit log
    Goal: Create global ordering while preserving causality
    """
    import heapq
    
    heap = []
    # Push first entry from each shard's log
    for shard_id, log in enumerate(logs):
        if log:
            heapq.heappush(heap, (log[0]['timestamp'], shard_id, 0))
    
    global_order = []
    
    while heap:
        timestamp, shard_id, idx = heapq.heappop(heap)
        global_order.append(logs[shard_id][idx])
        
        # Move this shard's pointer forward
        if idx + 1 < len(logs[shard_id]):
            next_entry = logs[shard_id][idx + 1]
            heapq.heappush(heap, (next_entry['timestamp'], shard_id, idx + 1))
    
    return global_order

# The Insight: Two-pointer synchronization requires...
# 1. Each pointer moves independently (shard autonomy)
# 2. Global ordering via heap (coordinator)
# 3. Progress only when all can advance (consensus)

# This maps directly to 2PC:
# - Shards = lists
# - Prepare = check if pointer can advance  
# - Commit = actually advance pointer
```

---

3. Distributed Systems Mapping

The Transaction Protocol Spectrum

Protocol Consistency Latency Fault Tolerance Use Case
2PC Strong (serializable) High (2 RTTs) Low (blocks on coordinator) Banking transfers
Percolator Snapshot isolation Medium (1 RTT to timestamp oracle) High (no blocking) Google Search index
Sagas Eventual Variable (async) Very high E-commerce workflows
3PC Strong Highest (3 RTTs) Medium (non-blocking) Theoretical, rarely used

The CAP Theorem Reality

```
During network partition:
- 2PC chooses C (consistency) → blocks or aborts
- Percolator chooses A (availability) → continues with possible conflicts
- Sagas choose AP (availability + partition tolerance) → eventual consistency

No protocol gives you all three. You must choose based on business requirements.
```

Visual Timeline: 2PC vs Percolator vs Sagas

```
2PC (Blocking, Pessimistic):
Client → Coordinator → Prepare → All Participants
      ← All "YES" ←
Client → Coordinator → Commit → All Participants
      ← All "ACK" ←
PROBLEM: If coordinator crashes after prepare, participants block forever.

Percolator (Non-blocking, Optimistic):
Client → Get StartTS → Prewrite all keys (with locks) → Get CommitTS
      → Commit Primary → Commit Secondaries → Done
KEY: Primary lock makes recovery trivial - check primary's state.

Sagas (Eventual, Compensating):
Client → Step 1 (debit account) → Step 2 (reserve inventory) → Step 3 (ship)
If Step 3 fails → Compensate Step 2 → Compensate Step 1
Tradeoff: No atomicity, but no blocking either.
```

Why 3PC is Never Used in Production

```
3PC adds a "pre-commit" phase between prepare and commit:
Goal: Make protocol non-blocking
Reality: Breaks under network partitions

Scenario: Network splits cluster in half
- Left side: Coordinator in pre-commit → eventually commits
- Right side: Coordinator times out → aborts
Result: Data corruption - both commit and abort happen

Lesson: You can't have non-blocking strong consistency without perfect failure detection.
And perfect failure detection is impossible in asynchronous networks (FLP impossibility).
```

---

4. Production System Build

Component 1: Simplified Timestamp Oracle

```python
class TimestampOracle:
    """
    Single source of truth for timestamps
    In production: Spanner uses TrueTime, CockroachDB uses HLC
    """
    
    def __init__(self):
        self.last_timestamp = 0
        self.lock = threading.Lock()
        
    def get_timestamp(self):
        """Monotonically increasing timestamp"""
        with self.lock:
            self.last_timestamp += 1
            return self.last_timestamp
    
    def get_commit_ts(self, start_ts):
        """Commit timestamp must be > start_ts"""
        with self.lock:
            self.last_timestamp = max(self.last_timestamp, start_ts) + 1
            return self.last_timestamp

# Critical insight: Timestamp ordering replaces locking
# Later timestamp → later transaction in serialization order
```

Component 2: Core Percolator Protocol with Primary Lock

```python
class PercolatorTransaction:
    """
    Google's Percolator: Optimistic, timestamp-based
    KEY INNOVATION: Primary lock simplifies recovery
    
    Design: Among all keys in transaction, pick one as "primary"
    - Commit timestamp written to primary first
    - During recovery: check primary → know entire transaction state
    - No need to check all keys individually
    """
    
    def __init__(self, timestamp_oracle):
        self.start_ts = timestamp_oracle.get_timestamp()
        self.commit_ts = None
        self.writes = {}  # key → new_value
        self.buffer = {}  # read cache
        self.primary_key = None  # One key is special for recovery
        
    def get(self, key):
        """Read at start_ts using MVCC"""
        # Find latest version ≤ start_ts
        versions = storage.get_versions(key)
        for ts, value in sorted(versions.items(), reverse=True):
            if ts <= self.start_ts:
                commit_ts = storage.get_commit_ts(key, ts)
                if commit_ts and commit_ts <= self.start_ts:
                    return value
        return None
    
    def commit(self):
        """Two-phase without blocking: prewrite then commit"""
        # Choose primary key (any key in write set)
        self.primary_key = next(iter(self.writes.keys()))
        
        # Phase 1: Write data with locks (invisible to others)
        for key, value in self.writes.items():
            if not self._prewrite(key, value, is_primary=(key == self.primary_key)):
                self._cleanup()
                return False
        
        # Get commit timestamp
        self.commit_ts = timestamp_oracle.get_commit_ts(self.start_ts)
        
        # Phase 2: Commit primary first, then secondaries
        # This ordering makes recovery trivial
        if not self._commit_write(self.primary_key):
            return False
            
        for key in self.writes.keys():
            if key != self.primary_key:
                if not self._commit_write(key):
                    # Secondary commit failed, but primary committed
                    # Recovery will fix this eventually
                    pass
        
        self._cleanup_locks()
        return True
    
    def _prewrite(self, key, value, is_primary=False):
        """Write with optimistic conflict detection"""
        # Check for concurrent writes (write-write conflict)
        lock = storage.get_lock(key)
        if lock and lock.ts < self.start_ts:
            # Older transaction stalled → clean it up
            self._resolve_stalled_lock(lock)
        elif lock:
            # Newer transaction → we abort
            # EXAMPLE: Tx1 prewrites at TS=10, Tx2 tries at TS=20 → Tx2 aborts
            return False
        
        # Write data with lock (mark if primary)
        storage.write(key, self.start_ts, value, lock=True, is_primary=is_primary)
        return True
    
    def _recover_stalled_transaction(self, key_with_lock):
        """
        Recovery using primary lock optimization:
        1. Find primary key for this transaction
        2. Check if primary committed
        3. Apply same outcome to all keys
        """
        # Find primary key (stored in lock metadata)
        primary_key = storage.get_primary_key(key_with_lock)
        
        # Check primary's state
        if storage.is_committed(primary_key, key_with_lock.start_ts):
            # Transaction committed → push commits to all keys
            commit_ts = storage.get_commit_ts(primary_key, key_with_lock.start_ts)
            for key in storage.get_all_keys_in_transaction(primary_key):
                storage.push_commit(key, key_with_lock.start_ts, commit_ts)
        else:
            # Transaction aborted → clean up
            for key in storage.get_all_keys_in_transaction(primary_key):
                storage.cleanup_aborted(key, key_with_lock.start_ts)
```

MVCC Visibility Rules (Critical for Understanding)

```python
# Example: Key X history
# Timestamp | Value | Commit Timestamp
# ----------|-------|-----------------
# 10        | "A"   | 20
# 30        | "B"   | 40
# 50        | "C"   | 60

def what_transaction_sees(start_ts):
    """
    Transaction sees latest version where:
    1. Version timestamp ≤ start_ts
    2. Commit timestamp ≤ start_ts
    3. No newer uncommitted version
    """
    if start_ts == 25:
        return "A"  # Version at 10, committed at 20 ≤ 25
    
    if start_ts == 45:
        return "B"  # Version at 30, committed at 40 ≤ 45
    
    if start_ts == 55:
        return "B"  # Version at 50 NOT visible (committed at 60 > 55)
    
    if start_ts == 65:
        return "C"  # Version at 50, committed at 60 ≤ 65

# Key insight: Values become visible when commit_ts ≤ current_time
# This is how snapshot isolation works - each transaction sees consistent snapshot
```

Component 3: 2PC for Comparison

```python
class TwoPhaseCommit:
    """
    Traditional 2PC: Blocking but strongly consistent
    """
    
    def commit(self, transaction_id, participants):
        # Phase 1: Prepare (can everyone commit?)
        prepare_oks = {}
        for participant in participants:
            prepare_oks[participant] = participant.prepare(transaction_id)
        
        if not all(prepare_oks.values()):
            self._abort_all(participants, transaction_id)
            return False
        
        # Phase 2: Commit (actually do it)
        # CRITICAL: If coordinator crashes here, participants block
        commit_oks = {}
        for participant in participants:
            commit_oks[participant] = participant.commit(transaction_id)
        
        return all(commit_oks.values())

# The blocking problem visualized:
#
# Time    Coordinator    Participant A    Participant B
# 0       send prepare   → prepare ok     → prepare ok
# 1       send commit    → commit         X CRASH!
# 2       DEAD          BLOCKED!         BLOCKED!
#
# Participant A now holds locks forever, waiting for commit/abort
```

Component 4: CockroachDB's Write Skew Prevention

```python
class CockroachDBTransaction:
    """
    CockroachDB achieves serializable snapshot isolation
    Key trick: Read refresh to detect write skew
    
    Problem with naive snapshot isolation:
    T1: Read A=10, B=20, sum=30
    T2: Read A=10, B=20, sum=30  
    T1: Write A=0
    T2: Write B=0
    Result: A=0, B=0, sum=0 (violates sum=30 invariant)
    
    Solution: Before commit, refresh reads to ensure no writes happened
    """
    
    def commit_with_serializability(self):
        """
        CockroachDB's approach:
        1. Write data with "write intents" (optimistic locks)
        2. Before commit, refresh all reads
        3. If any read changed, abort (write skew detected)
        """
        # Phase 1: Write data with intents
        for key, value in self.writes.items():
            storage.write_intent(key, self.start_ts, value)
        
        # Phase 2: Validate no write skew occurred
        for key in self.read_set:
            current_value = storage.get_latest_committed(key, self.start_ts)
            if current_value != self.read_set[key]:
                # Write skew detected - abort
                self._cleanup_intents()
                return False
        
        # Phase 3: Commit
        self.commit_ts = hlc.now()
        for key in self.writes:
            storage.commit_intent(key, self.start_ts, self.commit_ts)
        
        return True

# This gives serializable isolation, not just snapshot isolation
# Detects and prevents "write skew" anomalies
```

Component 5: Saga Pattern for Business Workflows

```python
class OrderSaga:
    """
    Real-world example: E-commerce order processing
    Each step can fail, needs compensation
    """
    
    def create_order(self, user_id, items):
        """
        Saga steps:
        1. Reserve inventory
        2. Charge credit card
        3. Create shipping label
        4. Send confirmation email
        
        If any step fails, run compensation in reverse
        """
        steps = [
            {
                'action': self._reserve_inventory,
                'compensation': self._release_inventory,
                'params': {'items': items}
            },
            {
                'action': self._charge_credit_card,
                'compensation': self._refund_payment,
                'params': {'user_id': user_id, 'amount': self._calculate_total(items)}
            },
            {
                'action': self._create_shipping_label,
                'compensation': self._cancel_shipping,
                'params': {'user_id': user_id, 'items': items}
            },
            {
                'action': self._send_confirmation_email,
                'compensation': None,  # No compensation for email
                'params': {'user_id': user_id, 'order_id': self.order_id}
            }
        ]
        
        executed_steps = []
        
        for step in steps:
            try:
                step['action'](**step['params'])
                executed_steps.append(step)
            except Exception as e:
                # Compensate in reverse order
                for executed in reversed(executed_steps):
                    if executed['compensation']:
                        executed['compensation'](**executed['params'])
                raise OrderFailedError(f"Step failed: {e}")
        
        return self.order_id

# Other real-world Saga examples:
# 1. Ride booking: Find driver → Charge rider → Start trip
# 2. Hotel + flight: Book flight → Book hotel → Reserve car
# 3. Banking: Check balance → Debit account → Credit recipient
```

Component 6: Smart Transaction Router

```python
class TransactionRouter:
    """
    Chooses protocol based on transaction characteristics
    Real systems use ML for this (Google's AutoPilot, Aurora's ML)
    """
    
    def choose_protocol(self, transaction):
        features = self._extract_features(transaction)
        
        # Decision tree (simplified)
        if features['requires_serializable']:
            return '2pc'
        elif features['num_shards'] == 1:
            return 'percolator'  # Single-shard optimization
        elif features['estimated_duration'] > 10:  # seconds
            return 'saga'  # Long-running
        elif features['contention_risk'] > 0.3:
            return '2pc'  # High conflict needs locking
        else:
            return 'percolator'  # Default OCC
    
    def _extract_features(self, transaction):
        return {
            'num_shards': len(transaction['participants']),
            'read_write_ratio': transaction['reads'] / max(transaction['writes'], 1),
            'contention_risk': self._estimate_contention(transaction),
            'estimated_duration': self._estimate_duration(transaction),
            'requires_serializable': transaction.get('isolation') == 'serializable'
        }
```

---

5. Isolation Levels: What Each Protocol Guarantees

Isolation Level Comparison Table

```python
isolation_levels = {
    "Read Uncommitted": {
        "Guarantee": "None - dirty reads possible",
        "Protocols": "None (avoid in production)",
        "Use Case": "Analytics only",
        "Performance": "Highest"
    },
    "Read Committed": {
        "Guarantee": "No dirty reads",
        "Protocols": "Most 2PC implementations",
        "Use Case": "General OLTP",
        "Performance": "High"
    },
    "Snapshot Isolation": {
        "Guarantee": "Repeatable reads, no write skew (but phantom reads possible)",
        "Protocols": "Percolator, Spanner's read-only",
        "Use Case": "Google Search index, analytics",
        "Performance": "Medium"
    },
    "Serializable": {
        "Guarantee": "Full serializability (no anomalies)",
        "Protocols": "2PC with strict locking, CockroachDB with read refresh",
        "Use Case": "Banking, financial transactions",
        "Performance": "Lowest"
    },
    "External Consistency": {
        "Guarantee": "Commit order matches real-time order (stronger than serializable)",
        "Protocols": "Spanner with TrueTime",
        "Use Case": "Global financial systems",
        "Performance": "Low (requires commit wait)"
    }
}

# Key insight: You pay for what you get
# Higher isolation = lower performance
# Choose the minimum isolation your business needs
```

---

6. Failure Modes & Solutions

Failure 1: 2PC Coordinator Crash

```python
# Problem: Coordinator crashes after prepare, before commit
# Participants are stuck in prepared state (blocking resources)

class NonBlocking2PC:
    def __init__(self):
        self.timeout = 30  # seconds
        self.recovery_log = WriteAheadLog('/2pc/recovery.log')
    
    def prepare_with_timeout(self, participant, txid):
        """Prepare with automatic timeout-based abort"""
        try:
            # Log prepare before sending
            self.recovery_log.write(f'prepare:{txid}:{participant}')
            
            # Send with timeout
            return participant.prepare(txid, timeout=self.timeout)
        except TimeoutError:
            # Timeout → assume abort
            self.recovery_log.write(f'timeout:{txid}:{participant}')
            return False
    
    def recovery_daemon(self):
        """Clean up stuck transactions"""
        while True:
            stuck_txs = self._find_stuck_transactions()
            for txid, participants in stuck_txs:
                # Heuristic: If any participant committed, push commit
                # Otherwise, abort
                if self._any_committed(txid):
                    self._force_commit(txid, participants)
                else:
                    self._force_abort(txid, participants)
            time.sleep(10)
```

Failure 2: Clock Skew Breaking Serializability

```python
# Problem: T1 starts at time 100, T2 at time 90 due to clock skew
# T2 appears to happen before T1, breaking causality

class TrueTimeTransaction:
    """
    Spanner's solution: Bounded clock uncertainty
    """
    def __init__(self, epsilon_ms=7):
        self.epsilon = epsilon_ms  # Max clock error
    
    def commit_with_external_consistency(self):
        """
        Wait until commit is definitely in the past globally
        """
        commit_ts = truetime.now().latest
        
        # Wait epsilon to ensure no earlier commit could have this timestamp
        wait_until = commit_ts + self.epsilon
        truetime.sleep_until(wait_until)
        
        # Now safe to acknowledge to client
        return commit_ts

# This gives external consistency:
# If T1 commits before T2 starts in real time,
# T1's timestamp < T2's timestamp in system time
```

Failure 3: Write-Write Conflict in Percolator

```python
# Scenario demonstrating optimistic conflict detection:
# 
# Timeline:
# T=0: Tx1 starts (start_ts=10), reads key A=100
# T=1: Tx2 starts (start_ts=20), reads key A=100  
# T=2: Tx1 prewrites A=90 (succeeds, gets lock)
# T=3: Tx2 tries to prewrite A=80 (fails! sees lock from Tx1)
# T=4: Tx2 aborts, retries with new start_ts=30
# T=5: Tx1 commits (commit_ts=25)
# T=6: Tx2 reads A=90 (new value), prewrites A=80 (now succeeds)

class WriteWriteConflictExample:
    def demonstrate(self):
        print("""
        Key insight: Optimistic concurrency control detects conflicts at commit time
        Not at read time (like pessimistic locking)
        
        Advantage: No blocking during reads
        Disadvantage: Aborts under high contention
        
        This is why Percolator works great for:
        - Search indexing (low contention, many reads)
        - Analytics (batch writes, few conflicts)
        
        But not for:
        - Hot counters (high contention → many aborts)
        - Auction systems (last write should win)
        """)
```

Failure 4: Cross-Shard Foreign Keys

```python
# Problem: Order references User, but they're on different shards
# Can't validate foreign key atomically

class DeferredConstraintValidation:
    def validate_async(self, constraint):
        """
        Validate in background, fix violations eventually
        """
        # Queue for async validation
        self.validation_queue.put(constraint)
        
        # Return success immediately
        return True
    
    def validation_worker(self):
        """Background constraint checking"""
        while True:
            constraint = self.validation_queue.get()
            
            try:
                valid = self._check_constraint(constraint)
                if not valid:
                    self._handle_violation(constraint)  # Email, alert, fix
            except Exception:
                # Retry later
                self.validation_queue.put(constraint)
            
            time.sleep(0.1)
```

---

7. Hardening for Production

Optimization 1: Parallel 2PC

```python
class Parallel2PC:
    """
    Execute independent operations in parallel
    Reduces latency from O(n) to O(1) for independent ops
    """
    def execute_parallel(self, operations):
        # Group by dependency
        independent_sets = self._find_independent_operations(operations)
        
        # Execute each set in parallel
        with ThreadPoolExecutor() as executor:
            futures = []
            for op_set in independent_sets:
                future = executor.submit(self._execute_operation_set, op_set)
                futures.append(future)
            
            # Wait for all
            results = [f.result() for f in futures]
        
        return self._combine_results(results)
```

Optimization 2: Hybrid Logical Clocks

```python
class HybridLogicalClock:
    """
    CockroachDB's approach: Combine physical + logical time
    No need for TrueTime's GPS/atomic clocks
    """
    def __init__(self, node_id):
        self.node_id = node_id
        self.last_physical = 0
        self.logical = 0
    
    def now(self):
        physical = int(time.time() * 1000)
        
        if physical > self.last_physical:
            self.last_physical = physical
            self.logical = 0
        else:
            self.logical += 1
        
        return (self.last_physical, self.logical, self.node_id)
    
    def update(self, incoming_ts):
        """Ensure causal ordering"""
        incoming_physical, incoming_logical, _ = incoming_ts
        
        if incoming_physical > self.last_physical:
            self.last_physical = incoming_physical
            self.logical = incoming_logical + 1
        elif incoming_physical == self.last_physical:
            self.logical = max(self.logical, incoming_logical) + 1
        else:
            self.logical += 1
```

Optimization 3: Read-Only Transaction Optimization

```python
class ReadOnlyOptimizer:
    """
    Read-only transactions don't need locking
    Can use any consistent snapshot
    """
    def execute_read_only(self, query, consistency='strong'):
        if consistency == 'strong':
            # Read latest snapshot
            snapshot_ts = storage.get_latest_snapshot()
            return self._execute_at_snapshot(query, snapshot_ts)
        elif consistency == 'stale':
            # Read from replica with bounded staleness
            replica_ts = self._get_replica_snapshot(max_staleness_ms=1000)
            return self._execute_at_snapshot(query, replica_ts)
    
    def _execute_at_snapshot(self, query, snapshot_ts):
        """MVCC read at specific timestamp"""
        # No locks needed!
        results = []
        for table in query['tables']:
            versions = storage.get_versions_at(table, snapshot_ts)
            results.extend(self._filter_versions(versions, query['predicates']))
        return results
```

Optimization 4: Transaction Batching

```python
class TransactionBatcher:
    """
    Batch multiple transactions together
    Reduces coordination overhead
    """
    def __init__(self, batch_window_ms=10):
        self.batch_window = batch_window_ms / 1000.0
        self.current_batch = []
        
    def submit(self, transaction):
        self.current_batch.append(transaction)
        
        if (len(self.current_batch) >= 1000 or
            transaction.get('priority') == 'high'):
            return self._execute_batch_now()
        else:
            return self._wait_for_batch()
    
    def _execute_batch(self, batch):
        # Group by shard
        shard_groups = defaultdict(list)
        for tx in batch:
            for shard in tx['shards']:
                shard_groups[shard].append(tx)
        
        # Execute per-shard batches in parallel
        with ThreadPoolExecutor() as executor:
            futures = {}
            for shard, txs in shard_groups.items():
                future = executor.submit(self._execute_shard_batch, shard, txs)
                futures[shard] = future
            
            # Combine results
            results = {}
            for shard, future in futures.items():
                results[shard] = future.result()
        
        return results
```

---

8. Interview Cheatsheet

Protocol Decision Framework

```python
def choose_transaction_protocol(requirements):
    """
    Rule of thumb for interviews:
    """
    if requirements['atomic_money_transfers']:
        return "2PC (strong consistency needed)"
    
    elif requirements['scalable_indexing_or_analytics']:
        return "Percolator/OCC (low contention, many reads)"
    
    elif requirements['multi_step_business_workflow']:
        return "Sagas (long-running, needs compensation)"
    
    elif requirements['global_consistency_across_continents']:
        return "Spanner/TrueTime (external consistency)"
    
    elif requirements['high_contention_hot_keys']:
        return "2PC or pessimistic locking (avoid aborts)"
    
    else:
        return "Percolator (default for most cases)"
```

Production Checklist

```markdown
- [ ] Timeout-based recovery for stuck transactions
- [ ] Deadlock detection (wait-for graphs)
- [ ] Write-ahead logging for crash recovery  
- [ ] Metrics: commit/abort rates, latency percentiles
- [ ] Automatic retry for aborted transactions
- [ ] Protocol choice based on workload
- [ ] Read-only transaction optimization
- [ ] Cross-shard constraint validation (async)
```

Common Interview Questions & Answers

```python
qa = {
    "Q: How do you handle coordinator failure in 2PC?": 
    "A: Timeout-based recovery with write-ahead logging. Participants abort after timeout unless they see commit in recovery log.",
    
    "Q: Why does Percolator need a timestamp oracle?":
    "A: To establish global ordering without locking. Timestamps determine serialization order.",
    
    "Q: When would you use Sagas over 2PC?":
    "A: For long-running business processes where blocking for seconds is unacceptable. Sagas use compensation instead of rollback.",
    
    "Q: How does Spanner achieve external consistency?":
    "A: TrueTime with bounded uncertainty + commit wait. Wait ε ms after commit to ensure global visibility.",
    
    "Q: What's the tradeoff between OCC and pessimistic locking?":
    "A: OCC has lower latency in low contention, higher abort rates in high contention. Pessimistic locking has higher latency but no aborts.",
    
    "Q: How does CockroachDB prevent write skew?":
    "A: Read refresh before commit. Re-reads all keys in read set, aborts if any changed (write skew detected).",
    
    "Q: Why isn't 3PC used in production?":
    "A: Breaks under network partitions - both sides can commit independently, causing data corruption."
}
```

---

9. What You've Built

Protocol Comparison Summary

```python
protocol_comparison = {
    "2PC": {
        "Best for": "Atomic money transfers, inventory management",
        "Isolation": "Serializable (strongest)",
        "Failure mode": "Blocks on coordinator crash",
        "Real use": "Traditional banking, XA transactions"
    },
    "Percolator": {
        "Best for": "Search indexing, analytics, logging",
        "Isolation": "Snapshot isolation",
        "Failure mode": "Aborts under high contention",
        "Real use": "Google Search, Bigtable indexing"
    },
    "CockroachDB": {
        "Best for": "General OLTP with serializable guarantees",
        "Isolation": "Serializable snapshot isolation",
        "Failure mode": "Aborts on write skew",
        "Real use": "Cloud-native distributed SQL"
    },
    "Sagas": {
        "Best for": "E-commerce workflows, ride booking",
        "Isolation": "Eventual (business-level consistency)",
        "Failure mode": "Complex compensation logic",
        "Real use": "Amazon order processing, Uber rides"
    },
    "Spanner": {
        "Best for": "Global financial systems",
        "Isolation": "External consistency (stronger than serializable)",
        "Failure mode": "Higher latency due to commit wait",
        "Real use": "Google Ads, global banking"
    }
}
```

Key Insights:

1. No free lunch: CAP theorem applies - choose based on business needs
2. Timestamp ordering replaces locking in scalable systems
3. Recovery is as important as the happy path
4. Different workloads need different protocols - no one-size-fits-all
5. Isolation levels matter - understand what each protocol guarantees

This knowledge powers:

· Banking systems (2PC for transfers)
· Google Search (Percolator for indexing)
· E-commerce (Sagas for order processing)
· Global databases (Spanner for worldwide consistency)
· Modern distributed SQL (CockroachDB for general OLTP)

---

Next Episode: Episode 3.6 — Global Databases (Spanner)
We'll build TrueTime, understand why ε = 7ms matters, and create a database where commits in Tokyo are instantly visible in New York.

Ready for global consistency? Let's build a database that treats the Earth as one computer.